{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 13:18:57.908478: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-09 13:18:57.908536: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-09 13:18:57.908576: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    MarianMTModel,\n",
    "    MarianTokenizer,\n",
    ")\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "\n",
    "# Load tokenizers and models\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)\n",
    "\n",
    "# Load dataset\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "\n",
    "# Set batch size\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Random seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Thresholds for toxicity and acceptability\n",
    "ACCEPTABILITY_THRESHOLD_SOURCE = 0.6541633009910583\n",
    "ACCEPTABILITY_THRESHOLD_TARGET = 0.7226778864860535\n",
    "SIMILARITY_THRESHOLD = 0.9040371657728449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 23034.0MB\n",
      "Free GPU memory: 20429.0MB\n",
      "Used GPU memory: 2088.0MB\n"
     ]
    }
   ],
   "source": [
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not batched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bertscore = None\n",
    "\n",
    "def calc_bert_score_nobatch(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_tox_acceptability_nobatch(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batched form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bertscore = None\n",
    "\n",
    "# def calc_bert_score(\n",
    "#     refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "#     ):\n",
    "#     \"\"\"\n",
    "#     Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "#     Args:\n",
    "#         refs (list): List of reference sentences.\n",
    "#         y_pred (list): List of predicted sentences.\n",
    "#         model_type (str): Type of BERT model to use.\n",
    "#         output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "#     Returns:\n",
    "#         list of precision, recall, f1 scores.\n",
    "\n",
    "#     \"\"\"\n",
    "#     global model_bertscore\n",
    "\n",
    "#     if model_bertscore is None:\n",
    "#         model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "#     results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "#     precision = np.array(results[\"precision\"])\n",
    "#     recall = np.array(results[\"recall\"])\n",
    "#     f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "#     if output_mean:\n",
    "#         precision = precision.mean()\n",
    "#         recall = recall.mean()\n",
    "#         f1 = f1.mean()\n",
    "\n",
    "#     return precision, recall, f1\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", batch_size=32, output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line in batches.\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        batch_size (int): Number of examples per batch.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores if output_mean=False.\n",
    "        mean of precision, recall, f1 scores if output_mean=True.\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "    all_precision, all_recall, all_f1 = [], [], []\n",
    "    for i in range(0, len(refs), batch_size):\n",
    "        batch_refs = refs[i:i+batch_size]\n",
    "        batch_preds = preds[i:i+batch_size]\n",
    "        batch_results = model_bertscore.compute(predictions=batch_preds, references=batch_refs, model_type=model_type)\n",
    "        all_precision.extend(batch_results[\"precision\"])\n",
    "        all_recall.extend(batch_results[\"recall\"])\n",
    "        all_f1.extend(batch_results[\"f1\"])\n",
    "\n",
    "    if output_mean:\n",
    "        precision = np.mean(all_precision)\n",
    "        recall = np.mean(all_recall)\n",
    "        f1 = np.mean(all_f1)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    return all_precision, all_recall, all_f1\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    batch_size=32,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset in batches.\n",
    "\n",
    "    Args:\n",
    "        data: list of strings to be evaluated\n",
    "        tokenizer: tokenizer for the model\n",
    "        model: model to be used for evaluation\n",
    "        batch_size: size of the batch for processing\n",
    "        output_score: whether to output the score or the label\n",
    "        output_mean: whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        Array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    all_results = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_data = data[i:i+batch_size]\n",
    "        inputs = tokenizer(batch_data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs)[\"logits\"]\n",
    "            if output_score:\n",
    "                batch_results = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "            else:\n",
    "                batch_results = logits.argmax(1)\n",
    "            all_results.append(batch_results.cpu().numpy())\n",
    "\n",
    "        del inputs\n",
    "        del logits\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    if not output_mean:\n",
    "        return np.concatenate(all_results)\n",
    "\n",
    "    return np.mean(np.concatenate(all_results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example reference sentences\n",
    "refs = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Artificial intelligence has transformed many technological domains.\",\n",
    "    \"London is known for its rich history and cultural heritage.\"\n",
    "]\n",
    "\n",
    "# Example predicted sentences\n",
    "preds = [\n",
    "    \"A fast dark-colored fox leaps above the inactive canine.\",\n",
    "    \"AI has revolutionized multiple aspects of technology.\",\n",
    "    \"London is renowned for its historical and cultural legacy.\"\n",
    "]\n",
    "\n",
    "assert calc_bert_score(refs=refs, preds=preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True) == calc_bert_score_nobatch(refs=refs, preds=preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True)\n",
    "assert calc_tox_acceptability(data=refs, tokenizer=tokenizer_toxicity, model=model_toxicity, output_score=True, output_mean=True) == calc_tox_acceptability_nobatch(data=refs, tokenizer=tokenizer_toxicity, model=model_toxicity, output_score=True, output_mean=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA to set tresholds for filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 23034.0MB\n",
      "Free GPU memory: 20429.0MB\n",
      "Used GPU memory: 2088.0MB\n"
     ]
    }
   ],
   "source": [
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get acceptability scores for raw source and target sentences\n",
    "# raw_src_acceptability = calc_tox_acceptability(raw_datasets[\"train\"][\"source\"], tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "# raw_tgt_acceptability = calc_tox_acceptability(raw_datasets[\"train\"][\"target\"], tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "\n",
    "# # Calculate semantic similarity scores for raw source and target sentences\n",
    "# raw_similarity = calc_bert_score(raw_datasets[\"train\"][\"source\"], raw_datasets[\"train\"][\"target\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "# # Plot in density plots\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# axs[0].set_title(\"Acceptability scores for raw source and target sentences\")\n",
    "# axs[0].set_xlabel(\"Acceptability score\")\n",
    "# axs[0].set_ylabel(\"Density\")\n",
    "# axs[0].hist(raw_src_acceptability, bins=20, alpha=0.5, label=\"Source\")\n",
    "# axs[0].hist(raw_tgt_acceptability, bins=20, alpha=0.5, label=\"Target\")\n",
    "# axs[0].legend()\n",
    "\n",
    "# axs[1].set_title(\"Semantic similarity scores for raw source and target sentences\")\n",
    "# axs[1].set_xlabel(\"Semantic similarity score\")\n",
    "# axs[1].set_ylabel(\"Density\")\n",
    "# axs[1].hist(raw_similarity, bins=20, alpha=0.5)\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # Calculate acceptability thresholds\n",
    "# ACCEPTABILITY_THRESHOLD_SOURCE = np.mean(raw_src_acceptability)\n",
    "# ACCEPTABILITY_THRESHOLD_TARGET = np.mean(raw_tgt_acceptability)\n",
    "\n",
    "# print(f\"acceptability_threshold_source: {ACCEPTABILITY_THRESHOLD_SOURCE}\")\n",
    "# print(f\"acceptability_threshold_target: {ACCEPTABILITY_THRESHOLD_TARGET}\")\n",
    "\n",
    "# # Calculate semantic similarity threshold\n",
    "# SIMILARITY_THRESHOLD = np.mean(raw_similarity)\n",
    "# print(f\"similarity_threshold: {SIMILARITY_THRESHOLD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Translation Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Make back-translated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [02:59<00:00, 11.20s/it]\n",
      "100%|██████████| 16/16 [01:53<00:00,  7.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# Helper function to download data for a language\n",
    "def download(model_name):\n",
    "  tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "  model = MarianMTModel.from_pretrained(model_name)\n",
    "  return tokenizer, model\n",
    "\n",
    "# download model for English -> Romance\n",
    "tmp_lang_tokenizer, tmp_lang_model = download('Helsinki-NLP/opus-mt-en-ROMANCE')\n",
    "\n",
    "# download model for Romance -> English\n",
    "src_lang_tokenizer, src_lang_model = download('Helsinki-NLP/opus-mt-ROMANCE-en')\n",
    "\n",
    "# Move models to GPU\n",
    "src_lang_model.to(DEVICE)\n",
    "tmp_lang_model.to(DEVICE)\n",
    "\n",
    "def translate(batch_texts, model, tokenizer, language):\n",
    "    \"\"\"\n",
    "    Translate texts into a target language\n",
    "    \n",
    "    Args:\n",
    "        batch_texts (list): list of texts to be translated\n",
    "        model (model): MarianMTModel\n",
    "        tokenizer (tokenizer): MarianTokenizer\n",
    "        language (str): target language\n",
    "\n",
    "    Returns:\n",
    "        list of translated texts\n",
    "    \"\"\"\n",
    "    formatter_fn = lambda txt: f\">>{language}<<\" + txt if language != \"en\" else txt\n",
    "    formatted_texts = [formatter_fn(txt) for txt in batch_texts]\n",
    "\n",
    "    tokens = tokenizer(formatted_texts, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        translated = model.generate(**tokens, num_return_sequences=2)\n",
    "\n",
    "    translated_texts = tokenizer.batch_decode(translated, skip_special_tokens=True)\n",
    "\n",
    "    del tokens\n",
    "    del translated\n",
    "    cleanup()\n",
    "\n",
    "    return translated_texts\n",
    "\n",
    "def back_translate(texts, language_src, language_dst, batch_size=BATCH_SIZE):\n",
    "    \"\"\"\n",
    "    Implements back translation using batch processing\n",
    "    \n",
    "    Args:\n",
    "        texts (list): list of texts to be back translated\n",
    "        language_src (str): source language\n",
    "        language_dst (list): list of target languages\n",
    "        batch_size (int): batch size\n",
    "\n",
    "    Returns:\n",
    "        list of back translated texts    \n",
    "    \"\"\"\n",
    "    all_back_translated_texts = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        for lang in language_dst:\n",
    "            translated_batch = translate(batch, tmp_lang_model, tmp_lang_tokenizer, lang)\n",
    "            back_translated_batch = translate(translated_batch, src_lang_model, src_lang_tokenizer, language_src)\n",
    "            all_back_translated_texts.extend(back_translated_batch)\n",
    "\n",
    "    return all_back_translated_texts\n",
    "\n",
    "def make_back_translate_df(raw_datasets,\n",
    "                           language_src = \"en\",\n",
    "                           language_dst = ['fr', 'es', 'it']):\n",
    "    \n",
    "    # Create a pandas dataframe\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Back translate the sentences and add to pandas dataframe\n",
    "    df['source_bt'] = back_translate(raw_datasets['train']['source'], language_src, language_dst)\n",
    "    df['target_bt'] = back_translate(raw_datasets['train']['target'], language_src, language_dst)\n",
    "\n",
    "    # Delete rows with duplicate 'source_bt' or 'target_bt'\n",
    "    df = df.drop_duplicates(subset=['source_bt'])\n",
    "    df = df.drop_duplicates(subset=['target_bt'])\n",
    "\n",
    "    # Delete rows that are not distinct from raw_train_dataset['source'] or raw_train_dataset['target']\n",
    "    df = df[~df['source_bt'].isin(raw_datasets['train']['source'])]\n",
    "    df = df[~df['target_bt'].isin(raw_datasets['train']['target'])]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create a dataframe for back translated sentences\n",
    "df_backtranslated = make_back_translate_df()\n",
    "print(f\"Length of df_backtranslated: {len(df_backtranslated)}\")\n",
    "\n",
    "# Delete models and clear GPU memory\n",
    "del tmp_lang_model\n",
    "del tmp_lang_tokenizer\n",
    "del src_lang_model\n",
    "del src_lang_tokenizer\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export df_backtranslated to pickle\n",
    "df_backtranslated.to_pickle(\"../data/processed/df_backtranslated.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add filters to the back-translated dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of df_backtranslated: 3150\n"
     ]
    }
   ],
   "source": [
    "def calc_filters(df, acceptability_threshold_source=ACCEPTABILITY_THRESHOLD_SOURCE, acceptability_threshold_target=ACCEPTABILITY_THRESHOLD_TARGET, similarity_threshold=SIMILARITY_THRESHOLD):\n",
    "    \n",
    "    # Convert source_bt and target_bt to lists\n",
    "    source_bt = df[\"source_bt\"].tolist()\n",
    "    target_bt = df[\"target_bt\"].tolist()\n",
    "    \n",
    "    # Calculate toxicity scores for the candidate sentence pairs\n",
    "    df['source_bt_toxicity'] = calc_tox_acceptability(source_bt, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    df['target_bt_toxicity'] = calc_tox_acceptability(target_bt, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate acceptability scores for the candidate sentence pairs\n",
    "    df['source_bt_acceptability'] = calc_tox_acceptability(source_bt, tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "    df['target_bt_acceptability'] = calc_tox_acceptability(target_bt, tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "\n",
    "    # Calculate similarity scores for the candidate sentence pairs - return the F1 score\n",
    "    df['bt_similarity'] = calc_bert_score(source_bt, target_bt, model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "    # Create filters for the candidate sentence pairs\n",
    "    ## Filter 1: Toxicity\n",
    "    df['f_toxicity'] = (df['source_bt_toxicity'] == 1) & (df['target_bt_toxicity'] == 0)\n",
    "\n",
    "    ## Filter 2: Acceptability\n",
    "    df['f_acceptability'] = (df['source_bt_acceptability'] >= acceptability_threshold_source) & (df['target_bt_acceptability'] >= acceptability_threshold_target)\n",
    "\n",
    "    ## Filter 3: Similarity\n",
    "    df['f_similarity'] = (df['bt_similarity'] >= similarity_threshold)\n",
    "  \n",
    "    # Delete redundant columns\n",
    "    df = df.drop(columns=['source_bt_toxicity', 'target_bt_toxicity', 'source_bt_acceptability', 'target_bt_acceptability', 'bt_similarity'])\n",
    "\n",
    "    return df\n",
    "\n",
    "df_backtranslated = pd.read_pickle(\"../data/processed/df_backtranslated.pkl\")\n",
    "df_backtranslated_with_filters = calc_filters(df_backtranslated)\n",
    "df_backtranslated_with_filters.to_pickle(\"../data/processed/df_backtranslated_with_filters.pkl\")\n",
    "print(f\"Length of df_backtranslated: {len(df_backtranslated)}\")\n",
    "\n",
    "# Delete models and clear GPU memory\n",
    "del tokenizer_toxicity\n",
    "del model_toxicity\n",
    "del tokenizer_acceptability\n",
    "del model_acceptability\n",
    "cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create dataframes with filters applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with all filters: 636\n",
      "Number of rows with no toxicity filter:  1002\n",
      "Number of rows with no acceptability filter:  911\n",
      "Number of rows with no similarity filter:  1606\n"
     ]
    }
   ],
   "source": [
    "def create_filtered_df(df, f_toxicity, f_acceptability, f_similarity, raw_train_dataset=raw_train_dataset):\n",
    "    \"\"\"\n",
    "    Creates a filtered dataframe based on the filters provided, adds the original source and target sentences, removes duplicates, and returns the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (dataframe): dataframe to be filtered\n",
    "        f_toxicity (bool): whether to filter based on toxicity\n",
    "        f_acceptability (bool): whether to filter based on acceptability\n",
    "        f_similarity (bool): whether to filter based on similarity\n",
    "\n",
    "    Returns:\n",
    "        filtered dataframe\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df = df.copy()\n",
    "\n",
    "    # Apply filters\n",
    "    if f_toxicity:\n",
    "        df = df[df['f_toxicity'] == True]\n",
    "    if f_acceptability:\n",
    "        df = df[df['f_acceptability'] == True]\n",
    "    if f_similarity:\n",
    "        df = df[df['f_similarity'] == True]\n",
    "\n",
    "    # Drop filter columns\n",
    "    df = df.drop(columns=['f_toxicity', 'f_acceptability', 'f_similarity'])\n",
    "\n",
    "    # Rename columns\n",
    "    df = df.rename(columns={'source_bt': 'source', 'target_bt': 'target'})\n",
    "    \n",
    "    # Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Create a dataframe applying all filters\n",
    "df_all_filters = create_filtered_df(df=df_backtranslated_with_filters, f_toxicity=True, f_acceptability=True, f_similarity=True)\n",
    "print(f\"Number of rows with all filters: {len(df_all_filters)}\")\n",
    "\n",
    "# Create a dataframe without toxicity filter\n",
    "df_no_toxicity_filter = create_filtered_df(df=df_backtranslated_with_filters, f_toxicity=False, f_acceptability=True, f_similarity=True)\n",
    "print(\"Number of rows with no toxicity filter: \", len(df_no_toxicity_filter))\n",
    "\n",
    "# Create a dataframe without acceptability filter\n",
    "df_no_acceptability_filter = create_filtered_df(df=df_backtranslated_with_filters, f_toxicity=True, f_acceptability=False, f_similarity=True)\n",
    "print(\"Number of rows with no acceptability filter: \", len(df_no_acceptability_filter))\n",
    "\n",
    "# Create a dataframe without similarity filter\n",
    "df_no_similarity_filter = create_filtered_df(df=df_backtranslated_with_filters, f_toxicity=True, f_acceptability=True, f_similarity=False)\n",
    "print(\"Number of rows with no similarity filter: \", len(df_no_similarity_filter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the length of df_all_filters to use as \n",
    "MAX_SAMPLE_SIZE = len(df_all_filters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add original data to augmented data and create dataset dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of augmented dataframe: 636\n",
      "Length of augmented dataframe after sampling: 636\n",
      "Length of original dataframe: 10733\n",
      "Length of augmented dataframe after concatenation: 11369\n",
      "Length of augmented dataframe after dropping duplicates: 11347\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 11347\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 1193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_data(df_aug, raw_datasets, sample_size=MAX_SAMPLE_SIZE, random_state=RANDOM_SEED):\n",
    "    \"\"\"\n",
    "    Add original data to the top of the dataframe and return a dataset dictionary\n",
    "\n",
    "    Args:\n",
    "        df_aug (dataframe): augmented dataframe with 'source' and 'target' columns\n",
    "        raw_datasets (dataset dictionary): original dataset dictionary\n",
    "        aug_sample (int): number of augmented samples to include\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe\n",
    "    df_aug = df_aug.copy()\n",
    "    print(f\"Length of augmented dataframe: {len(df_aug)}\")\n",
    "\n",
    "    # Randomly sample from the augmented dataframe, setting a seed for reproducibility\n",
    "    df_aug = df_aug.sample(n=sample_size, random_state=random_state)\n",
    "    print(f\"Length of augmented dataframe after sampling: {len(df_aug)}\")\n",
    "\n",
    "    # Create a dataframe with original source and target sentences\n",
    "    df_orig = pd.DataFrame({'source': raw_datasets['train']['source'], 'target': raw_datasets['train']['target']})\n",
    "    print(f\"Length of original dataframe: {len(df_orig)}\")\n",
    "\n",
    "    # Concatenate the original source and target sentences into source_bt and target_bt\n",
    "    df_aug = pd.concat([df_orig, df_aug], axis=0)\n",
    "    print(f\"Length of augmented dataframe after concatenation: {len(df_aug)}\")\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_aug = df_aug.drop_duplicates(subset=['source'])\n",
    "    df_aug = df_aug.drop_duplicates(subset=['target'])\n",
    "    print(f\"Length of augmented dataframe after dropping duplicates: {len(df_aug)}\")\n",
    "\n",
    "    # Reset index\n",
    "    df_aug = df_aug.reset_index(drop=True)\n",
    "\n",
    "    # Create a dataset dictionary\n",
    "    dataset_dict = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(df_aug),\n",
    "        \"validation\": raw_datasets[\"validation\"],\n",
    "        \"test\": raw_datasets[\"test\"],\n",
    "    })\n",
    "\n",
    "    return dataset_dict\n",
    "\n",
    "aug_datasets_all_filters = combine_data(df_all_filters, raw_datasets)\n",
    "aug_datasets_no_toxicity_filter = combine_data(df_no_toxicity_filter, raw_datasets)\n",
    "aug_datasets_no_acceptability_filter = combine_data(df_no_acceptability_filter, raw_datasets)\n",
    "aug_datasets_no_similarity_filter = combine_data(df_no_similarity_filter, raw_datasets)\n",
    "\n",
    "# Export augmented datasets to pickle\n",
    "aug_datasets_all_filters.save_to_disk(\"../data/processed/aug_datasets_all_filters\")\n",
    "aug_datasets_no_toxicity_filter.save_to_disk(\"../data/processed/aug_datasets_no_toxicity_filter\")\n",
    "aug_datasets_no_acceptability_filter.save_to_disk(\"../data/processed/aug_datasets_no_acceptability_filter\")\n",
    "aug_datasets_no_similarity_filter.save_to_disk(\"../data/processed/aug_datasets_no_similarity_filter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
