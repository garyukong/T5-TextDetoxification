{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 14:45:00.086738: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 14:45:00.086801: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 14:45:00.086836: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "from numba import cuda\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "from typing import Dict, Union, Optional, Tuple, List, Any\n",
    "import pandas as pd\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgarykong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WAND_NOTEBOOK_NAME\"] = \"w266_final_project_models\"\n",
    "os.environ[\"WANDB_DIR\"] = \"../models/wandb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"w266_final_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Parameters for classification\n",
    "BATCH_SIZE_EVAL = 32\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "\n",
    "# Default parameters for T5 model fine-tuning\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 64\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "AUG_DATASET_ALL_FILTERS_PATH = \"../data/processed/aug_datasets_all_filters\"\n",
    "AUG_DATASET_NO_TOXICITY_FILTER_PATH = \"../data/processed/aug_datasets_no_toxicity_filter\"\n",
    "AUG_DATASET_NO_SIMILARITY_FILTER_PATH = \"../data/processed/aug_datasets_no_similarity_filter\"\n",
    "AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH = \"../data/processed/aug_datasets_no_acceptability_filter\"\n",
    "\n",
    "# Set path for txt file containing best model checkpoints\n",
    "BEST_MODEL_CHECKPOINT_PATH = \"../models/best_model_checkpoints.txt\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and models\n",
    "tokenizer_t5_base = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model_t5_base = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(DEVICE)\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(DEVICE)\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "aug_datasets_all_filters = DatasetDict.load_from_disk(AUG_DATASET_ALL_FILTERS_PATH)\n",
    "aug_datasets_no_acceptability_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH)\n",
    "aug_datasets_no_similarity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_SIMILARITY_FILTER_PATH)\n",
    "aug_datasets_no_toxicity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_TOXICITY_FILTER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the time it takes to run a function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA\": sta_pred,\n",
    "        \"FLU\": acc_pred,\n",
    "        \"SEM\": bert_score_f1,\n",
    "        \"Overall\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    y_pred_delete = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        y_pred_delete.append(text)\n",
    "\n",
    "    return y_pred_delete\n",
    "\n",
    "def bart_detoxifier(text_list):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text using BART\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "    y_pred_bart = pipe_bart(text_list, max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    \n",
    "    return y_pred_bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.5291006187073797,\n",
       " 'STA': 0.6596814752724225,\n",
       " 'FLU': 0.47865131,\n",
       " 'SEM': 0.9118211839944499,\n",
       " 'Overall': 0.6477872136441012}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate DELETE model on validation set\n",
    "delete_preds_val = baseline_detoxifier(raw_datasets[\"validation\"]['source'])\n",
    "delete_val_metrics = evaluate_metrics(raw_datasets[\"validation\"]['target'], delete_preds_val)\n",
    "delete_val_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.7015951162845684,\n",
       " 'STA': 0.9178541492036881,\n",
       " 'FLU': 0.71802455,\n",
       " 'SEM': 0.9451393333184849,\n",
       " 'Overall': 0.8400934599757737}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate BART model on validation set\n",
    "bart_preds_val = bart_detoxifier(raw_datasets[\"validation\"]['source'])\n",
    "bart_val_metrics = evaluate_metrics(raw_datasets[\"validation\"]['target'], bart_preds_val)\n",
    "bart_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to Fine-tune T5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    \"\"\"Adds a prefix to the source sequence in the dataset.\"\"\"\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def create_bidirectional_dataset(datasets, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    if shuffle:\n",
    "        extended_datasets[\"train\"] = extended_datasets[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "        \n",
    "    return extended_datasets\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Preprocess function for T5.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "        batched=True,\n",
    "        remove_columns=[\"source\", \"target\"],\n",
    "    )\n",
    "\n",
    "def post_process(preds, refs, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5.\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        refs (list): list of reference sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "        decoded_refs (list): list of decoded reference sequences\n",
    "    \"\"\"\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    return decoded_preds, decoded_refs\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=False\n",
    "    )\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset to use for testing created using create_bidirectional_datasets\n",
    "                                  For example, raw_datasets_bd[\"validation\"] or raw_datasets_bd[\"test\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = [i for i, input_sentence in enumerate(bd_dataset['source']) if input_sentence.startswith(\"to_neutral\")]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                compute_metrics,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                report_to=\"wandb\",\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Evaluation dataset.\n",
    "        compute_metrics (function): Function to compute metrics. Change this to compute_metrics_bd if using a bi-directional model.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "        report_to (str): Where to report results to. Either \"wandb\" or \"none\".\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    gen_config_path = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(gen_config_path)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=gen_config_path,\n",
    "        fp16=True,\n",
    "        report_to=report_to,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "   \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def training_pipeline(model_name, project_name=\"t5-detox\", model_checkpoint=\"t5-small\", use_validation=True, raw_datasets=raw_datasets, bidirectional=False, shuffle=False, do_train=True):\n",
    "    \"\"\"\n",
    "    Pipeline for training a T5 model. Saves the best model checkpoint to a txt file. Can also be used for evaluating a model (use test set instead of validation set).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to name the output directory and wandb run.\n",
    "        project_name (str): Name of the wandb project.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        use_validation (bool): Whether to use the validation set or not.\n",
    "        raw_datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        bidirectional (bool): Whether to use a bi-directional model or not.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "        do_train (bool): Whether to train the model or not.\n",
    "\n",
    "    Returns:\n",
    "        trainer (Seq2SeqTrainer): Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess dataset (add prefixes / make bidirectional)\n",
    "    if bidirectional:\n",
    "        raw_datasets = create_bidirectional_dataset(raw_datasets, shuffle=shuffle)\n",
    "    else:\n",
    "        raw_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = preprocess_dataset(raw_datasets, tokenizer_t5_small)\n",
    "\n",
    "    # Define compute_metrics function depending on bidirectional or not\n",
    "    if bidirectional and use_validation:\n",
    "        bd_dataset = raw_datasets[\"validation\"]\n",
    "    elif bidirectional and not use_validation:\n",
    "        bd_dataset = raw_datasets[\"test\"]\n",
    "    else:\n",
    "        bd_dataset = None\n",
    "\n",
    "    compute_metrics_fn = partial(compute_metrics_bd, bd_dataset=bd_dataset, shuffled_data=shuffle) if bd_dataset else compute_metrics\n",
    "\n",
    "    # Setup trainer\n",
    "    trainer = setup_trainer(\n",
    "        output_dir_name=model_name,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"] if use_validation else tokenized_datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    if do_train:\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=project_name, name=model_name)\n",
    "        trainer.train()\n",
    "        wandb.finish()\n",
    "\n",
    "        # Get the best checkpoint path for the model\n",
    "        checkpoint_path = trainer.state.best_model_checkpoint\n",
    "\n",
    "        # Save the checkpoint path for the best model\n",
    "        with open(BEST_MODEL_CHECKPOINT_PATH, \"a\") as file:\n",
    "            file.write(f\"{model_name}: {checkpoint_path}\\n\")\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 (Unidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-6f6c4b59a1cdcdb8.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-f65f93e11effeee3.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-f0018b87d088f13b.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_175023-gx6th67u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/gx6th67u' target=\"_blank\">t5_small_unidir</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/gx6th67u' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/gx6th67u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1176' max='3360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1176/3360 07:03 < 13:07, 2.77 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.181400</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.589415</td>\n",
       "      <td>0.833194</td>\n",
       "      <td>0.691846</td>\n",
       "      <td>0.923100</td>\n",
       "      <td>0.774150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.988400</td>\n",
       "      <td>0.929747</td>\n",
       "      <td>0.596775</td>\n",
       "      <td>0.856664</td>\n",
       "      <td>0.695719</td>\n",
       "      <td>0.924650</td>\n",
       "      <td>0.786094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>0.922288</td>\n",
       "      <td>0.596999</td>\n",
       "      <td>0.866723</td>\n",
       "      <td>0.692071</td>\n",
       "      <td>0.925726</td>\n",
       "      <td>0.789648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.918338</td>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.885163</td>\n",
       "      <td>0.703110</td>\n",
       "      <td>0.925951</td>\n",
       "      <td>0.800494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.915330</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.931547</td>\n",
       "      <td>0.604702</td>\n",
       "      <td>0.891869</td>\n",
       "      <td>0.701773</td>\n",
       "      <td>0.926318</td>\n",
       "      <td>0.803306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.938001</td>\n",
       "      <td>0.599846</td>\n",
       "      <td>0.891031</td>\n",
       "      <td>0.706182</td>\n",
       "      <td>0.926642</td>\n",
       "      <td>0.802946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0191dbd410964ab29bb5849450bb6fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▁▄▄▇█▇▅</td></tr><tr><td>eval/FLU</td><td>▁▃▁▆█▅▇</td></tr><tr><td>eval/Overall</td><td>▁▃▄▆█▇▇</td></tr><tr><td>eval/SEM</td><td>▁▄▆▇▇▇█</td></tr><tr><td>eval/STA</td><td>▁▃▄▆█▇▇</td></tr><tr><td>eval/loss</td><td>█▄▂▂▁▄▅</td></tr><tr><td>eval/runtime</td><td>█▇▄▇▁▂▆</td></tr><tr><td>eval/samples_per_second</td><td>▁▂▅▂█▇▃</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▆▁██▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.59985</td></tr><tr><td>eval/FLU</td><td>0.70618</td></tr><tr><td>eval/Overall</td><td>0.80295</td></tr><tr><td>eval/SEM</td><td>0.92664</td></tr><tr><td>eval/STA</td><td>0.89103</td></tr><tr><td>eval/loss</td><td>0.938</td></tr><tr><td>eval/runtime</td><td>44.0632</td></tr><tr><td>eval/samples_per_second</td><td>27.075</td></tr><tr><td>eval/steps_per_second</td><td>0.227</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>1176</td></tr><tr><td>train/learning_rate</td><td>0.00019</td></tr><tr><td>train/loss</td><td>0.7325</td></tr><tr><td>train/total_flos</td><td>758042557218816.0</td></tr><tr><td>train/train_loss</td><td>0.8914</td></tr><tr><td>train/train_runtime</td><td>423.5593</td></tr><tr><td>train/train_samples_per_second</td><td>506.8</td></tr><tr><td>train/train_steps_per_second</td><td>7.933</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_unidir</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/gx6th67u' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/gx6th67u</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_175023-gx6th67u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_ud = training_pipeline(\n",
    "    model_name=\"t5_small_unidir\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=raw_datasets,\n",
    "    bidirectional=False,\n",
    "    shuffle=False,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Bi-directional, No custom loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial without shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9acb52139d7c44079cc6699f2c05a11c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0804410a5e8940d9b0f614d2b1caa7a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2359a5268834171b884ca6819a8c1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_175750-d3jdmltr</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/d3jdmltr' target=\"_blank\">t5_small_bidir_noshuf</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/d3jdmltr' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/d3jdmltr</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3024' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3024/6720 17:53 < 21:53, 2.81 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.487700</td>\n",
       "      <td>1.235429</td>\n",
       "      <td>0.589117</td>\n",
       "      <td>0.784577</td>\n",
       "      <td>0.678174</td>\n",
       "      <td>0.923197</td>\n",
       "      <td>0.751928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>1.167136</td>\n",
       "      <td>0.599725</td>\n",
       "      <td>0.836547</td>\n",
       "      <td>0.691290</td>\n",
       "      <td>0.925403</td>\n",
       "      <td>0.777902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.171800</td>\n",
       "      <td>1.151006</td>\n",
       "      <td>0.600607</td>\n",
       "      <td>0.860855</td>\n",
       "      <td>0.689214</td>\n",
       "      <td>0.925960</td>\n",
       "      <td>0.787498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>1.143344</td>\n",
       "      <td>0.605002</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.692520</td>\n",
       "      <td>0.926318</td>\n",
       "      <td>0.801851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.063400</td>\n",
       "      <td>1.128787</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.704711</td>\n",
       "      <td>0.926202</td>\n",
       "      <td>0.808504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>1.140509</td>\n",
       "      <td>0.602471</td>\n",
       "      <td>0.898575</td>\n",
       "      <td>0.700976</td>\n",
       "      <td>0.925787</td>\n",
       "      <td>0.805277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.989600</td>\n",
       "      <td>1.132926</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.909472</td>\n",
       "      <td>0.705822</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>0.810388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.956400</td>\n",
       "      <td>1.134517</td>\n",
       "      <td>0.600770</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.702148</td>\n",
       "      <td>0.925799</td>\n",
       "      <td>0.805844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.930500</td>\n",
       "      <td>1.137347</td>\n",
       "      <td>0.604801</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.699633</td>\n",
       "      <td>0.926039</td>\n",
       "      <td>0.806531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▁▆▆██▇▆▆█</td></tr><tr><td>eval/FLU</td><td>▁▄▄▅█▇█▇▆</td></tr><tr><td>eval/Overall</td><td>▁▄▅▇█▇█▇█</td></tr><tr><td>eval/SEM</td><td>▁▆▇██▇▇▇▇</td></tr><tr><td>eval/STA</td><td>▁▄▅▇█▇█▇█</td></tr><tr><td>eval/loss</td><td>█▄▂▂▁▂▁▁▂</td></tr><tr><td>eval/runtime</td><td>▁▇█▆▇▄▅▆▅</td></tr><tr><td>eval/samples_per_second</td><td>█▂▁▃▂▅▄▃▃</td></tr><tr><td>eval/steps_per_second</td><td>█▂▁▃▂▅▃▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▃▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.6048</td></tr><tr><td>eval/FLU</td><td>0.69963</td></tr><tr><td>eval/Overall</td><td>0.80653</td></tr><tr><td>eval/SEM</td><td>0.92604</td></tr><tr><td>eval/STA</td><td>0.90109</td></tr><tr><td>eval/loss</td><td>1.13735</td></tr><tr><td>eval/runtime</td><td>86.8202</td></tr><tr><td>eval/samples_per_second</td><td>27.482</td></tr><tr><td>eval/steps_per_second</td><td>0.219</td></tr><tr><td>train/epoch</td><td>9.0</td></tr><tr><td>train/global_step</td><td>3024</td></tr><tr><td>train/learning_rate</td><td>0.00016</td></tr><tr><td>train/loss</td><td>0.9305</td></tr><tr><td>train/total_flos</td><td>1859842187919360.0</td></tr><tr><td>train/train_loss</td><td>1.11014</td></tr><tr><td>train/train_runtime</td><td>1073.8643</td></tr><tr><td>train/train_samples_per_second</td><td>399.79</td></tr><tr><td>train/train_steps_per_second</td><td>6.258</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_bidir_noshuf</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/d3jdmltr' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/d3jdmltr</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_175750-d3jdmltr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_bd_ns = training_pipeline(\n",
    "    model_name=\"t5_small_bidir_noshuf\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=raw_datasets,\n",
    "    bidirectional=True,\n",
    "    shuffle=False,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial with shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1562466267014f4a9d68c59cd1e34457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b56a472bcd54d13b1bc72475ce6f497",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488e5fe773b7432184c39b2a5c868539",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_181606-aw3qsoux</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/aw3qsoux' target=\"_blank\">t5_small_bidir_shuf</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/aw3qsoux' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/aw3qsoux</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3696' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3696/6720 21:44 < 17:48, 2.83 it/s, Epoch 11/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.495300</td>\n",
       "      <td>1.230033</td>\n",
       "      <td>0.596087</td>\n",
       "      <td>0.780386</td>\n",
       "      <td>0.690938</td>\n",
       "      <td>0.922194</td>\n",
       "      <td>0.753998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.258700</td>\n",
       "      <td>1.176923</td>\n",
       "      <td>0.599054</td>\n",
       "      <td>0.850796</td>\n",
       "      <td>0.688809</td>\n",
       "      <td>0.924131</td>\n",
       "      <td>0.782717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.170300</td>\n",
       "      <td>1.152364</td>\n",
       "      <td>0.604108</td>\n",
       "      <td>0.879296</td>\n",
       "      <td>0.707586</td>\n",
       "      <td>0.923768</td>\n",
       "      <td>0.798811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>1.140243</td>\n",
       "      <td>0.607092</td>\n",
       "      <td>0.881811</td>\n",
       "      <td>0.704174</td>\n",
       "      <td>0.925567</td>\n",
       "      <td>0.800091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.064300</td>\n",
       "      <td>1.131201</td>\n",
       "      <td>0.604929</td>\n",
       "      <td>0.887678</td>\n",
       "      <td>0.696949</td>\n",
       "      <td>0.925645</td>\n",
       "      <td>0.800576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.021000</td>\n",
       "      <td>1.137493</td>\n",
       "      <td>0.605037</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.705234</td>\n",
       "      <td>0.926110</td>\n",
       "      <td>0.807377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.986400</td>\n",
       "      <td>1.132168</td>\n",
       "      <td>0.605474</td>\n",
       "      <td>0.914501</td>\n",
       "      <td>0.712354</td>\n",
       "      <td>0.925643</td>\n",
       "      <td>0.814495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.954200</td>\n",
       "      <td>1.133756</td>\n",
       "      <td>0.599532</td>\n",
       "      <td>0.911148</td>\n",
       "      <td>0.709170</td>\n",
       "      <td>0.925135</td>\n",
       "      <td>0.811227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.928600</td>\n",
       "      <td>1.135228</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.903500</td>\n",
       "      <td>1.142378</td>\n",
       "      <td>0.607924</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.704527</td>\n",
       "      <td>0.926615</td>\n",
       "      <td>0.808920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.882100</td>\n",
       "      <td>1.148569</td>\n",
       "      <td>0.605275</td>\n",
       "      <td>0.911987</td>\n",
       "      <td>0.709868</td>\n",
       "      <td>0.925369</td>\n",
       "      <td>0.812897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5ff6376d14049da8e454d858e4763dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.072192…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▁▂▅▆▅▅▅▃█▆▅</td></tr><tr><td>eval/FLU</td><td>▂▁▇▆▃▆█▇█▆▇</td></tr><tr><td>eval/Overall</td><td>▁▄▆▆▆▇███▇█</td></tr><tr><td>eval/SEM</td><td>▁▄▃▆▆▇▆▅██▆</td></tr><tr><td>eval/STA</td><td>▁▅▆▆▇▇███▇█</td></tr><tr><td>eval/loss</td><td>█▄▂▂▁▁▁▁▁▂▂</td></tr><tr><td>eval/runtime</td><td>▃▃▃▃█▃▄▃▄▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▆▅▅▅▁▆▅▆▄█▆</td></tr><tr><td>eval/steps_per_second</td><td>▆▅▅▅▁▆▄▆▄█▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▂▃▃▄▄▅▅▅▅▆▆▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▇▆▅▅▄▃▂▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▄▃▃▂▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.60527</td></tr><tr><td>eval/FLU</td><td>0.70987</td></tr><tr><td>eval/Overall</td><td>0.8129</td></tr><tr><td>eval/SEM</td><td>0.92537</td></tr><tr><td>eval/STA</td><td>0.91199</td></tr><tr><td>eval/loss</td><td>1.14857</td></tr><tr><td>eval/runtime</td><td>85.6147</td></tr><tr><td>eval/samples_per_second</td><td>27.869</td></tr><tr><td>eval/steps_per_second</td><td>0.222</td></tr><tr><td>train/epoch</td><td>11.0</td></tr><tr><td>train/global_step</td><td>3696</td></tr><tr><td>train/learning_rate</td><td>0.00014</td></tr><tr><td>train/loss</td><td>0.8821</td></tr><tr><td>train/total_flos</td><td>2274030923415552.0</td></tr><tr><td>train/train_loss</td><td>1.07043</td></tr><tr><td>train/train_runtime</td><td>1305.0151</td></tr><tr><td>train/train_samples_per_second</td><td>328.977</td></tr><tr><td>train/train_steps_per_second</td><td>5.149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_bidir_shuf</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/aw3qsoux' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/aw3qsoux</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_181606-aw3qsoux/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_bd_s = training_pipeline(\n",
    "    model_name=\"t5_small_bidir_shuf\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=raw_datasets,\n",
    "    bidirectional=True,\n",
    "    shuffle=True,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Data Augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_all_filters/train/cache-15f5ef51cb1d0cdc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_all_filters/validation/cache-840125865a41d0a5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_all_filters/test/cache-f6b0bc561ef0903f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0de6ff9c87f47f9a8b25b1f0d522194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d01fa7c54349a39f06138270fb5eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e57f5667524b448789c9f50d76e026",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_183909-fwu4bhf7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/fwu4bhf7' target=\"_blank\">t5_small_aug_all</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/fwu4bhf7' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/fwu4bhf7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3240' max='6480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3240/6480 12:15 < 12:15, 4.40 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.032800</td>\n",
       "      <td>0.949773</td>\n",
       "      <td>0.590953</td>\n",
       "      <td>0.870075</td>\n",
       "      <td>0.695258</td>\n",
       "      <td>0.924241</td>\n",
       "      <td>0.790121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.879400</td>\n",
       "      <td>0.935219</td>\n",
       "      <td>0.590480</td>\n",
       "      <td>0.887678</td>\n",
       "      <td>0.702628</td>\n",
       "      <td>0.925248</td>\n",
       "      <td>0.798742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.804800</td>\n",
       "      <td>0.929614</td>\n",
       "      <td>0.593576</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.705193</td>\n",
       "      <td>0.925695</td>\n",
       "      <td>0.801976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.746700</td>\n",
       "      <td>0.937668</td>\n",
       "      <td>0.589856</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.705696</td>\n",
       "      <td>0.924643</td>\n",
       "      <td>0.804139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.698400</td>\n",
       "      <td>0.941970</td>\n",
       "      <td>0.590971</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>0.707449</td>\n",
       "      <td>0.926075</td>\n",
       "      <td>0.802317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.658100</td>\n",
       "      <td>0.943837</td>\n",
       "      <td>0.589302</td>\n",
       "      <td>0.911987</td>\n",
       "      <td>0.722105</td>\n",
       "      <td>0.924957</td>\n",
       "      <td>0.812067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.622200</td>\n",
       "      <td>0.961058</td>\n",
       "      <td>0.593418</td>\n",
       "      <td>0.896060</td>\n",
       "      <td>0.704455</td>\n",
       "      <td>0.925313</td>\n",
       "      <td>0.803061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.592200</td>\n",
       "      <td>0.972355</td>\n",
       "      <td>0.593218</td>\n",
       "      <td>0.916178</td>\n",
       "      <td>0.714662</td>\n",
       "      <td>0.925547</td>\n",
       "      <td>0.813157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.565100</td>\n",
       "      <td>0.985333</td>\n",
       "      <td>0.587340</td>\n",
       "      <td>0.906957</td>\n",
       "      <td>0.716202</td>\n",
       "      <td>0.924817</td>\n",
       "      <td>0.808455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.539300</td>\n",
       "      <td>0.998677</td>\n",
       "      <td>0.589139</td>\n",
       "      <td>0.910310</td>\n",
       "      <td>0.716812</td>\n",
       "      <td>0.925013</td>\n",
       "      <td>0.810317</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a8d6a9b2124de283bd05e6316b5538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.072241…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▅▅█▄▅▃██▁▃</td></tr><tr><td>eval/FLU</td><td>▁▃▄▄▄█▃▆▆▇</td></tr><tr><td>eval/Overall</td><td>▁▄▅▅▅█▅█▇▇</td></tr><tr><td>eval/SEM</td><td>▁▅▇▃█▄▅▆▃▄</td></tr><tr><td>eval/STA</td><td>▁▄▄▆▅▇▅█▇▇</td></tr><tr><td>eval/loss</td><td>▃▂▁▂▂▂▄▅▇█</td></tr><tr><td>eval/runtime</td><td>▃█▆▇▅▅▂▅▇▁</td></tr><tr><td>eval/samples_per_second</td><td>▆▁▃▂▄▄▇▄▂█</td></tr><tr><td>eval/steps_per_second</td><td>▆▁▃▂▃▄▇▄▂█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.58914</td></tr><tr><td>eval/FLU</td><td>0.71681</td></tr><tr><td>eval/Overall</td><td>0.81032</td></tr><tr><td>eval/SEM</td><td>0.92501</td></tr><tr><td>eval/STA</td><td>0.91031</td></tr><tr><td>eval/loss</td><td>0.99868</td></tr><tr><td>eval/runtime</td><td>41.8907</td></tr><tr><td>eval/samples_per_second</td><td>28.479</td></tr><tr><td>eval/steps_per_second</td><td>0.239</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>3240</td></tr><tr><td>train/learning_rate</td><td>0.00015</td></tr><tr><td>train/loss</td><td>0.5393</td></tr><tr><td>train/total_flos</td><td>2032557094699008.0</td></tr><tr><td>train/train_loss</td><td>0.71391</td></tr><tr><td>train/train_runtime</td><td>735.5019</td></tr><tr><td>train/train_samples_per_second</td><td>563.18</td></tr><tr><td>train/train_steps_per_second</td><td>8.81</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_aug_all</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/fwu4bhf7' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/fwu4bhf7</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_183909-fwu4bhf7/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_aug_all = training_pipeline(\n",
    "    model_name=\"t5_small_aug_all\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=aug_datasets_all_filters,\n",
    "    bidirectional=False,\n",
    "    shuffle=False,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No acceptability filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_acceptability_filter/train/cache-772ff5af25097272.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_acceptability_filter/validation/cache-840125865a41d0a5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_acceptability_filter/test/cache-f6b0bc561ef0903f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60cfff44992b484f8e1863480c5cfc9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2efbed083e4a4c5e9149d875069140be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504a5ef3b3df4bb3ac4828cdbcf17a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_185209-oa61ulq5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/oa61ulq5' target=\"_blank\">t5_small_aug_noaccept</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/oa61ulq5' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/oa61ulq5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2268' max='6480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2268/6480 08:38 < 16:04, 4.37 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.076800</td>\n",
       "      <td>0.949558</td>\n",
       "      <td>0.589193</td>\n",
       "      <td>0.874267</td>\n",
       "      <td>0.699519</td>\n",
       "      <td>0.924891</td>\n",
       "      <td>0.792427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.916100</td>\n",
       "      <td>0.936198</td>\n",
       "      <td>0.588487</td>\n",
       "      <td>0.884325</td>\n",
       "      <td>0.701398</td>\n",
       "      <td>0.925719</td>\n",
       "      <td>0.796851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.839900</td>\n",
       "      <td>0.929998</td>\n",
       "      <td>0.595734</td>\n",
       "      <td>0.897737</td>\n",
       "      <td>0.708047</td>\n",
       "      <td>0.925757</td>\n",
       "      <td>0.805002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.782200</td>\n",
       "      <td>0.937293</td>\n",
       "      <td>0.594991</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.708011</td>\n",
       "      <td>0.924540</td>\n",
       "      <td>0.805944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.734500</td>\n",
       "      <td>0.944445</td>\n",
       "      <td>0.595025</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.709992</td>\n",
       "      <td>0.925311</td>\n",
       "      <td>0.807508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.692700</td>\n",
       "      <td>0.948201</td>\n",
       "      <td>0.586651</td>\n",
       "      <td>0.899413</td>\n",
       "      <td>0.713622</td>\n",
       "      <td>0.925139</td>\n",
       "      <td>0.804848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.657200</td>\n",
       "      <td>0.964795</td>\n",
       "      <td>0.587344</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.714377</td>\n",
       "      <td>0.925392</td>\n",
       "      <td>0.805523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▃▂█▇▇▁▂</td></tr><tr><td>eval/FLU</td><td>▁▂▅▅▆██</td></tr><tr><td>eval/Overall</td><td>▁▃▇▇█▇▇</td></tr><tr><td>eval/SEM</td><td>▃██▁▅▄▆</td></tr><tr><td>eval/STA</td><td>▁▃▇▇█▇▇</td></tr><tr><td>eval/loss</td><td>▅▂▁▂▄▅█</td></tr><tr><td>eval/runtime</td><td>▁▄▄▄█▅▄</td></tr><tr><td>eval/samples_per_second</td><td>█▄▅▅▁▄▅</td></tr><tr><td>eval/steps_per_second</td><td>█▅▅▅▁▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.58734</td></tr><tr><td>eval/FLU</td><td>0.71438</td></tr><tr><td>eval/Overall</td><td>0.80552</td></tr><tr><td>eval/SEM</td><td>0.92539</td></tr><tr><td>eval/STA</td><td>0.90025</td></tr><tr><td>eval/loss</td><td>0.9648</td></tr><tr><td>eval/runtime</td><td>43.3594</td></tr><tr><td>eval/samples_per_second</td><td>27.514</td></tr><tr><td>eval/steps_per_second</td><td>0.231</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>2268</td></tr><tr><td>train/learning_rate</td><td>0.00019</td></tr><tr><td>train/loss</td><td>0.6572</td></tr><tr><td>train/total_flos</td><td>1448022198288384.0</td></tr><tr><td>train/train_loss</td><td>0.81421</td></tr><tr><td>train/train_runtime</td><td>518.9667</td></tr><tr><td>train/train_samples_per_second</td><td>798.163</td></tr><tr><td>train/train_steps_per_second</td><td>12.486</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_aug_noaccept</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/oa61ulq5' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/oa61ulq5</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_185209-oa61ulq5/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_aug_no_acc = training_pipeline(\n",
    "    model_name=\"t5_small_aug_noaccept\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=aug_datasets_no_acceptability_filter,\n",
    "    bidirectional=False,\n",
    "    shuffle=False,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No similarity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_similarity_filter/train/cache-1a7e006040e3d5f9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_similarity_filter/validation/cache-840125865a41d0a5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_similarity_filter/test/cache-f6b0bc561ef0903f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59e83bd129ae42b2b473e1af5104ad7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb253fd96ba94e09a53c1731371c7e84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd5ce0fa365b4ea085ca542fbe7464f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_190111-r8od64kp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/r8od64kp' target=\"_blank\">t5_small_aug_nosim</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/r8od64kp' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/r8od64kp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3240' max='6480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3240/6480 12:26 < 12:26, 4.34 it/s, Epoch 10/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.311000</td>\n",
       "      <td>0.954922</td>\n",
       "      <td>0.588395</td>\n",
       "      <td>0.849120</td>\n",
       "      <td>0.704302</td>\n",
       "      <td>0.923190</td>\n",
       "      <td>0.782825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.137600</td>\n",
       "      <td>0.933897</td>\n",
       "      <td>0.590768</td>\n",
       "      <td>0.877619</td>\n",
       "      <td>0.705733</td>\n",
       "      <td>0.924761</td>\n",
       "      <td>0.795300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.050900</td>\n",
       "      <td>0.926761</td>\n",
       "      <td>0.596456</td>\n",
       "      <td>0.906119</td>\n",
       "      <td>0.707856</td>\n",
       "      <td>0.924799</td>\n",
       "      <td>0.808270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.983600</td>\n",
       "      <td>0.928941</td>\n",
       "      <td>0.597504</td>\n",
       "      <td>0.904443</td>\n",
       "      <td>0.703530</td>\n",
       "      <td>0.924731</td>\n",
       "      <td>0.806930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.939296</td>\n",
       "      <td>0.592530</td>\n",
       "      <td>0.908634</td>\n",
       "      <td>0.707966</td>\n",
       "      <td>0.925006</td>\n",
       "      <td>0.808554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.877100</td>\n",
       "      <td>0.949860</td>\n",
       "      <td>0.593862</td>\n",
       "      <td>0.917016</td>\n",
       "      <td>0.715056</td>\n",
       "      <td>0.925457</td>\n",
       "      <td>0.813681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.834100</td>\n",
       "      <td>0.964241</td>\n",
       "      <td>0.592720</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.706382</td>\n",
       "      <td>0.926139</td>\n",
       "      <td>0.806490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.796000</td>\n",
       "      <td>0.974892</td>\n",
       "      <td>0.591599</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.719671</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.814390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.763300</td>\n",
       "      <td>0.985350</td>\n",
       "      <td>0.589331</td>\n",
       "      <td>0.909472</td>\n",
       "      <td>0.713934</td>\n",
       "      <td>0.924521</td>\n",
       "      <td>0.809346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.734800</td>\n",
       "      <td>0.995770</td>\n",
       "      <td>0.597483</td>\n",
       "      <td>0.911148</td>\n",
       "      <td>0.715207</td>\n",
       "      <td>0.925603</td>\n",
       "      <td>0.812118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8838e272c3f451097e83826f80e5a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.072210…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▁▃▇█▄▅▄▃▂█</td></tr><tr><td>eval/FLU</td><td>▁▂▃▁▃▆▂█▆▆</td></tr><tr><td>eval/Overall</td><td>▁▄▇▆▇█▆█▇▇</td></tr><tr><td>eval/SEM</td><td>▁▅▅▅▅▆█▅▄▇</td></tr><tr><td>eval/STA</td><td>▁▄▇▇▇█▇█▇▇</td></tr><tr><td>eval/loss</td><td>▄▂▁▁▂▃▅▆▇█</td></tr><tr><td>eval/runtime</td><td>▇█▅▁█▃▅▆▁▃</td></tr><tr><td>eval/samples_per_second</td><td>▂▁▄█▁▆▄▃█▆</td></tr><tr><td>eval/steps_per_second</td><td>▂▁▄█▁▆▄▃█▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▆▅▄▃▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▃▃▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.59748</td></tr><tr><td>eval/FLU</td><td>0.71521</td></tr><tr><td>eval/Overall</td><td>0.81212</td></tr><tr><td>eval/SEM</td><td>0.9256</td></tr><tr><td>eval/STA</td><td>0.91115</td></tr><tr><td>eval/loss</td><td>0.99577</td></tr><tr><td>eval/runtime</td><td>43.5657</td></tr><tr><td>eval/samples_per_second</td><td>27.384</td></tr><tr><td>eval/steps_per_second</td><td>0.23</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>3240</td></tr><tr><td>train/learning_rate</td><td>0.00015</td></tr><tr><td>train/loss</td><td>0.7348</td></tr><tr><td>train/total_flos</td><td>2054954841145344.0</td></tr><tr><td>train/train_loss</td><td>0.94158</td></tr><tr><td>train/train_runtime</td><td>746.3255</td></tr><tr><td>train/train_samples_per_second</td><td>555.013</td></tr><tr><td>train/train_steps_per_second</td><td>8.683</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_aug_nosim</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/r8od64kp' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/r8od64kp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_190111-r8od64kp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_aug_no_sim = training_pipeline(\n",
    "    model_name=\"t5_small_aug_nosim\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=aug_datasets_no_similarity_filter,\n",
    "    bidirectional=False,\n",
    "    shuffle=False,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No toxicity filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_toxicity_filter/train/cache-30cb93e69121ab8a.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_toxicity_filter/validation/cache-840125865a41d0a5.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/aug_datasets_no_toxicity_filter/test/cache-f6b0bc561ef0903f.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d146e4adadc94787a87bb9b471bb7a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/20711 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd4fb202c14442ca06e2f568c6ab591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8504d78fced14458b0eaf5d8facff093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231110_191400-l3taknln</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/t5-detox/runs/l3taknln' target=\"_blank\">t5_small_aug_notox</a></strong> to <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/t5-detox' target=\"_blank\">https://wandb.ai/garykong/t5-detox</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/t5-detox/runs/l3taknln' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/l3taknln</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2592' max='6480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2592/6480 09:47 < 14:42, 4.41 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta</th>\n",
       "      <th>Flu</th>\n",
       "      <th>Sem</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.012500</td>\n",
       "      <td>0.952226</td>\n",
       "      <td>0.590286</td>\n",
       "      <td>0.843252</td>\n",
       "      <td>0.696211</td>\n",
       "      <td>0.924479</td>\n",
       "      <td>0.779496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.869700</td>\n",
       "      <td>0.941054</td>\n",
       "      <td>0.594841</td>\n",
       "      <td>0.863370</td>\n",
       "      <td>0.701323</td>\n",
       "      <td>0.925570</td>\n",
       "      <td>0.789695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.797800</td>\n",
       "      <td>0.929157</td>\n",
       "      <td>0.595369</td>\n",
       "      <td>0.886002</td>\n",
       "      <td>0.695625</td>\n",
       "      <td>0.926623</td>\n",
       "      <td>0.797924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.742600</td>\n",
       "      <td>0.938356</td>\n",
       "      <td>0.590351</td>\n",
       "      <td>0.876781</td>\n",
       "      <td>0.699310</td>\n",
       "      <td>0.925064</td>\n",
       "      <td>0.793658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.695900</td>\n",
       "      <td>0.941544</td>\n",
       "      <td>0.600925</td>\n",
       "      <td>0.886840</td>\n",
       "      <td>0.704962</td>\n",
       "      <td>0.926235</td>\n",
       "      <td>0.801160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.656300</td>\n",
       "      <td>0.936969</td>\n",
       "      <td>0.598625</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.713698</td>\n",
       "      <td>0.925555</td>\n",
       "      <td>0.804659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.621500</td>\n",
       "      <td>0.967940</td>\n",
       "      <td>0.595544</td>\n",
       "      <td>0.887678</td>\n",
       "      <td>0.700756</td>\n",
       "      <td>0.925482</td>\n",
       "      <td>0.799428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.970850</td>\n",
       "      <td>0.591349</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.708442</td>\n",
       "      <td>0.925324</td>\n",
       "      <td>0.802106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>▁▄▄▁█▆▄▂</td></tr><tr><td>eval/FLU</td><td>▁▃▁▂▅█▃▆</td></tr><tr><td>eval/Overall</td><td>▁▄▆▅▇█▇▇</td></tr><tr><td>eval/SEM</td><td>▁▅█▃▇▅▄▄</td></tr><tr><td>eval/STA</td><td>▁▄▇▆▇█▇█</td></tr><tr><td>eval/loss</td><td>▅▃▁▃▃▂██</td></tr><tr><td>eval/runtime</td><td>▁▆▄▂▇▃▆█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▅▇▂▆▃▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▅▇▂▆▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▄▃▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/BLEU</td><td>0.59135</td></tr><tr><td>eval/FLU</td><td>0.70844</td></tr><tr><td>eval/Overall</td><td>0.80211</td></tr><tr><td>eval/SEM</td><td>0.92532</td></tr><tr><td>eval/STA</td><td>0.89271</td></tr><tr><td>eval/loss</td><td>0.97085</td></tr><tr><td>eval/runtime</td><td>43.8182</td></tr><tr><td>eval/samples_per_second</td><td>27.226</td></tr><tr><td>eval/steps_per_second</td><td>0.228</td></tr><tr><td>train/epoch</td><td>8.0</td></tr><tr><td>train/global_step</td><td>2592</td></tr><tr><td>train/learning_rate</td><td>0.00018</td></tr><tr><td>train/loss</td><td>0.5902</td></tr><tr><td>train/total_flos</td><td>1621345350156288.0</td></tr><tr><td>train/train_loss</td><td>0.74833</td></tr><tr><td>train/train_runtime</td><td>587.706</td></tr><tr><td>train/train_samples_per_second</td><td>704.808</td></tr><tr><td>train/train_steps_per_second</td><td>11.026</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5_small_aug_notox</strong> at: <a href='https://wandb.ai/garykong/t5-detox/runs/l3taknln' target=\"_blank\">https://wandb.ai/garykong/t5-detox/runs/l3taknln</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231110_191400-l3taknln/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer_aug_no_tox = training_pipeline(\n",
    "    model_name=\"t5_small_aug_notox\",\n",
    "    project_name=\"t5-detox\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    use_validation=True,\n",
    "    raw_datasets=aug_datasets_no_toxicity_filter,\n",
    "    bidirectional=False,\n",
    "    shuffle=False,\n",
    "    do_train=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 (Unidirectional, with Negative Lexically Constrained Decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the IDs using the appropriate tokenizer for the bad words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_words_list(dataset, tokenizer=tokenizer_toxicity, model=model_toxicity, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Gets the top k bad words for each sentence in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of sentences.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use (toxicity classifier).\n",
    "        model (PreTrainedModel): Model to use (toxicity classifier).\n",
    "        num_layers (int): Number of layers to use.\n",
    "        top_k (int): Number of bad words to return.\n",
    "\n",
    "    Returns:\n",
    "        bad_words_list (list): List of lists of bad words.\n",
    "    \"\"\"    \n",
    "    bad_words_list = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        # Tokenize sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        # Get attention scores\n",
    "        attention = model(input_ids, output_attentions=True)['attentions']\n",
    "\n",
    "        # Get the last num_layers layer attention scores and average them\n",
    "        attention = torch.stack(attention[-num_layers:]).mean(0)\n",
    "\n",
    "        # Average across each head\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Sum each row to get the attention score for each token\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Exclude separator tokens and punctuation\n",
    "        token_list = input_ids.squeeze().tolist()\n",
    "        punctuation_ids = {tokenizer.convert_tokens_to_ids(token) for token in string.punctuation}\n",
    "        exclude_ids = set([tokenizer.bos_token_id, tokenizer.eos_token_id]) | punctuation_ids\n",
    "\n",
    "        valid_indices = [i for i, token_id in enumerate(token_list) if token_id not in exclude_ids]\n",
    "\n",
    "        # Filter out the valid indices from the attention scores\n",
    "        valid_attention = attention.squeeze()[valid_indices]\n",
    "\n",
    "        # Get the indices of the top k tokens with the highest attention scores among valid tokens\n",
    "        top_indices = valid_attention.topk(top_k).indices.tolist()\n",
    "        top_token_indices = [valid_indices[i] for i in top_indices]\n",
    "\n",
    "        # Decode the tokens\n",
    "        bad_words = [tokenizer.decode(token_list[index]).strip() for index in top_token_indices]\n",
    "\n",
    "        bad_words_list.append(bad_words)\n",
    "\n",
    "    return bad_words_list\n",
    "\n",
    "def get_bad_word_ids(dataset,\n",
    "                     tokenizer_toxicity=tokenizer_toxicity,\n",
    "                     model_toxicity=model_toxicity,\n",
    "                     tokenizer_t5=tokenizer_t5_small,\n",
    "                     num_layers=3,\n",
    "                     top_k=3):\n",
    "    \n",
    "    # Get list of bad words as identified using attention from toxicity classifier\n",
    "    bad_words_list = get_bad_words_list(dataset, tokenizer_toxicity, model_toxicity, num_layers, top_k)\n",
    "\n",
    "    # Convert each list of bad words to a string\n",
    "    bad_words_str_list = [\" \".join(bad_words) for bad_words in bad_words_list]\n",
    "\n",
    "    # Encode the bad words using the T5 tokenizer encode\n",
    "    bad_word_ids = [tokenizer_t5.encode(bad_words, add_special_tokens=False) for bad_words in bad_words_str_list]\n",
    "\n",
    "    return bad_word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[720, 524, 3, 15, 51, 3, 189, 32],\n",
       " [96, 3, 89, 4636, 1131, 26],\n",
       " [720, 524, 25, 3, 9],\n",
       " [3, 89, 4636, 53, 278, 34],\n",
       " [3, 89, 15318, 3, 31, 7, 33],\n",
       " [3, 834, 3, 89, 4636, 3, 18],\n",
       " [3, 7, 10536, 1466, 3, 15294],\n",
       " [3, 7, 10536, 16, 16],\n",
       " [3, 4636, 7, 3, 7, 10536, 3, 7],\n",
       " [16497, 110, 12032, 872],\n",
       " [3, 7, 10536, 224, 3, 9],\n",
       " [3, 7, 10536, 3, 9, 888],\n",
       " [3, 89, 4636, 363, 24],\n",
       " [34, 4488, 79],\n",
       " [3, 7, 10536, 24, 16],\n",
       " [3, 7, 10536, 3, 189, 32, 466],\n",
       " [38, 7, 470, 10657],\n",
       " [3, 7, 10536, 14732, 405],\n",
       " [3, 4636, 7, 225, 3, 89],\n",
       " [3, 89, 4636, 38, 17112],\n",
       " [38, 7, 3, 7137, 470],\n",
       " [1001, 7, 73, 8715, 9357],\n",
       " [3, 7, 10536, 2219, 3, 9],\n",
       " [3, 7, 10536, 48, 103],\n",
       " [16497, 3, 7, 10536, 24],\n",
       " [3, 89, 4636, 77, 25, 28],\n",
       " [148, 43, 1780],\n",
       " [15708, 100, 1176],\n",
       " [3, 7, 10536, 3, 172, 14924],\n",
       " [3, 89, 4636, 53, 94, 7, 59],\n",
       " [38, 7, 39, 56],\n",
       " [15708, 1670, 3, 157],\n",
       " [3, 26, 3142, 25, 130],\n",
       " [39, 6660, 7683],\n",
       " [3, 7, 10536, 16497, 25],\n",
       " [3, 89, 4636, 53, 81, 3, 35],\n",
       " [3, 89, 4636, 53, 22784, 5301],\n",
       " [5591, 3, 23, 39],\n",
       " [3, 7, 10536, 59, 3, 9],\n",
       " [17227, 3, 51, 75, 615],\n",
       " [696, 13721, 25],\n",
       " [38, 7, 8, 25],\n",
       " [3, 7059, 3, 834, 272],\n",
       " [3, 89, 4636, 1429, 1429],\n",
       " [3, 89, 4636, 95, 3, 13366],\n",
       " [25851, 25, 1176],\n",
       " [3, 89, 4636, 53, 270, 19, 29],\n",
       " [754, 9348, 647],\n",
       " [3, 89, 4636, 10287, 25],\n",
       " [3, 4636, 53, 273, 1858],\n",
       " [21562, 19, 789],\n",
       " [3, 89, 4636, 53, 1108, 3, 7, 10536],\n",
       " [3, 7, 10536, 43, 29, 3, 31, 17],\n",
       " [3, 7, 10536, 3359, 3, 23],\n",
       " [3, 89, 4636, 53, 25, 33],\n",
       " [3, 9, 4102, 3, 89, 3, 32, 17],\n",
       " [3, 89, 15318, 3, 15, 51, 95],\n",
       " [34, 4488, 3, 23],\n",
       " [3, 89, 4636, 53, 79, 4073],\n",
       " [3, 89, 4636, 8, 414],\n",
       " [3, 89, 4636, 77, 25, 819],\n",
       " [2753, 29651, 19],\n",
       " [42, 29, 23, 9, 135, 135],\n",
       " [38, 7, 3, 3158, 112],\n",
       " [3, 89, 4636, 53, 3, 3158, 146, 221],\n",
       " [3, 89, 4636, 53, 16471, 175],\n",
       " [29651, 20855, 59],\n",
       " [3, 7, 10536, 612, 34],\n",
       " [3, 7, 10536, 12792, 70],\n",
       " [3, 89, 4636, 30, 17, 3, 354],\n",
       " [3, 7, 4636, 7, 3, 26, 3142, 3, 89],\n",
       " [123, 29, 17, 140, 34],\n",
       " [38, 7, 19206, 26, 112],\n",
       " [3, 89, 4636, 53, 33, 3, 31, 60],\n",
       " [3, 89, 4636, 103, 66],\n",
       " [3, 89, 4636, 118, 2840],\n",
       " [3, 89, 4636, 77, 3, 89, 4636, 77, 720, 524],\n",
       " [3, 89, 4636, 53, 3, 23, 762],\n",
       " [3, 7, 10536, 1318, 3, 29],\n",
       " [38, 7, 376, 11126],\n",
       " [3, 89, 15318, 641, 19],\n",
       " [3, 89, 4636, 53, 15306, 744],\n",
       " [3, 89, 4636, 30, 326],\n",
       " [26557, 25, 148],\n",
       " [25851, 7, 3513, 94, 7],\n",
       " [38, 7, 16497, 3, 88],\n",
       " [3, 157, 3, 89, 3, 476],\n",
       " [3, 7, 10536, 6679, 3, 32, 115],\n",
       " [3, 89, 15318, 95, 4726],\n",
       " [3, 89, 4636, 25, 37],\n",
       " [38, 7, 82, 142],\n",
       " [29651, 3, 7, 175],\n",
       " [11178, 15, 26, 2710, 17261],\n",
       " [3, 26, 3142, 696, 47],\n",
       " [3, 4636, 53, 377, 7927],\n",
       " [412, 3, 76, 3, 7, 10536],\n",
       " [38, 7, 3, 1171, 7, 3641],\n",
       " [3, 89, 4636, 53, 589, 8],\n",
       " [3, 7, 10536, 3, 31, 7, 34],\n",
       " [377, 4636, 3, 23, 34],\n",
       " [377, 4636, 4244, 1024, 372],\n",
       " [3, 89, 4636, 16, 9, 28],\n",
       " [27, 26, 3, 23, 32, 17, 7, 770],\n",
       " [2241, 148, 11693, 1765],\n",
       " [4550, 159, 22449, 3, 9],\n",
       " [38, 7, 3, 450, 1429],\n",
       " [3, 89, 4636, 8524, 51, 33],\n",
       " [3, 7, 10536, 39, 25],\n",
       " [18554, 19, 12705, 7],\n",
       " [353, 3, 89, 4636, 53, 466],\n",
       " [3, 89, 4636, 3, 23, 16497],\n",
       " [3, 7, 10536, 10653, 68],\n",
       " [3, 89, 4636, 3, 834, 125],\n",
       " [38, 7, 405, 8],\n",
       " [37, 3, 23, 26, 3, 23, 9798],\n",
       " [3, 89, 4636, 53, 789, 571],\n",
       " [720, 524, 1044, 39],\n",
       " [30, 8030, 59],\n",
       " [3, 7, 10536, 641, 815],\n",
       " [3, 7, 10536, 3, 89, 4636, 3, 3158],\n",
       " [3, 7, 10536, 16497, 943],\n",
       " [3, 63, 32, 38, 7, 3, 14547],\n",
       " [3, 23, 16497, 3, 29, 23, 1572, 965],\n",
       " [3, 89, 4636, 53, 955, 25],\n",
       " [3, 89, 4636, 25, 33],\n",
       " [34, 4488, 16, 9],\n",
       " [720, 524, 3, 17065, 1429],\n",
       " [3, 7, 10536, 54, 156],\n",
       " [34, 4488, 3, 23],\n",
       " [3, 7, 10536, 3, 24303, 107, 3, 23, 26],\n",
       " [3, 89, 4636, 53, 62, 363],\n",
       " [3, 89, 4636, 256, 664],\n",
       " [13721, 130, 59],\n",
       " [3, 7, 10536, 2508, 8],\n",
       " [17227, 3, 8665, 25],\n",
       " [17056, 363, 3, 9],\n",
       " [3, 26, 3142, 39, 6179],\n",
       " [38, 7, 1871, 82],\n",
       " [18554, 286, 19],\n",
       " [3, 7, 10536, 48, 1615],\n",
       " [9078, 142, 1584],\n",
       " [18554, 363, 8],\n",
       " [3, 7, 10536, 20237, 152, 1466],\n",
       " [3, 89, 4636, 8, 3, 157],\n",
       " [17056, 8400, 442],\n",
       " [3, 89, 4636, 53, 1624, 26, 155, 19],\n",
       " [38, 7, 71, 4503],\n",
       " [3, 7, 10536, 16497, 270],\n",
       " [38, 7, 6545, 8, 15],\n",
       " [13721, 3, 5828, 8],\n",
       " [3, 7, 10536, 27, 13],\n",
       " [3, 7, 10536, 2483, 703],\n",
       " [720, 524, 6545, 3, 23],\n",
       " [3, 7, 10536, 3, 9, 133],\n",
       " [29651, 100, 16, 30916, 3772],\n",
       " [3, 89, 4636, 53, 3, 23, 3127],\n",
       " [3, 7, 10536, 148, 3, 23],\n",
       " [3, 26, 3142, 522, 39],\n",
       " [3, 89, 4636, 53, 38, 7, 33],\n",
       " [1874, 2241, 216],\n",
       " [3, 89, 4636, 24, 3, 32, 15],\n",
       " [3, 89, 4636, 53, 388, 3, 31, 7],\n",
       " [4488, 34, 56],\n",
       " [13721, 275, 3, 23, 26],\n",
       " [25, 3, 7, 10536, 1615],\n",
       " [2615, 52, 3, 9, 3, 29, 17],\n",
       " [34, 4488, 3, 23],\n",
       " [3, 89, 4636, 16497, 3, 9],\n",
       " [3, 89, 4636, 53, 140, 572],\n",
       " [17227, 25, 2048],\n",
       " [3, 89, 4636, 53, 3, 89, 4636, 53, 96],\n",
       " [720, 524, 3, 40, 173, 8434, 7, 10536],\n",
       " [21562, 148, 1405],\n",
       " [3, 89, 4636, 25, 39],\n",
       " [3, 19230, 3, 2951, 720],\n",
       " [3, 7, 10536, 70, 275],\n",
       " [3, 7, 10536, 16, 326],\n",
       " [3, 89, 4636, 53, 3, 32, 521, 301],\n",
       " [3, 7, 10536, 571, 3971],\n",
       " [16, 30916, 29, 17, 3, 9, 23, 23, 19],\n",
       " [3, 7, 10536, 3, 76, 3, 76],\n",
       " [6802, 10215, 3, 12488, 157],\n",
       " [148, 25851, 33],\n",
       " [38, 7, 8238, 3, 18],\n",
       " [28617, 3, 102, 75, 165],\n",
       " [3, 89, 4636, 77, 3, 102, 3, 102],\n",
       " [38, 7, 112, 45],\n",
       " [3, 89, 4636, 3052, 8],\n",
       " [1429, 38, 7, 39],\n",
       " [30, 281, 2753],\n",
       " [148, 3, 27826, 33],\n",
       " [3, 7, 10536, 3, 102, 3, 18],\n",
       " [11178, 15, 26, 46, 79],\n",
       " [3, 7, 10536, 24, 27],\n",
       " [3, 7, 10536, 136, 2514],\n",
       " [720, 524, 3, 7, 10536, 48],\n",
       " [3, 986, 7, 3, 60, 17, 506],\n",
       " [3, 7315, 7922, 710],\n",
       " [3, 89, 4636, 19, 8],\n",
       " [3, 7, 10536, 141, 70],\n",
       " [18754, 2102, 1350],\n",
       " [148, 33, 5591],\n",
       " [34, 3, 89, 4636, 53, 4488],\n",
       " [1584, 3822, 288, 39, 5851],\n",
       " [3, 7059, 1190, 2198],\n",
       " [15375, 7, 38, 7, 8032],\n",
       " [19329, 15, 26, 5464, 27],\n",
       " [73, 17, 1806, 205],\n",
       " [3, 89, 4636, 25, 3, 23],\n",
       " [3, 7, 10536, 1076, 7285],\n",
       " [3, 89, 4636, 53, 148, 82],\n",
       " [3, 32, 17, 7, 3103, 600],\n",
       " [3, 7, 10536, 3, 9, 25],\n",
       " [3, 7, 10536, 69, 363],\n",
       " [4792, 861, 19],\n",
       " [3, 7, 10536, 36, 9121],\n",
       " [96, 3, 89, 4636, 96],\n",
       " [328, 36, 66],\n",
       " [3, 89, 15318, 39, 4516],\n",
       " [18554, 59, 531],\n",
       " [13721, 1318, 4363],\n",
       " [38, 7, 14734, 4244, 1024],\n",
       " [3, 7, 10536, 33, 276],\n",
       " [3, 7, 10536, 275, 1466],\n",
       " [3, 89, 4636, 96, 25],\n",
       " [8030, 30, 24],\n",
       " [25851, 7, 1445, 21],\n",
       " [3, 7, 10536, 465, 609, 447],\n",
       " [3, 7, 10536, 8, 19],\n",
       " [2523, 16, 16, 30916, 29, 17],\n",
       " [3, 89, 4636, 3, 5937, 3, 9, 122, 7],\n",
       " [25, 3, 7, 10536, 3, 10769],\n",
       " [3, 7, 10536, 25, 7019],\n",
       " [3, 89, 4636, 53, 38, 7, 8034],\n",
       " [1335, 25575, 1548],\n",
       " [25851, 7, 59, 4336, 7],\n",
       " [1977, 363, 3, 32],\n",
       " [3, 7, 10536, 3, 29, 3, 76, 107],\n",
       " [38, 7, 5949, 15, 26, 1077],\n",
       " [3, 89, 15318, 95, 33],\n",
       " [13721, 538, 48],\n",
       " [10939, 29, 3, 7, 10536, 48],\n",
       " [3, 7, 10536, 24, 451],\n",
       " [377, 4636, 8, 682],\n",
       " [3, 7, 10536, 25, 3, 89, 4636],\n",
       " [3, 7, 10536, 33, 3, 7, 10536],\n",
       " [3, 89, 4636, 38, 7, 3, 52],\n",
       " [38, 7, 625, 451],\n",
       " [3, 7, 10536, 27, 3567],\n",
       " [17261, 2523, 8],\n",
       " [25851, 7623, 112],\n",
       " [17261, 38, 3, 115],\n",
       " [3, 89, 4636, 53, 165, 62],\n",
       " [720, 524, 16497, 3, 76],\n",
       " [3, 89, 4636, 23601, 12],\n",
       " [3, 89, 4636, 53, 125, 1229],\n",
       " [3, 7, 10536, 466, 895],\n",
       " [3, 89, 4636, 696, 95],\n",
       " [3, 7, 10536, 24, 13315],\n",
       " [3, 26, 3142, 6952, 3, 21516, 9],\n",
       " [3, 7, 10536, 148, 54],\n",
       " [3, 7059, 17227, 272],\n",
       " [123, 29, 17, 34, 14371, 9],\n",
       " [3, 76, 3, 89, 4636, 25],\n",
       " [720, 524, 38, 7, 25],\n",
       " [3, 7, 10536, 3, 547, 43],\n",
       " [3, 7, 10536, 25, 1615],\n",
       " [34, 4488, 3, 28848],\n",
       " [3, 7, 10536, 112, 391],\n",
       " [3, 89, 4636, 53, 769, 1309],\n",
       " [3, 89, 4636, 53, 25, 3856, 5983, 26],\n",
       " [17227, 3, 89, 4636, 53, 2056],\n",
       " [3, 89, 4636, 2898, 13418],\n",
       " [3, 89, 4636, 53, 3, 23, 275],\n",
       " [3, 7, 10536, 47, 13654],\n",
       " [3, 89, 4636, 53, 3500, 216],\n",
       " [15708, 48, 852],\n",
       " [3, 89, 4636, 3, 9, 3, 9, 32],\n",
       " [3, 89, 4636, 8, 3, 10769],\n",
       " [3, 89, 4636, 3, 9, 1615],\n",
       " [71, 23713, 600, 32, 17],\n",
       " [3, 7, 10536, 8, 256],\n",
       " [3, 23, 9798, 3, 23, 26, 11518],\n",
       " [34, 4488, 65, 29],\n",
       " [3, 61, 41, 3, 7, 4636],\n",
       " [47, 18554, 3, 9],\n",
       " [34, 1222, 6660, 328],\n",
       " [3, 7, 10536, 3702, 880],\n",
       " [96, 123, 29, 17, 96],\n",
       " [10381, 277, 28617, 28617],\n",
       " [3, 89, 4636, 38, 3, 31, 60],\n",
       " [8030, 30, 484],\n",
       " [3, 7, 10536, 3, 26, 3, 29],\n",
       " [377, 4636, 25, 388],\n",
       " [68, 17, 1473, 95],\n",
       " [96, 3, 89, 4636, 3, 63],\n",
       " [25, 10950, 25863, 3202],\n",
       " [3, 26, 3, 7, 10536, 270],\n",
       " [3, 89, 4636, 273, 11475],\n",
       " [3, 89, 4636, 53, 2783, 15, 19],\n",
       " [17081, 3, 9, 428],\n",
       " [21562, 47, 332],\n",
       " [3, 7, 10536, 3, 23, 3, 7820],\n",
       " [3, 12488, 19743, 3, 157, 7],\n",
       " [3, 7, 10536, 16497, 6979],\n",
       " [21562, 3, 88, 1584, 3822, 288],\n",
       " [3, 89, 4636, 53, 68, 27],\n",
       " [3, 89, 4636, 53, 15708, 4886],\n",
       " [3, 7, 10536, 376, 68],\n",
       " [720, 524, 3, 23, 3, 32, 7],\n",
       " [3, 7, 10536, 48, 230],\n",
       " [3, 7, 10536, 1131, 26, 155, 11],\n",
       " [38, 7, 887, 79],\n",
       " [13721, 485, 19, 304],\n",
       " [282, 7, 112, 28],\n",
       " [5562, 3, 22463, 75, 5591],\n",
       " [3, 7, 10536, 16497, 270],\n",
       " [34, 1222, 3, 40, 6660],\n",
       " [3, 89, 4636, 53, 81, 4394],\n",
       " [3, 89, 4636, 8, 8698, 75, 52],\n",
       " [720, 524, 255, 779],\n",
       " [3, 7, 10536, 100, 140],\n",
       " [564, 3, 89, 4636, 53, 696],\n",
       " [3, 89, 4636, 53, 1231, 78],\n",
       " [3, 89, 4636, 38, 132],\n",
       " [3, 89, 4636, 53, 27197, 1922],\n",
       " [3, 89, 4636, 3424, 3, 23],\n",
       " [256, 21562, 1670],\n",
       " [769, 1271, 26, 155, 3, 7, 10536, 48],\n",
       " [3, 76, 38, 7, 3, 184],\n",
       " [38, 7, 128, 1615],\n",
       " [31973, 1329, 3, 23, 1608, 8],\n",
       " [3, 26, 3142, 363, 589],\n",
       " [38, 7, 30, 17, 25],\n",
       " [3, 89, 4636, 38, 1634, 53],\n",
       " [3, 89, 4636, 135, 66],\n",
       " [38, 7, 4284, 3853],\n",
       " [3, 89, 4636, 100, 8],\n",
       " [3, 7, 10536, 4195, 24],\n",
       " [34, 4488, 3, 23],\n",
       " [3, 7, 10536, 24, 4244, 1024],\n",
       " [3, 89, 4636, 53, 3, 226, 3, 102],\n",
       " [22641, 3, 7, 10536, 363],\n",
       " [38, 7, 3, 8665, 58, 3, 63, 32],\n",
       " [3, 89, 4636, 77, 3, 7, 10536, 48],\n",
       " [38, 7, 385, 2093],\n",
       " [3, 7, 10536, 3, 23, 466],\n",
       " [123, 29, 17, 135, 223],\n",
       " [25851, 7, 178, 1615],\n",
       " [29651, 3, 7, 2891],\n",
       " [3, 7, 10536, 3, 40, 173, 3858],\n",
       " [3, 7, 10536, 3, 63, 9, 3, 89, 265],\n",
       " [3, 26, 3142, 96, 600],\n",
       " [720, 524, 3, 7315, 664],\n",
       " [3, 7, 10536, 3, 9, 1517],\n",
       " [3, 89, 4636, 8, 19],\n",
       " [3, 7, 10536, 3, 7, 10536, 3, 3158],\n",
       " [3, 7, 10536, 11475, 24],\n",
       " [38, 7, 38, 7, 27],\n",
       " [3, 7, 10536, 3, 9, 25],\n",
       " [38, 7, 100, 47],\n",
       " [38, 7, 407, 4411, 82],\n",
       " [59, 33, 3, 4477, 15],\n",
       " [38, 7, 39, 3, 18],\n",
       " [3, 2951, 720, 258],\n",
       " [3, 40, 23, 291, 16, 8],\n",
       " [13721, 3, 10, 34],\n",
       " [3, 7, 10536, 8, 11],\n",
       " [96, 38, 7, 819],\n",
       " [3, 89, 4636, 135, 48],\n",
       " [3, 89, 4636, 53, 1429, 1429],\n",
       " [3, 7, 10536, 4107, 26, 100],\n",
       " [25, 13721, 274],\n",
       " [96, 3, 4636, 7, 14938, 7],\n",
       " [3, 89, 4636, 8, 4404],\n",
       " [19433, 12937, 19],\n",
       " [16885, 720, 524, 25],\n",
       " [3, 89, 4636, 53, 3, 9, 3, 115],\n",
       " [140, 38, 7, 3, 31, 17],\n",
       " [34, 4488, 16497],\n",
       " [2523, 19, 12068],\n",
       " [1429, 3, 4636, 7, 103],\n",
       " [15708, 12730, 147, 7535],\n",
       " [3, 7, 10536, 48, 930],\n",
       " [3, 89, 4636, 10657, 3, 9],\n",
       " [135, 5781, 3, 23],\n",
       " [4498, 21562, 48],\n",
       " [3, 89, 4636, 34, 8],\n",
       " [3, 7, 10536, 19, 8],\n",
       " [3, 89, 4636, 53, 3, 1598, 15, 48],\n",
       " [3, 89, 4636, 909, 1140],\n",
       " [3, 7, 10536, 3, 23, 23, 230],\n",
       " [3, 23, 1670, 25851, 7],\n",
       " [33, 54, 29, 23, 3849, 114],\n",
       " [3, 89, 4636, 53, 3, 89, 4636, 53, 216],\n",
       " [3, 89, 4636, 3, 23, 6679],\n",
       " [3, 7, 10536, 1576, 1224],\n",
       " [3, 7, 10536, 3, 9, 428],\n",
       " [3, 89, 4636, 3, 31, 3, 15, 51],\n",
       " [328, 15708, 479],\n",
       " [148, 27539, 3, 60],\n",
       " [3, 7, 10536, 33, 421],\n",
       " [3, 7, 10536, 94, 7, 3, 13366],\n",
       " [13721, 3, 23, 3413],\n",
       " [3, 89, 4636, 77, 3, 7820, 3, 10207],\n",
       " [3, 7, 10536, 24, 165],\n",
       " [3, 7, 10536, 19, 2508],\n",
       " [3, 89, 4636, 53, 17227, 25851],\n",
       " [3, 89, 4636, 77, 604, 1514],\n",
       " [3, 89, 15318, 160, 47],\n",
       " [3, 89, 4636, 53, 145, 271],\n",
       " [3, 89, 4636, 53, 25, 34, 1222],\n",
       " [25851, 3, 55, 5, 70],\n",
       " [3, 157, 506, 3, 342, 1329],\n",
       " [3, 89, 4636, 53, 4946, 3, 52, 9, 4703],\n",
       " [3, 89, 15318, 95, 3, 115, 51],\n",
       " [3, 7, 10536, 12685, 7, 1246],\n",
       " [3, 7, 10536, 3, 3158, 8],\n",
       " [3, 26, 38, 7, 3, 11060],\n",
       " [34, 25, 4488],\n",
       " [3, 7, 10536, 275, 230],\n",
       " [3, 89, 4636, 53, 3, 10696, 6246],\n",
       " [3, 7, 10536, 38, 7, 16998],\n",
       " [38, 7, 82, 8],\n",
       " [3, 10, 863, 25851],\n",
       " [17081, 5423, 11947, 7],\n",
       " [3, 89, 4636, 7326, 7, 33],\n",
       " [3, 89, 4636, 130, 25],\n",
       " [3, 7, 10536, 3, 29, 48],\n",
       " [3, 7, 10536, 275, 376],\n",
       " [3, 89, 4636, 53, 27, 5597],\n",
       " [3, 7, 10536, 3, 9, 2654],\n",
       " [3, 89, 4636, 8, 8420],\n",
       " [73, 17, 7, 3, 31, 60, 3, 4884, 1211],\n",
       " [3, 89, 15318, 892, 95],\n",
       " [3, 7, 10536, 3, 9, 1527],\n",
       " [3, 7, 10536, 24, 16],\n",
       " [4575, 3, 89, 15318, 95],\n",
       " [38, 7, 3, 31, 7, 4583],\n",
       " [3, 89, 4636, 6660, 17132, 3, 23],\n",
       " [17081, 24, 54, 17],\n",
       " [165, 6660, 385],\n",
       " [3, 7, 10536, 320, 2065],\n",
       " [3, 2951, 720, 33],\n",
       " [178, 7, 3, 9, 9, 3, 63, 7],\n",
       " [3, 7, 10536, 100, 396],\n",
       " [3, 23, 9798, 3, 23, 26, 21029],\n",
       " [3, 89, 4636, 53, 165, 7827],\n",
       " [38, 17, 63, 10215, 445],\n",
       " [3, 89, 4636, 95, 160],\n",
       " [1350, 33, 3, 2951],\n",
       " [13721, 8, 492],\n",
       " [3, 89, 4636, 8, 19],\n",
       " [16497, 3, 7, 10536, 94, 7],\n",
       " [17227, 1429, 720, 524],\n",
       " [3, 4796, 3, 2666, 1167, 19],\n",
       " [38, 7, 112, 522],\n",
       " [3, 89, 4636, 1429, 3, 23],\n",
       " [3, 26, 3142, 148, 3, 9],\n",
       " [3, 89, 4636, 53, 38, 7, 328],\n",
       " [3, 89, 15318, 328, 95],\n",
       " [2039, 819, 25],\n",
       " [720, 524, 3, 31, 3, 76],\n",
       " [38, 7, 2716, 508],\n",
       " [3, 7, 10536, 16497, 16],\n",
       " [3, 7, 10536, 717, 48],\n",
       " [10939, 29, 3, 17065, 3, 10769],\n",
       " [3, 7, 10536, 48, 114],\n",
       " [3, 89, 4636, 53, 10018, 1142],\n",
       " [3, 7059, 272, 278],\n",
       " [3, 89, 4636, 53, 25, 230],\n",
       " [3, 7, 10536, 131, 8],\n",
       " [3, 7, 10536, 165, 1560],\n",
       " [10381, 277, 17985, 7, 17, 3, 61],\n",
       " [3, 7, 10536, 8434, 24],\n",
       " [151, 100, 38, 17],\n",
       " [3, 7, 10536, 3, 7, 10536, 10939, 29],\n",
       " [3, 89, 4636, 53, 328, 103],\n",
       " [3, 89, 4636, 4374, 28],\n",
       " [3, 7, 10536, 1350, 328],\n",
       " [165, 5413, 6660],\n",
       " [18554, 7, 506, 2483],\n",
       " [3, 7, 10536, 133, 421],\n",
       " [861, 3, 17, 4046, 7, 28617],\n",
       " [3, 7, 10536, 3, 23, 3359],\n",
       " [3, 4636, 7, 70, 3, 23],\n",
       " [3, 7, 10536, 2218, 328],\n",
       " [38, 7, 3, 31, 7, 34],\n",
       " [17081, 48, 56],\n",
       " [3, 7, 10536, 3, 31, 17, 744],\n",
       " [3, 26, 3142, 25, 39],\n",
       " [3, 7, 10536, 165, 3, 9],\n",
       " [3, 7, 10536, 3, 88, 112],\n",
       " [3, 7, 10536, 24, 6179],\n",
       " [3, 7, 10536, 3, 102, 75, 12],\n",
       " [3, 7, 10536, 114, 48],\n",
       " [3, 12488, 3, 157, 7, 16649],\n",
       " [3, 7, 10536, 1416, 1134],\n",
       " [3, 89, 4636, 53, 3, 23, 3, 4941],\n",
       " [442, 3, 27826, 3, 29, 17],\n",
       " [3, 7, 10536, 1028, 3, 7820],\n",
       " [123, 29, 17, 4024, 44],\n",
       " [3, 89, 4636, 53, 376, 3, 40, 76, 4370, 8283],\n",
       " [25, 9863, 3, 7, 10536, 3, 23],\n",
       " [12937, 2615, 3, 15, 17, 7],\n",
       " [3, 89, 4636, 3413, 25],\n",
       " [3, 89, 4636, 1452, 147],\n",
       " [3, 7, 10536, 48, 9758],\n",
       " [39, 28617, 21029],\n",
       " [720, 524, 48, 300],\n",
       " [25851, 466, 2472],\n",
       " [3, 7, 10536, 3, 12840, 115, 7, 16497],\n",
       " [25, 11761, 114],\n",
       " [34, 4488, 3, 23],\n",
       " [9078, 3, 89, 4636, 53, 3, 7, 10536],\n",
       " [3, 189, 9, 3, 31, 3, 89, 4636],\n",
       " [3, 7, 10536, 8400, 70],\n",
       " [3, 4636, 7, 4752, 3, 89],\n",
       " [10283, 3, 7, 10536, 116],\n",
       " [3, 89, 4636, 53, 114, 27],\n",
       " [6139, 3, 60, 7, 3, 9],\n",
       " [3, 89, 4636, 25, 28617],\n",
       " [70, 3, 7, 10536, 3, 7, 10536],\n",
       " [3, 89, 4636, 8, 25],\n",
       " [17261, 44, 7, 8698, 75, 52],\n",
       " [377, 4636, 8, 615],\n",
       " [3, 89, 4636, 47, 146, 221],\n",
       " [9078, 3, 7, 10536, 3413],\n",
       " [13721, 148, 11],\n",
       " [3, 7, 10536, 79, 59],\n",
       " [3, 9, 122, 7, 1609, 27293],\n",
       " [22641, 19, 100],\n",
       " [3, 7, 10536, 3, 32, 102, 3, 9],\n",
       " [3, 89, 4636, 48, 8],\n",
       " [3, 89, 4636, 53, 38, 7, 34],\n",
       " [3, 7, 10536, 8034, 145],\n",
       " [3, 7, 10536, 48, 414],\n",
       " [3, 26, 3142, 3, 23, 5591],\n",
       " [3, 7, 10536, 79, 423],\n",
       " [12937, 2161, 3, 19668],\n",
       " [3, 52, 82, 3, 89, 4636],\n",
       " [3, 89, 4636, 3, 23, 3, 210],\n",
       " [123, 29, 17, 3, 88, 7, 1953],\n",
       " [25851, 7, 25, 270],\n",
       " [3, 89, 4636, 77, 21562, 3, 23],\n",
       " [3, 89, 4636, 3, 157, 4447],\n",
       " [3, 7, 10536, 549, 79],\n",
       " [3, 7, 10536, 94, 7, 48],\n",
       " [3, 89, 4636, 3, 4608, 47],\n",
       " [3, 7, 10536, 367, 44, 7],\n",
       " [47, 10950, 75, 52, 3, 7, 75],\n",
       " [3, 89, 4636, 53, 470, 27],\n",
       " [3, 7, 10536, 224, 3, 8739],\n",
       " [38, 7, 1874, 130],\n",
       " [13721, 2112, 8181, 7],\n",
       " [3, 7, 10536, 3, 9, 1527],\n",
       " [3, 7, 10536, 4761, 39],\n",
       " [3, 7, 10536, 3, 23, 410],\n",
       " [3, 7, 10536, 16497, 79],\n",
       " [3, 89, 4636, 720, 524, 6979],\n",
       " [3, 7, 10536, 7457, 81],\n",
       " [3, 7, 10536, 3, 189, 3, 32, 40, 3216],\n",
       " [3, 7, 10536, 19, 3, 1167],\n",
       " [17081, 94, 7, 114],\n",
       " [3, 89, 4636, 25, 8],\n",
       " [17227, 95, 24],\n",
       " [3, 7, 10536, 65, 1548],\n",
       " [3, 7, 10536, 47, 3641],\n",
       " [3, 7, 10536, 3, 89, 15318, 178],\n",
       " [25, 223, 19329, 15, 26],\n",
       " [3, 7, 10536, 19, 8],\n",
       " [17227, 2085, 63, 6897],\n",
       " [8030, 30, 48],\n",
       " [377, 4636, 24, 376],\n",
       " [3, 4911, 363, 9872],\n",
       " [17227, 3, 3158, 3, 89, 4636],\n",
       " [3, 7, 10536, 3, 22188, 499],\n",
       " [3, 7, 10536, 3, 757, 148],\n",
       " [3, 7, 10536, 16497, 3, 63, 9],\n",
       " [3, 7, 10536, 3, 13283, 114],\n",
       " [38, 7, 22269, 3, 76],\n",
       " [3, 7, 10536, 3, 89, 265, 977],\n",
       " [3, 89, 15318, 96, 129],\n",
       " [3, 89, 4636, 16541, 36],\n",
       " [3, 157, 25851, 256],\n",
       " [16497, 3, 89, 4636, 8],\n",
       " [38, 7, 3, 1603, 112],\n",
       " [3, 89, 4636, 53, 8204, 4336],\n",
       " [3, 26, 3142, 160, 28],\n",
       " [10939, 29, 3, 88, 132],\n",
       " [3, 89, 4636, 53, 3, 14916, 80],\n",
       " [3, 89, 4636, 3, 89, 1342],\n",
       " [38, 7, 38, 7, 82],\n",
       " [3, 89, 4636, 363, 8],\n",
       " [377, 4636, 5378, 388],\n",
       " [38, 7, 4930, 157, 150],\n",
       " [3, 7, 10536, 2145, 8],\n",
       " [3, 7, 10536, 5803, 671, 17],\n",
       " [3, 26, 3142, 3, 7, 10536, 3, 7, 4636],\n",
       " [3, 7, 10536, 3, 9, 1026],\n",
       " [25851, 7, 25, 700],\n",
       " [27539, 25, 94],\n",
       " [3, 89, 4636, 1974, 231],\n",
       " [3, 26, 3142, 540, 7494],\n",
       " [3, 7, 10536, 48, 19],\n",
       " [3, 89, 4636, 258, 2056],\n",
       " [3, 89, 4636, 4589, 8],\n",
       " [18554, 7, 502, 30],\n",
       " [4488, 3, 10769, 34],\n",
       " [34, 1222, 3, 7675, 6031, 6660],\n",
       " [2090, 2647, 3, 7159],\n",
       " [17227, 4954, 5306],\n",
       " [3, 7, 10536, 133, 6679],\n",
       " [3, 4636, 7, 14246, 3, 89],\n",
       " [3, 7, 10536, 38, 3, 23],\n",
       " [872, 23713, 7, 10998],\n",
       " [3, 7, 10536, 3, 23, 3, 9],\n",
       " [3, 7, 10536, 48, 2181],\n",
       " [3, 89, 15318, 696, 95],\n",
       " [3, 7, 10536, 466, 19],\n",
       " [16497, 3, 7315, 7922],\n",
       " [3, 4636, 53, 29744, 377],\n",
       " [3, 7, 10536, 132, 79],\n",
       " [25, 10123, 7927],\n",
       " [1108, 21562, 3, 26909],\n",
       " [19367, 100, 178],\n",
       " [34, 4488, 3, 7, 10536],\n",
       " [3, 89, 4636, 37, 38],\n",
       " [3, 89, 15318, 178, 36],\n",
       " [3, 7, 10536, 100, 7326, 7],\n",
       " [3, 32, 106, 25529, 100],\n",
       " [3, 89, 4636, 25, 25],\n",
       " [13721, 485, 3, 29, 17, 930],\n",
       " [3, 89, 4636, 53, 25, 363],\n",
       " [3, 7, 10536, 13190, 1768],\n",
       " [10939, 29, 2039, 3, 89, 4636],\n",
       " [3, 89, 4636, 3, 9, 1527],\n",
       " [3, 4636, 7, 3, 7315, 16497],\n",
       " [93, 6174, 179, 112, 646],\n",
       " [3, 89, 4636, 14565, 17, 25328],\n",
       " [4840, 71, 39],\n",
       " [8030, 30, 7, 14552],\n",
       " [3, 89, 4636, 53, 2085, 27],\n",
       " [2592, 49, 623, 17, 25],\n",
       " [3, 7, 10536, 3963, 2233],\n",
       " [3, 89, 4636, 53, 19329, 15, 26, 10476],\n",
       " [7797, 39, 93, 6174, 179],\n",
       " [3, 7, 10536, 48, 270],\n",
       " [3, 7, 10536, 3, 31, 17, 696],\n",
       " [649, 29, 3, 15, 26, 69],\n",
       " [3, 7, 10536, 424, 3359],\n",
       " [8434, 63, 3, 9, 36],\n",
       " [3, 7, 10536, 27, 7, 16],\n",
       " [3, 89, 4636, 38, 3, 23],\n",
       " [17081, 372, 3, 23],\n",
       " [3, 89, 4636, 53, 16352, 2321],\n",
       " [819, 3, 89, 15318, 8],\n",
       " [689, 1761, 8423, 1225],\n",
       " [11923, 3, 2, 36],\n",
       " [3, 7, 10536, 24, 95],\n",
       " [3, 7, 10536, 24, 2506],\n",
       " [34, 4488, 3, 88],\n",
       " [3, 89, 4636, 53, 3, 23, 641],\n",
       " [3, 7, 10536, 25, 168],\n",
       " [275, 576, 2239, 3, 9],\n",
       " [3, 7, 10536, 454, 635, 572],\n",
       " [3, 89, 4636, 53, 79, 79],\n",
       " [720, 524, 8977, 165],\n",
       " [3, 89, 15318, 95, 48],\n",
       " [3, 7, 10536, 12285, 8],\n",
       " [29651, 36, 38, 17],\n",
       " [95, 3, 89, 4636, 17112],\n",
       " [38, 7, 3, 4636, 53, 572],\n",
       " [3, 89, 4636, 53, 94, 3, 31, 7],\n",
       " [3, 89, 15318, 62, 33],\n",
       " [25851, 23713, 625],\n",
       " [3, 89, 4636, 53, 579, 924, 8],\n",
       " [123, 29, 17, 48, 1615],\n",
       " [3, 89, 4636, 2018, 3, 2951],\n",
       " [38, 7, 4514, 3896, 14658],\n",
       " [3, 89, 4636, 53, 2324, 2792],\n",
       " [3, 7, 10536, 465, 33],\n",
       " [3, 8509, 720, 3, 7, 232],\n",
       " [3, 7, 10536, 270, 3, 76],\n",
       " [17227, 34, 3, 31],\n",
       " [3, 2666, 1167, 6802, 3, 32, 7],\n",
       " [3, 7, 10536, 2306, 160],\n",
       " [3, 89, 4636, 11279, 3, 9],\n",
       " [3, 89, 15318, 33, 25],\n",
       " [3, 450, 720, 524, 3, 9, 77],\n",
       " [3, 89, 4636, 3, 9, 1527],\n",
       " [38, 7, 16, 114],\n",
       " [3, 7, 10536, 48, 16],\n",
       " [3, 89, 4636, 8, 19],\n",
       " [3, 7, 10536, 466, 38],\n",
       " [6802, 25, 3, 32],\n",
       " [10950, 75, 33, 363],\n",
       " [3, 23, 3, 7, 10536, 29704],\n",
       " [25, 17056, 1282],\n",
       " [38, 7, 3, 76, 148],\n",
       " [3, 89, 4636, 53, 499, 82],\n",
       " [13721, 23139, 14319],\n",
       " [3, 89, 4636, 53, 1001, 255],\n",
       " [3, 2951, 720, 3, 89, 4636, 77],\n",
       " [720, 524, 47, 34],\n",
       " [3, 7, 10536, 101, 103],\n",
       " [720, 524, 376, 3, 7, 8478],\n",
       " [3, 89, 4636, 3, 3158, 3, 10769],\n",
       " [13721, 3, 2666, 1167, 33],\n",
       " [3, 89, 4636, 53, 96, 27],\n",
       " [3, 7, 10536, 25, 33],\n",
       " [12937, 5361, 48],\n",
       " [3, 89, 4636, 53, 25, 116],\n",
       " [3, 89, 4636, 8, 376],\n",
       " [13721, 363, 3, 9],\n",
       " [2615, 52, 24, 1688],\n",
       " [17227, 3, 60, 7, 11797],\n",
       " [3, 7, 10536, 15273, 3, 31, 17],\n",
       " [377, 4636, 3, 23, 16],\n",
       " [872, 151, 5591],\n",
       " [3, 7, 10536, 15273, 3, 23],\n",
       " [38, 7, 3, 31, 7, 6841],\n",
       " [3, 7, 10536, 16497, 24],\n",
       " [38, 7, 24, 28],\n",
       " [3, 27826, 25, 3, 60],\n",
       " [442, 696, 15708],\n",
       " [3, 89, 4636, 53, 720, 524, 613],\n",
       " [377, 4636, 3, 23, 3, 63, 15, 9],\n",
       " [3, 89, 4636, 53, 25, 29744],\n",
       " [3, 7, 10536, 48, 705],\n",
       " [3, 4636, 7, 2759, 3, 860],\n",
       " [18554, 7, 148, 258],\n",
       " [613, 3, 89, 4636, 38],\n",
       " [38, 7, 328, 3, 9325],\n",
       " [3, 7, 10536, 1782, 2161],\n",
       " [3, 7, 10536, 48, 62],\n",
       " [3, 7, 10536, 367, 28617],\n",
       " [34, 1222, 25, 6660],\n",
       " [3, 7, 10536, 3, 9, 1527],\n",
       " [3, 89, 4636, 38, 4623],\n",
       " [38, 7, 100, 16],\n",
       " [3, 7, 10536, 27, 231],\n",
       " [3, 26, 3142, 14352, 3, 23],\n",
       " [3, 2666, 1167, 12447, 924, 3, 32, 102, 1493, 202],\n",
       " [377, 4636, 326, 44],\n",
       " [720, 524, 3912, 25],\n",
       " [3, 89, 4636, 24, 81],\n",
       " [3, 2, 3, 2, 3, 2],\n",
       " [3, 7, 10536, 25, 39],\n",
       " [38, 7, 4930, 157, 3, 76],\n",
       " [3, 89, 4636, 53, 3, 23, 589],\n",
       " [3, 4636, 7, 376, 274],\n",
       " [2039, 17227, 720, 524],\n",
       " [3, 7, 10536, 70, 3, 52, 9, 15, 40],\n",
       " [696, 182, 11],\n",
       " [38, 7, 3, 9, 19367],\n",
       " [17227, 15, 26, 17227, 15, 26, 25],\n",
       " [3, 7, 10536, 3, 76, 2325, 7],\n",
       " [3, 4636, 7, 3, 89, 14246],\n",
       " [34, 4488, 3, 23],\n",
       " [3, 7, 10536, 4475, 24],\n",
       " [6629, 7, 63, 13559, 1429],\n",
       " [160, 160, 22214],\n",
       " [3, 7, 10536, 118, 27],\n",
       " [3, 89, 4636, 3, 7, 10536, 24],\n",
       " [12937, 48, 12],\n",
       " [5591, 3, 32, 15, 7, 8],\n",
       " [3, 89, 4636, 38, 62],\n",
       " [3, 89, 4636, 53, 13301, 3, 23],\n",
       " [3, 89, 15318, 5956, 82],\n",
       " [13721, 3, 9, 10802],\n",
       " [38, 7, 20063, 79],\n",
       " [3, 89, 4636, 53, 1615, 30337, 63],\n",
       " [377, 4636, 3, 31, 3, 23],\n",
       " [8445, 120, 216, 10399],\n",
       " [377, 4636, 25, 3, 7, 10536],\n",
       " [17227, 34, 3, 89, 4636, 53],\n",
       " [3, 89, 4636, 326, 135],\n",
       " [3, 7, 10536, 165, 6686],\n",
       " [38, 7, 499, 4781, 7],\n",
       " [19, 6257, 30119],\n",
       " [3, 89, 4636, 53, 4498, 3, 8546],\n",
       " [3, 89, 4636, 8026, 13],\n",
       " [3, 89, 4636, 209, 326],\n",
       " [3, 89, 4636, 53, 3, 5937, 19],\n",
       " [3, 89, 4636, 53, 13562, 216],\n",
       " [3, 89, 4636, 77, 3, 7820, 3, 3441],\n",
       " [3, 89, 4636, 25, 25],\n",
       " [25, 114, 33],\n",
       " [3, 89, 4636, 25, 3, 7820],\n",
       " [3, 89, 4636, 53, 79, 299],\n",
       " [3, 7, 10536, 424, 25],\n",
       " [165, 192, 6660],\n",
       " [3, 7, 10536, 33, 21331],\n",
       " [3, 89, 4636, 33, 25],\n",
       " [39, 17896, 3, 7, 12963],\n",
       " [12937, 3, 29, 17, 148],\n",
       " [377, 4636, 24, 381],\n",
       " [3, 89, 4636, 3359, 258],\n",
       " [17227, 3, 1572, 3, 23],\n",
       " [3, 7, 10536, 270, 299],\n",
       " [3, 7, 10536, 48, 1615],\n",
       " [3, 76, 3, 7, 10536, 756],\n",
       " [3, 76, 3, 89, 4636, 43],\n",
       " [38, 7, 48, 39],\n",
       " [10381, 277, 66, 33],\n",
       " [376, 38, 7, 94, 7],\n",
       " [3, 89, 15318, 25, 333],\n",
       " [3, 7, 10536, 396, 3, 23],\n",
       " [17227, 3, 89, 4636, 53, 3, 12534, 15],\n",
       " [38, 7, 46, 36],\n",
       " [34, 4488, 3, 23],\n",
       " [3, 210, 13419, 625, 2237],\n",
       " [38, 7, 16497, 16713],\n",
       " [377, 4636, 25, 21],\n",
       " [3, 26, 3142, 255, 16497],\n",
       " [6979, 38, 7, 95],\n",
       " [17081, 2701, 62],\n",
       " [25, 3, 23, 23713],\n",
       " [3, 89, 4636, 4930, 157, 2645],\n",
       " [3, 7, 10536, 128, 30],\n",
       " [3, 89, 4636, 3, 23, 25],\n",
       " [3, 7, 10536, 3, 834, 3, 834],\n",
       " [3, 89, 4636, 77, 3, 31, 25],\n",
       " [6629, 7, 63, 6067, 21506],\n",
       " [3, 89, 4636, 125, 8],\n",
       " [3, 7, 10536, 3088, 3832],\n",
       " [3, 7059, 25, 272],\n",
       " [754, 3, 7, 10536, 96],\n",
       " [3, 7, 10536, 621, 80],\n",
       " [720, 524, 1969, 37],\n",
       " [3, 7, 10536, 48, 5851],\n",
       " [3, 7, 10536, 3, 9, 405],\n",
       " [38, 7, 21562, 148],\n",
       " [3, 1603, 3, 7, 10536, 1466],\n",
       " [3, 7, 10536, 3, 23, 3, 17701],\n",
       " [3, 89, 4636, 1933, 8],\n",
       " [3, 7, 10536, 3, 157, 27, 26],\n",
       " [17227, 872, 275],\n",
       " [3, 89, 4636, 53, 14642, 4073],\n",
       " [27539, 39, 25],\n",
       " [3, 89, 4636, 53, 79, 306],\n",
       " [3, 7, 10536, 3, 9, 1960],\n",
       " [3, 89, 4636, 8, 135],\n",
       " [3, 226, 3, 7, 10536, 1782],\n",
       " [720, 524, 25, 3, 9],\n",
       " [3, 7, 10536, 94, 3, 31, 7],\n",
       " [46, 138, 25, 256],\n",
       " [3, 89, 4636, 8, 3, 55, 58],\n",
       " [720, 524, 62, 48],\n",
       " [561, 7, 363, 3, 986, 7],\n",
       " [25851, 7, 112, 33],\n",
       " [3, 4554, 6688, 8030],\n",
       " [3, 15, 51, 6476, 8],\n",
       " [22641, 70, 7718],\n",
       " [148, 3, 60, 1728],\n",
       " [178, 7, 3, 725, 25],\n",
       " [38, 7, 326, 39],\n",
       " [19, 93, 6174, 179, 3, 1433],\n",
       " [299, 3, 7, 10536, 19],\n",
       " [3, 7, 10536, 3, 31, 7, 466],\n",
       " [3, 76, 3, 3158, 255],\n",
       " [13721, 25214, 7, 3337],\n",
       " [3, 89, 4636, 53, 25, 3, 31, 60],\n",
       " [3, 7, 10536, 19, 19, 51],\n",
       " [38, 7, 39, 1615],\n",
       " [26557, 39, 27907],\n",
       " [96, 25, 720, 524],\n",
       " [3, 89, 4636, 146, 221, 4047],\n",
       " [3, 23, 9798, 19, 243],\n",
       " [38, 7, 22658, 7873],\n",
       " [3, 89, 15318, 1134, 95],\n",
       " [3, 89, 4636, 53, 1131, 26, 155, 10283],\n",
       " [3, 89, 4636, 53, 95, 47],\n",
       " [3, 89, 4636, 53, 273, 17205],\n",
       " [3, 89, 4636, 53, 19329, 15, 26, 4073],\n",
       " [3, 7, 10536, 3836, 281],\n",
       " [3, 7, 10536, 81, 451],\n",
       " [38, 7, 38, 7, 9136, 281],\n",
       " [34, 17, 38, 7, 3, 17],\n",
       " [3, 7059, 1429, 1429],\n",
       " [3, 89, 4636, 135, 156],\n",
       " [3, 89, 4636, 3, 9, 89, 8],\n",
       " [8434, 7, 10536, 376, 25],\n",
       " [3, 89, 4636, 140, 147],\n",
       " [3, 89, 4636, 25, 1548],\n",
       " [733, 3, 7, 10536, 6679],\n",
       " [3, 61, 17081, 39],\n",
       " [3, 7, 10536, 38, 328],\n",
       " [3, 89, 4636, 3, 10769, 3, 7, 10536],\n",
       " [10950, 25863, 25, 3, 7, 7],\n",
       " [16096, 2959, 19],\n",
       " [3, 7, 10536, 16, 17, 3, 29],\n",
       " [377, 4636, 3, 31, 7, 116],\n",
       " [25, 16497, 3, 7315],\n",
       " [3, 7, 10536, 4244, 1024, 17205],\n",
       " [3, 89, 4636, 6802, 466],\n",
       " [38, 7, 150, 17307],\n",
       " [2241, 39, 1562],\n",
       " [3, 89, 4636, 25, 3, 23],\n",
       " [3, 89, 4636, 38, 3, 7],\n",
       " [95, 3, 89, 15318, 34],\n",
       " [1001, 1342, 19],\n",
       " [3, 5840, 3, 5937, 24694],\n",
       " [38, 7, 14565, 17, 39],\n",
       " [17227, 3, 7, 4636, 7, 1176],\n",
       " [3, 89, 4636, 125, 8],\n",
       " [38, 7, 8, 135],\n",
       " [4546, 19329, 15, 26, 100],\n",
       " [19329, 15, 26, 506, 19],\n",
       " [3, 32, 7, 8090, 3, 226],\n",
       " [95, 1108, 48],\n",
       " [3, 7, 10536, 4244, 1024, 24],\n",
       " [3, 4636, 53, 2490, 1396],\n",
       " [3, 11644, 33, 113],\n",
       " [377, 4636, 3, 1603, 13],\n",
       " [3, 7, 10536, 48, 79],\n",
       " [3, 89, 4636, 53, 466, 1800],\n",
       " [25851, 7, 1934, 499],\n",
       " [377, 4636, 11513, 1084],\n",
       " [18168, 148, 25],\n",
       " [12410, 13721, 2586],\n",
       " [3, 89, 4636, 3, 63, 32, 6545],\n",
       " [38, 7, 129, 1953],\n",
       " [3, 89, 15318, 95, 132],\n",
       " [3, 7, 10536, 100, 19],\n",
       " [3, 7, 10536, 3, 195, 95],\n",
       " [3, 89, 4636, 720, 524, 113],\n",
       " [3, 7059, 25, 272],\n",
       " [3, 7, 10536, 273, 9832],\n",
       " [96, 3, 89, 4636, 96],\n",
       " [3, 7, 10536, 11475, 26199],\n",
       " [3, 4636, 7, 3, 450, 25],\n",
       " [3, 7, 10536, 79, 3, 107],\n",
       " [3, 89, 4636, 3, 3158, 3, 189, 32],\n",
       " [38, 7, 16497, 25],\n",
       " [17227, 39, 230],\n",
       " [38, 2260, 135, 70],\n",
       " [8030, 30, 7, 2615],\n",
       " [3, 89, 4636, 53, 2049, 21],\n",
       " [3, 26, 3142, 3955, 7, 16],\n",
       " [3, 89, 4636, 53, 1429, 1429],\n",
       " [3, 7, 10536, 48, 328],\n",
       " [7226, 26, 3, 2666, 1167, 19],\n",
       " [13721, 2005, 194],\n",
       " [6629, 7, 63, 887, 96, 5],\n",
       " [3, 89, 4636, 53, 216, 3, 31, 7],\n",
       " [377, 4636, 82, 2191],\n",
       " [720, 524, 25, 3, 23],\n",
       " [38, 7, 789, 66],\n",
       " [1624, 26, 155, 3, 4636, 7, 3, 89],\n",
       " [16, 7927, 8],\n",
       " [3, 7, 10536, 3, 76, 3, 63],\n",
       " [112, 255, 38, 7],\n",
       " [3, 23, 275, 720, 524],\n",
       " [3, 7, 10536, 2817, 3, 7, 14471],\n",
       " [3, 986, 7, 9321, 3, 25486],\n",
       " [3, 89, 4636, 53, 278, 486, 17],\n",
       " [3, 19230, 3, 7, 10536, 3, 9],\n",
       " [3, 89, 4636, 38, 696],\n",
       " [3, 89, 4636, 3, 9, 1527],\n",
       " [3, 17, 89, 38, 7, 39],\n",
       " [3, 27077, 43, 118],\n",
       " [1670, 13721, 25],\n",
       " [3, 2951, 720, 256],\n",
       " [148, 3, 27826, 33],\n",
       " [3, 7, 10536, 24, 59],\n",
       " [3, 89, 4636, 148, 281],\n",
       " [3, 7, 10536, 156, 25],\n",
       " [819, 12, 8],\n",
       " [27197, 15708, 8690, 120],\n",
       " [3, 7, 10536, 25, 3, 9],\n",
       " [17227, 230, 933],\n",
       " [135, 3, 189, 32, 3, 2951],\n",
       " [1131, 26, 155, 3, 7, 10536, 48],\n",
       " [720, 524, 16, 48],\n",
       " [1429, 1429, 1429],\n",
       " [3, 7, 10536, 3, 23, 114],\n",
       " [3, 89, 15318, 79, 95],\n",
       " [1131, 26, 155, 3, 2951, 720],\n",
       " [3, 89, 4636, 11604, 7, 175],\n",
       " [19329, 9459, 6659],\n",
       " [3, 89, 4636, 53, 3, 18, 3, 23],\n",
       " [38, 7, 6772, 16497],\n",
       " [3, 7, 10536, 48, 8],\n",
       " [720, 524, 3, 89, 4636, 77, 39],\n",
       " [10939, 29, 3, 17, 52, 3, 867],\n",
       " [3, 89, 4636, 8, 48],\n",
       " [3, 7, 10536, 16497, 2767],\n",
       " [13446, 16, 30916, 29, 17, 3368],\n",
       " [3, 89, 4636, 53, 19, 3, 15, 9],\n",
       " [720, 524, 16, 3, 834],\n",
       " [3, 7, 10536, 3519, 7, 47],\n",
       " [475, 3, 7, 10536, 1391],\n",
       " [3, 89, 4636, 8, 1837],\n",
       " [720, 524, 100, 469],\n",
       " [1108, 3, 89, 4636, 48],\n",
       " [38, 7, 720, 524, 3, 7315],\n",
       " [3, 63, 32, 3, 89, 4636, 95],\n",
       " [3, 89, 4636, 25, 33],\n",
       " ...]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bad_word_ids(raw_datasets[\"validation\"]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we try experimenting with generation using the bad_words_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_checkpoints():\n",
    "    # Get checkpoint values for the best models\n",
    "    with open(BEST_MODEL_CHECKPOINT_PATH, \"r\") as f:\n",
    "        best_model_checkpoints = f.readlines()\n",
    "\n",
    "    # Convert to a dictionary\n",
    "    best_model_checkpoints_dict = {}\n",
    "    for line in best_model_checkpoints:\n",
    "        model_name, checkpoint_path = line.split(\": \")\n",
    "        best_model_checkpoints_dict[model_name] = checkpoint_path.strip()\n",
    "\n",
    "    return best_model_checkpoints_dict\n",
    "\n",
    "model_checkpoints = get_model_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-f6ce81031cd2c9b9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-95addc4f6725a34d.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-1fcdd884e5e702c4.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "t5_ud = training_pipeline(\n",
    "    model_name = \"t5_small_unidir\",\n",
    "    project_name = \"t5-detox\",\n",
    "    model_checkpoint = model_checkpoints[\"t5_small_unidir\"],\n",
    "    use_validation = True,\n",
    "    raw_datasets = raw_datasets,\n",
    "    bidirectional = False,\n",
    "    shuffle = False,\n",
    "    do_train = False   \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-c829f36186d209db.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 1193\n",
       "})"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  391,    17,    82,  ...,     0,     0,     0],\n",
       "        [   96,  1131,    26,  ...,     0,     0,     0],\n",
       "        [ 6214,   102,   258,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [  465,    25,    38,  ...,     0,     0,     0],\n",
       "        [11456,     6,    68,  ...,     0,     0,     0],\n",
       "        [20067,     3, 14312,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = t5_ud.tokenizer(\n",
    "    raw_datasets[\"validation\"]['source'],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,    27,   333,  ...,     0,     0,     0],\n",
       "        [    0,  1624,    26,  ...,     0,     0,     0],\n",
       "        [    0,  6214,   102,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [    0,    27,  3290,  ...,     0,     0,     0],\n",
       "        [    0, 11456,     6,  ...,     0,     0,     0],\n",
       "        [    0, 20067,     3,  ...,     0,     0,     0]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Move t5 ud model to cpu\n",
    "t5_ud.model.to(\"cpu\")\n",
    "\n",
    "t5_ud.model.generate(\n",
    "    **inputs,\n",
    "    max_length=MAX_OUTPUT_LENGTH,\n",
    "    num_beams=NUM_BEAMS,\n",
    "    early_stopping=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-Jvh1WXlz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
