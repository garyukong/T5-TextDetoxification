{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 17:22:02.690986: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-21 17:22:02.691047: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-21 17:22:02.691079: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "from numba import cuda\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "from typing import Dict, Union, Optional, Tuple, List, Any\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Default parameters for T5 model fine-tuning\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 64\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "# Include BLEURT score in evaluation\n",
    "INCLUDE_BLEURT = True\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "AUG_DATASET_ALL_FILTERS_PATH = \"../data/processed/aug_datasets_all_filters\"\n",
    "AUG_DATASET_NO_TOXICITY_FILTER_PATH = \"../data/processed/aug_datasets_no_toxicity_filter\"\n",
    "AUG_DATASET_NO_SIMILARITY_FILTER_PATH = \"../data/processed/aug_datasets_no_similarity_filter\"\n",
    "AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH = \"../data/processed/aug_datasets_no_acceptability_filter\"\n",
    "\n",
    "# Set path for txt file containing best model checkpoints\n",
    "BEST_MODEL_CHECKPOINT_PATH = \"../models/best_model_checkpoints.txt\"\n",
    "\n",
    "# Set path to save evaluation outputs to\n",
    "VAL_PREDS_PATH = \"../data/interim/val_preds.csv\"\n",
    "VAL_METRICS_PATH = \"../data/interim/val_metrics.csv\"\n",
    "TEST_PREDS_PATH = \"../data/final/test_preds.csv\"\n",
    "TEST_METRICS_PATH = \"../data/final/test_metrics.csv\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and models\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(DEVICE)\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "aug_datasets_all_filters = DatasetDict.load_from_disk(AUG_DATASET_ALL_FILTERS_PATH)\n",
    "aug_datasets_no_acceptability_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH)\n",
    "aug_datasets_no_similarity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_SIMILARITY_FILTER_PATH)\n",
    "aug_datasets_no_toxicity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_TOXICITY_FILTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_val_preds and df_val_metrics\n",
    "df_val_preds = pd.read_csv(VAL_PREDS_PATH)\n",
    "df_val_metrics = pd.read_csv(VAL_METRICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the time it takes to run a function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "    return gpu.memoryUsed\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total GPU memory: 23034.0MB\n",
      "Free GPU memory: 20193.0MB\n",
      "Used GPU memory: 2324.0MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2324.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model functions\n",
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    y_pred_delete = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        y_pred_delete.append(text)\n",
    "\n",
    "    return y_pred_delete\n",
    "\n",
    "def bart_detoxifier(text_list):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text using BART\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "    y_pred_bart = pipe_bart(text_list, max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    \n",
    "    return y_pred_bart\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(df, model_name, metrics, save_path=\"../data/processed/model_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a df if the input df is empty\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"Overall\"])\n",
    "\n",
    "    # Check if the model name already exists in the dataframe\n",
    "    if model_name in df[\"Model\"].values:\n",
    "        print(f\"Model {model_name} already exists in the dataframe.\")\n",
    "        return df\n",
    "    \n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"Overall\": [metrics[\"Overall\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=INCLUDE_BLEURT\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA\": sta_pred,\n",
    "        \"FLU\": acc_pred,\n",
    "        \"SEM\": bert_score_f1,\n",
    "        \"J\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preds_to_df(model_name, preds, raw_datasets=raw_datasets, use_validation=True, load_csv=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add model predictions to a pandas dataframe\n",
    "\n",
    "    Args:\n",
    "    - model_name: name of the model\n",
    "    - preds: list of predictions\n",
    "    - test_data: whether the data is test data or validation data (True for test data, False for validation data)\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    - replace_existing: whether to replace an existing column if it already exists\n",
    "\n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_PREDS_PATH\n",
    "        source = raw_datasets[\"validation\"][\"source\"]\n",
    "        target = raw_datasets[\"validation\"][\"target\"]\n",
    "    else:\n",
    "        save_path = TEST_PREDS_PATH\n",
    "        source = raw_datasets[\"test\"][\"source\"]\n",
    "        target = raw_datasets[\"test\"][\"target\"]\n",
    "\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "                \"source\": source,\n",
    "                \"target\": target,\n",
    "            })\n",
    "        \n",
    "    if f\"{model_name}_preds\" in df.columns and not replace_existing:\n",
    "        print(f\"Column {model_name}_preds already exists in the dataframe.\")\n",
    "        return df\n",
    "    \n",
    "    df[f\"{model_name}_preds\"] = preds\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_metric_cols_to_preds(preds_col_name, use_validation=True):\n",
    "    \"\"\"\n",
    "    Add metric columns to the dataframe\n",
    "\n",
    "    Args:\n",
    "    - preds_col_name: name of the column containing the predictions\n",
    "    - use_validation: whether to use validation data or test data\n",
    "\n",
    "    Returns:\n",
    "    - updated dataframe\n",
    "    \"\"\"\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_PREDS_PATH\n",
    "    else:\n",
    "        save_path = TEST_PREDS_PATH\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(save_path)\n",
    "\n",
    "    # Dynamically create column names\n",
    "    model_name = preds_col_name.split(\"_\")[0]\n",
    "    bleu_col_name = f\"{model_name}_BLEU\"\n",
    "    bleurt_col_name = f\"{model_name}_BLEURT\"\n",
    "    sta_col_name = f\"{model_name}_STA\"\n",
    "    flu_col_name = f\"{model_name}_FLU\"\n",
    "    sem_col_name = f\"{model_name}_SEM\"\n",
    "\n",
    "    # Calculate metrics\n",
    "    df[bleu_col_name] = df.apply(lambda row: calc_sacrebleu([row[\"target\"]], [row[preds_col_name]]), axis=1)\n",
    "    df[bleurt_col_name] = calc_bleurt(df[\"target\"], df[preds_col_name], output_mean=False)\n",
    "    df[sta_col_name] = 1 - calc_tox_acceptability(df[preds_col_name].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    df[flu_col_name] = calc_tox_acceptability(df[preds_col_name].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "    df[sem_col_name] = calc_bert_score(df[\"target\"], df[preds_col_name], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(model_name, metrics, use_validation=True, load_csv=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    - test_data: whether the data is test data or validation data (True for test data, False for validation data)\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    - replace_existing: whether to replace an existing column if it already exists\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_METRICS_PATH\n",
    "    else:\n",
    "        save_path = TEST_METRICS_PATH\n",
    "        \n",
    "    # Load the existing dataframe if it exists\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"J\"])\n",
    "\n",
    "    # Check if the model name already exists in the dataframe\n",
    "    if model_name in df[\"Model\"].values and not replace_existing:\n",
    "        print(f\"Model {model_name} already exists in the dataframe.\")\n",
    "        return df\n",
    "\n",
    "    # If replace existing, remove the existing row with the same model name\n",
    "    if model_name in df[\"Model\"].values and replace_existing:\n",
    "        df = df[df[\"Model\"] != model_name]\n",
    "\n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"J\": [metrics[\"J\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_words_list(dataset, tokenizer=tokenizer_toxicity, model=model_toxicity, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Gets the top k bad words for each sentence in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of sentences.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use (toxicity classifier).\n",
    "        model (PreTrainedModel): Model to use (toxicity classifier).\n",
    "        num_layers (int): Number of layers to use.\n",
    "        top_k (int): Number of bad words to return.\n",
    "\n",
    "    Returns:\n",
    "        bad_words_list (list): List of lists of bad words.\n",
    "    \"\"\"    \n",
    "    bad_words_list = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        # Tokenize sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        # Get attention scores\n",
    "        attention = model(input_ids, output_attentions=True)['attentions']\n",
    "\n",
    "        # Get the last num_layers layer attention scores and average them\n",
    "        attention = torch.stack(attention[-num_layers:]).mean(0)\n",
    "\n",
    "        # Average across each head\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Sum each row to get the attention score for each token\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Exclude separator tokens and punctuation\n",
    "        token_list = input_ids.squeeze().tolist()\n",
    "        punctuation_ids = {tokenizer.convert_tokens_to_ids(token) for token in string.punctuation}\n",
    "        exclude_ids = set([tokenizer.bos_token_id, tokenizer.eos_token_id]) | punctuation_ids\n",
    "\n",
    "        valid_indices = [i for i, token_id in enumerate(token_list) if token_id not in exclude_ids]\n",
    "\n",
    "        # Filter out the valid indices from the attention scores\n",
    "        valid_attention = attention.squeeze()[valid_indices]\n",
    "\n",
    "        # Get the indices of the top k tokens with the highest attention scores among valid tokens\n",
    "        top_indices = valid_attention.topk(top_k).indices.tolist()\n",
    "        top_token_indices = [valid_indices[i] for i in top_indices]\n",
    "\n",
    "        # Decode the tokens\n",
    "        bad_words = [tokenizer.decode(token_list[index]).strip() for index in top_token_indices]\n",
    "\n",
    "        bad_words_list.append(bad_words)\n",
    "\n",
    "    return bad_words_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Object Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    \"\"\"Adds a prefix to the source sequence in the dataset.\"\"\"\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def create_bidirectional_dataset(datasets, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    if shuffle:\n",
    "        extended_datasets[\"train\"] = extended_datasets[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "        \n",
    "    return extended_datasets\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    \"\"\"Preprocesses a dataset using a tokenizer.\"\"\"\n",
    "    def preprocess_function(examples, tokenizer):\n",
    "        \"\"\"Preprocess function for T5.\"\"\"\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"source\"],\n",
    "            text_target=examples[\"target\"],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "        batched=True,\n",
    "        remove_columns=[\"source\", \"target\"],\n",
    "    )\n",
    "\n",
    "def post_process(preds, refs, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5.\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        refs (list): list of reference sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "        decoded_refs (list): list of decoded reference sequences\n",
    "    \"\"\"\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    return decoded_preds, decoded_refs\n",
    "\n",
    "def post_process_preds(preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5 (only for predictions)\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "    \"\"\"\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "\n",
    "    return decoded_preds\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset to use for testing created using create_bidirectional_datasets\n",
    "                                  For example, raw_datasets_bd[\"validation\"] or raw_datasets_bd[\"test\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = [i for i, input_sentence in enumerate(bd_dataset['source']) if input_sentence.startswith(\"to_neutral\")]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                compute_metrics,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                report_to=\"wandb\",\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Evaluation dataset.\n",
    "        compute_metrics (function): Function to compute metrics. Change this to compute_metrics_bd if using a bi-directional model.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "        report_to (str): Where to report results to. Either \"wandb\" or \"none\".\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    gen_config_path = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(gen_config_path)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=gen_config_path,\n",
    "        fp16=True,\n",
    "        report_to=report_to,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "   \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def training_pipeline(model_name, project_name=\"t5-detox\", model_checkpoint=\"t5-small\", use_validation=True, raw_datasets=raw_datasets, bidirectional=False, shuffle=False, do_train=True):\n",
    "    \"\"\"\n",
    "    Pipeline for training a T5 model. Saves the best model checkpoint to a txt file. Can also be used for evaluating a model (use test set instead of validation set).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to name the output directory and wandb run.\n",
    "        project_name (str): Name of the wandb project.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        use_validation (bool): Whether to use the validation set or not.\n",
    "        raw_datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        bidirectional (bool): Whether to use a bi-directional model or not.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "        do_train (bool): Whether to train the model or not.\n",
    "\n",
    "    Returns:\n",
    "        trainer (Seq2SeqTrainer): Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess dataset (add prefixes / make bidirectional)\n",
    "    if bidirectional:\n",
    "        raw_datasets = create_bidirectional_dataset(raw_datasets, shuffle=shuffle)\n",
    "    else:\n",
    "        raw_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = preprocess_dataset(raw_datasets, tokenizer_t5_small)\n",
    "\n",
    "    # Define compute_metrics function depending on bidirectional or not\n",
    "    if bidirectional and use_validation:\n",
    "        bd_dataset = raw_datasets[\"validation\"]\n",
    "    elif bidirectional and not use_validation:\n",
    "        bd_dataset = raw_datasets[\"test\"]\n",
    "    else:\n",
    "        bd_dataset = None\n",
    "\n",
    "    compute_metrics_fn = partial(compute_metrics_bd, bd_dataset=bd_dataset, shuffled_data=shuffle) if bd_dataset else compute_metrics\n",
    "\n",
    "    # Setup trainer\n",
    "    trainer = setup_trainer(\n",
    "        output_dir_name=model_name,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"] if use_validation else tokenized_datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    if do_train:\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=project_name, name=model_name)\n",
    "        trainer.train()\n",
    "        wandb.finish()\n",
    "\n",
    "        # Get the best checkpoint path for the model\n",
    "        checkpoint_path = trainer.state.best_model_checkpoint\n",
    "\n",
    "        # Save the checkpoint path for the best model\n",
    "        with open(BEST_MODEL_CHECKPOINT_PATH, \"a\") as file:\n",
    "            file.write(f\"{model_name}: {checkpoint_path}\\n\")\n",
    "\n",
    "    return trainer, tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_checkpoints():\n",
    "    # Get checkpoint values for the best models\n",
    "    with open(BEST_MODEL_CHECKPOINT_PATH, \"r\") as f:\n",
    "        best_model_checkpoints = f.readlines()\n",
    "\n",
    "    # Convert to a dictionary\n",
    "    best_model_checkpoints_dict = {}\n",
    "    for line in best_model_checkpoints:\n",
    "        model_name, checkpoint_path = line.split(\": \")\n",
    "        best_model_checkpoints_dict[model_name] = checkpoint_path.strip()\n",
    "\n",
    "    return best_model_checkpoints_dict\n",
    "\n",
    "model_checkpoints = get_model_checkpoints()\n",
    "\n",
    "def get_t5_preds_metrics(model_checkpoint,\n",
    "                         raw_datasets=raw_datasets,\n",
    "                         bidirectional=False,\n",
    "                         shuffle=False,\n",
    "                         use_validation=True,\n",
    "                         do_train=False,\n",
    "                         tokenizer=tokenizer_t5_small\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Returns the predictions and metrics for a fine-tuned T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training pipeline\n",
    "    trainer, trainer_tokenized_ds = training_pipeline(\n",
    "        model_name=\"n/a\",\n",
    "        project_name=\"n/a\",\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        use_validation=use_validation,\n",
    "        raw_datasets=raw_datasets,\n",
    "        bidirectional=bidirectional,\n",
    "        shuffle=shuffle,\n",
    "        do_train=do_train\n",
    "    )\n",
    "\n",
    "    # Get raw predictions\n",
    "    if use_validation:\n",
    "        trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"validation\"])\n",
    "    else:\n",
    "        trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"test\"])\n",
    "\n",
    "    # Get encoded predictions and metrics\n",
    "    trainer_preds_encoded, trainer_metrics = trainer_preds_raw.predictions, trainer_preds_raw.metrics\n",
    "\n",
    "    # Post-process predictions\n",
    "    if isinstance(trainer_preds_encoded, tuple):\n",
    "        trainer_preds_encoded = trainer_preds_encoded[0]\n",
    "\n",
    "    trainer_preds_decoded = tokenizer.batch_decode(trainer_preds_encoded, skip_special_tokens=True)\n",
    "    trainer_preds_decoded = [pred.strip() for pred in trainer_preds_decoded]\n",
    "\n",
    "    #Return trainer metrics in the same format as evaluate_metrics\n",
    "    trainer_metrics = {\n",
    "        \"BLEU\": trainer_metrics[\"test_BLEU\"],\n",
    "        \"BLEURT\": trainer_metrics[\"test_BLEURT\"],\n",
    "        \"STA\": trainer_metrics[\"test_STA\"],\n",
    "        \"FLU\": trainer_metrics[\"test_FLU\"],\n",
    "        \"SEM\": trainer_metrics[\"test_SEM\"],\n",
    "        \"J\": trainer_metrics[\"test_J\"]\n",
    "    }\n",
    "        \n",
    "    # Return predictions and metrics\n",
    "    return trainer_preds_decoded, trainer_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_outputs(\n",
    "    df,\n",
    "    cols_to_compare = [\"source\", \"target\", \"BART_preds\", \"T5-UD-DA_preds\", \"T5-UD-DA-MinLoss_preds\"],\n",
    "    bad_words_cols = ['source', 'T5-UD_preds'],\n",
    "    num_examples = 5,\n",
    "    num_layers = 5,\n",
    "    top_k = 3,\n",
    "    print_toxic_words = True,\n",
    "    random_state = RANDOM_SEED,\n",
    "):\n",
    "    \"\"\"\"\"\"\n",
    "    # Randomly sample the filtered dataset\n",
    "    df_sample = df.sample(n=num_examples, random_state=random_state)\n",
    "\n",
    "    # Get toxic words for in bad_words_col_1, bad_words_col_2, and bad_words_col_3\n",
    "    if print_toxic_words and bad_words_cols is not None:\n",
    "        ## Initialize a dictionary of bad words\n",
    "        bad_words = {}\n",
    "\n",
    "        ## Get bad words for each model\n",
    "        for bad_words_col in bad_words_cols:\n",
    "            bad_words[bad_words_col] = get_bad_words_list(df_sample[bad_words_col], num_layers=num_layers, top_k=top_k)\n",
    "\n",
    "    # Print every line for each col in cols_to_compare, and the bad words for each col in bad_words_cols\n",
    "    for i in range(num_examples):\n",
    "        if cols_to_compare is not None:\n",
    "            for col in cols_to_compare:\n",
    "                # Format the column name to remove _preds and capitalise first letter\n",
    "                if col in ['source', 'target']:\n",
    "                    col_name = col.capitalize()\n",
    "                elif col.endswith(\"_preds\"):\n",
    "                    col_name = col[:-len(\"_preds\")]\n",
    "                print(f\"{col_name}: {df_sample[col].iloc[i]}\")\n",
    "        if bad_words_cols is not None:\n",
    "            for bad_words_col in bad_words_cols:\n",
    "                # Format the column name to remove _preds and capitalise first letter\n",
    "                if bad_words_col in ['source', 'target']:\n",
    "                    bad_words_col_name = bad_words_col.capitalize()\n",
    "                elif bad_words_col.endswith(\"preds\"):\n",
    "                    bad_words_col_name = bad_words_col[:-len(\"_preds\")]\n",
    "                print(f\"'Toxic' words in {bad_words_col_name}: {bad_words[bad_words_col][i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Using Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': 1.0000000000000004,\n",
       " 'STA': 0.9538977367979883,\n",
       " 'FLU': 0.7160852,\n",
       " 'SEM': 0.9999999969023571,\n",
       " 'J': 0.9247761332079433,\n",
       " 'BLEURT': 0.9892257596401237}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], raw_datasets[\"validation\"][\"target\"])\n",
    "human_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n",
      "/tmp/ipykernel_15355/1601193926.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, model_metrics_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "delete_val_preds = baseline_detoxifier(raw_datasets[\"validation\"][\"source\"])\n",
    "df_val_preds = add_preds_to_df(\"DELETE\", delete_val_preds, use_validation=True, load_csv=False)\n",
    "\n",
    "delete_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], delete_val_preds)\n",
    "df_val_metrics = add_metrics_to_df(\"DELETE\", delete_val_metrics, use_validation=True, load_csv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bart_val_preds = bart_detoxifier(raw_datasets[\"validation\"][\"source\"])\n",
    "df_val_preds = add_preds_to_df(\"BART\", bart_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "bart_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], bart_val_preds)\n",
    "df_val_metrics = add_metrics_to_df(\"BART\", bart_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_preds = add_metric_cols_to_preds(\"BART_preds\", use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0  DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1    BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adadc36bd7f94bce9fb3bda716a333df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42000f2c225f479a83b962b76cad0465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a1099f9bb7e460fb64f6c8ba7736f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_val_preds, t5_ud_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe and per-row metrics\n",
    "df_val_preds = add_preds_to_df(\"T5-UD\", t5_ud_val_preds, use_validation=True, load_csv=True)\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD_preds\", use_validation=True)\n",
    "\n",
    "# Add aggregated metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD\", t5_ud_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this example, the largest differences between BART and T5-Small-UD are in relation to BLEURT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of BLEURT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Most people understand trumps comments for what they were the whining of a sore loser.\n",
      "Target: Most people understand trumps comments were result of his defeat.\n",
      "BART: Most people understand trumps comments for what they were.\n",
      "T5-UD: Most people understand trumps comments for what they were the bad of a loser.\n",
      "\n",
      "Source: But then the incredibly stupid can continue to ignore the obvious.\n",
      "Target: But then those who don't know anything can continue to not pay attention to the obvious.\n",
      "BART: But then they can continue to ignore the obvious.\n",
      "T5-UD: But then the incredibly bad can continue to ignore the obvious.\n",
      "\n",
      "Source: Rt ima agree with you on that his shit is so bland and shallow\n",
      "Target: I agree with you that his performance is bland and shallow\n",
      "BART: I agree with you that his performance is bland and shallow\n",
      "T5-UD: Rt ima agree with you on that his thing is so bland and shallow\n",
      "\n",
      "Source: No bitch i miss being 80 you on the other hand are still a \" teen\n",
      "Target: No girl i miss being 80 you on the other hand are still a \" teen\n",
      "BART: No girl I miss being 80 you on the other hand are still a \"teen\n",
      "T5-UD: I miss being 80 you on the other hand are still a \" teen\n",
      "\n",
      "Source: Try thinking before you type, it makes you appear less foolish.\n",
      "Target: Try thinking before you type, it makes you more smart.\n",
      "BART: Try thinking before you type, it makes you more smart.\n",
      "T5-UD: Try thinking before you type, it makes you appear less foolish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART has better BLEURT score than T5-UD\n",
    "df_BLEURT_BART_vs_T5_UD = df_val_preds[(df_val_preds[\"BART_BLEURT\"] >= df_val_preds[\"T5-UD_BLEURT\"] * 1.2) & (df_val_preds[\"BART_BLEURT\"] != df_val_preds[\"T5-UD_BLEURT\"])]\n",
    "\n",
    "# Compmare outputs\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_BART_vs_T5_UD,\n",
    "    cols_to_compare=['source', 'target', 'BART_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Who the fuck cuts out part of the state of the union?\n",
      "Target: Who cuts out part of the state of tge union.\n",
      "BART: Who cuts out part of the state of the union?\n",
      "T5-UD: Who cuts out part of the state of the union?\n",
      "'Toxic' words in ourc: ['fuck', 'the', 'part']\n",
      "'Toxic' words in T5-UD: ['state', 'cuts', 'part']\n",
      "\n",
      "Source: Nah yu seen how i act n public sober n shit\n",
      "Target: Nah you’ve seen how i act in public, sober and all.\n",
      "BART: Nah yu seen how i act in public sober\n",
      "T5-UD: Nah you have seen how i act n public sober\n",
      "'Toxic' words in ourc: ['shit', 'i', 'u']\n",
      "'Toxic' words in T5-UD: ['i', 'sober', 'n']\n",
      "\n",
      "Source: Shit chillen fresh out the shower\n",
      "Target: Chillen fresh out of the shower\n",
      "BART: Chillen fresh out of the shower\n",
      "T5-UD: Coolen fresh out the shower\n",
      "'Toxic' words in ourc: ['it', 'Sh', 'chill']\n",
      "'Toxic' words in T5-UD: ['Cool', 'en', 'shower']\n",
      "\n",
      "Source: These repubicans are like cannibalistic pirañas.\n",
      "Target: These republicans are taking advantage.\n",
      "BART: These republicans are not good.\n",
      "T5-UD: These repubicans are like cannibalistic people.\n",
      "'Toxic' words in ourc: ['cannibal', 'are', 'icans']\n",
      "'Toxic' words in T5-UD: ['cannibal', 'are', 'icans']\n",
      "\n",
      "Source: I mean very irritating to see females fightin over some dick\n",
      "Target: Its irritating to see females fighting over boys\n",
      "BART: It is very irritating to see females fighting over boys\n",
      "T5-UD: I mean very irritating to see females fightin over something\n",
      "'Toxic' words in ourc: ['dick', 'females', 'some']\n",
      "'Toxic' words in T5-UD: ['females', 'fight', 'irritating']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART is better than T5-Small-UD in terms of STA\n",
    "df_STA_BART_vs_T5_UD = df_val_preds[(df_val_preds[\"BART_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_BART_vs_T5_UD,\n",
    "    cols_to_compare=['source', 'target', 'BART_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=['source', 'T5-UD_preds'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c8c4a83c765452e6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-564d1ea8c297b791.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-d7d403f057f73219.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_bd_val_preds, t5_bd_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_bidir_shuf\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-BD\", t5_bd_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add per-row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-BD_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-BD\", t5_bd_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0  DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1    BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2   T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055\n",
       "3   T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of BLEURT vs. T5-Small-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Lol fuck im ever doing that formsprings, perfect exampleas why not to do it\n",
      "Target: Lol im ever doing that formsprings, perfect exampleas why not to do it\n",
      "T5-UD: I'm ever doing that formsprings, perfect exampleas why not to do it\n",
      "T5-BD: Lol im ever doing that formsprings, perfect exampleas why not to do it\n",
      "\n",
      "Source: Hey, respect to op for sticking around and not giving a shit\n",
      "Target: Hey, respect to op for sticking around and not caring about it.\n",
      "T5-UD: Hey, respect to op for sticking around and not giving a thing\n",
      "T5-BD: Hey, respect to op for sticking around and not caring\n",
      "\n",
      "Source: Im having the hardest time accepting that shit man!!! irefuse to believe this shit\n",
      "Target: I'm having the hardest time accepting that man, I refuse to believe this.\n",
      "T5-UD: Im having the hardest time accepting that man!!! irefuse to believe this man\n",
      "T5-BD: I am having the hardest time accepting that man!!! irefuse to believe this\n",
      "\n",
      "Source: Icantdateyou if ur hairy af - _ - fucks noo! lol\n",
      "Target: I cant date you if ur hairy\n",
      "T5-UD: I cant date you if you are hairy noo! lol\n",
      "T5-BD: I cant date you if ur hairy noo! lol\n",
      "\n",
      "Source: | smh they really be stashing and don't tell us shit\n",
      "Target: | smh they really be stashing and don't tell us a word\n",
      "T5-UD: They really be stashing and don't tell us anything\n",
      "T5-BD: | smh they really be stashing and don't tell us anything\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BLEURT of T5-Small-BD exceeds T5-Small-UD by 20% and are not identical\n",
    "df_BLEURT_T5_BD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-BD_BLEURT\"] >= df_val_preds[\"T5-UD_BLEURT\"] * 1.2) & (df_val_preds[\"T5-BD_BLEURT\"] != df_val_preds[\"T5-UD_BLEURT\"])]\n",
    "\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_T5_BD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD_preds','T5-BD_preds'],\n",
    "    bad_words_cols=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of STA vs. T5-Small-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: You already burned down to a crisp in fort mac by your own stupidity, already.\n",
      "Target: You already lost in fort mac.\n",
      "T5-UD: You already burned down to a crisp in fort mac by your own ignorance, already.\n",
      "T5-BD: You already burned down to a crisp in fort mac by yourself, already.\n",
      "'Toxic' words in Source: ['stupidity', 'burned']\n",
      "'Toxic' words in T5-UD: ['You', 'burned']\n",
      "\n",
      "Source: True conservatives, that are not white racists, will have to find a new home.\n",
      "Target: Correct\n",
      "T5-UD: True conservatives, that are not white, will have to find a new home.\n",
      "T5-BD: True conservatives will have to find a new home.\n",
      "'Toxic' words in Source: ['white', 'racists']\n",
      "'Toxic' words in T5-UD: ['white', 'True']\n",
      "\n",
      "Source: So you would love to see the commys win over a duly elected democratic president, man you are sick\n",
      "Target: So you would love to see the communist win over a duly elected democratic president\n",
      "T5-UD: So you would love to see the commys win over a duly elected democratic president, man you are sick\n",
      "T5-BD: So you would love to see the commys win over a duly elected democratic president\n",
      "'Toxic' words in Source: ['sick', 'you']\n",
      "'Toxic' words in T5-UD: ['sick', 'you']\n",
      "\n",
      "Source: Shut the fuck up, geek!\n",
      "Target: Please, keep quiet!\n",
      "T5-UD: Shut up.\n",
      "T5-BD: Keep quiet, geek!\n",
      "'Toxic' words in Source: ['fuck', 'Shut']\n",
      "'Toxic' words in T5-UD: ['Shut', 'up']\n",
      "\n",
      "Source: We are done with the unamerican, disrespectful denver broncos!\n",
      "Target: We are done with them\n",
      "T5-UD: We are done with the unamerican, disrespectful denver broncos!\n",
      "T5-BD: We are done with the unamerican denver broncos!\n",
      "'Toxic' words in Source: ['disrespectful', 'the']\n",
      "'Toxic' words in T5-UD: ['disrespectful', 'the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART is better than T5-Small-UD in terms of STA\n",
    "df_STA_T5_BD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-BD_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_BD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD_preds','T5-BD_preds'],\n",
    "    bad_words_cols=['source','T5-UD_preds'],\n",
    "    num_examples=5,\n",
    "    num_layers=5,\n",
    "    top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional, Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c8c4a83c765452e6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-564d1ea8c297b791.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-d7d403f057f73219.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_val_preds, t5_ud_da_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_all\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA\", t5_ud_da_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-DA_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA\", t5_ud_da_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Filters (Minimum Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e53273a7b74dac91313a3e0502e75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8d3533741b498a8c3e30d441e9b90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf3220e78364676b7054b3a5ca3d44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_minloss_val_preds, t5_ud_da_minloss_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=\"../models/t5_small_aug_all/checkpoint-972\",\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-MinLoss\", t5_ud_da_minloss_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-DA-MinLoss_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "# df_val_metrics = add_metrics_to_df(\"T5-Small-Aug-All-MinLoss\", t5_ud_da_minloss_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Outputs of All Filter vs. All Filter (Minimum Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Bitch was so mad lmfao!! smh\n",
      "Target: She was so mad!\n",
      "T5-UD-DA-MinLoss: Bitch was so mad\n",
      "T5-UD-DA: He was so mad.\n",
      "'Toxic' words in Source: ['itch', 'B', 'was']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['itch', 'was', 'B']\n",
      "\n",
      "Source: Pitiful choice scalia was a hypocrite!\n",
      "Target: Scalia was a bad person\n",
      "T5-UD-DA-MinLoss: Choice scalia was a hypocrite!\n",
      "T5-UD-DA: Choice scalia was not a good person!\n",
      "'Toxic' words in Source: ['sc', 'was', 'hypocr']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['was', 'sc', 'hypocr']\n",
      "\n",
      "Source: He's a fucking street thug.\n",
      "Target: He's a street gang member.\n",
      "T5-UD-DA-MinLoss: He's a street thug\n",
      "T5-UD-DA: He's a street guy.\n",
      "'Toxic' words in Source: ['fucking', 'He', \"'s\"]\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['He', 'thug', \"'s\"]\n",
      "\n",
      "Source: Doosh doosh doosh doosh doosh fucking cock sucker queir bate ass wipe dont tell me what the fuck to write\n",
      "Target: Don't tell me what to write.\n",
      "T5-UD-DA-MinLoss: Doosh doosh doosh doosh cock sucker queir bate wipe dont tell me what to write\n",
      "T5-UD-DA: Don't tell me what to write\n",
      "'Toxic' words in Source: ['fucking', 'dont', 'fuck']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['dont', 'sucker', 'cock']\n",
      "\n",
      "Source: True conservatives, that are not white racists, will have to find a new home.\n",
      "Target: Correct\n",
      "T5-UD-DA-MinLoss: True conservatives, that are not white, will have to find a new home.\n",
      "T5-UD-DA: True conservatives, that are not racists, will have to find a new home.\n",
      "'Toxic' words in Source: ['white', 'racists', 'True']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['white', 'True', 'are']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to rows where T5-UD-DA-MinLoss has STA == 0 and T5-UD-DA has STA == 1\n",
    "df_STA_T5_UD_DA_MinLoss_vs_T5_UD_DA = df_val_preds[(df_val_preds[\"T5-UD-DA-MinLoss_STA\"] == 0) & (df_val_preds[\"T5-UD-DA_STA\"] == 1)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_UD_DA_MinLoss_vs_T5_UD_DA,\n",
    "    cols_to_compare=['source','target','T5-UD-DA-MinLoss_preds', 'T5-UD-DA_preds'],\n",
    "    bad_words_cols=['source','T5-UD-DA-MinLoss_preds'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Toxicity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_notox_val_preds, t5_ud_da_notox_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_notox\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoTOX\", t5_ud_da_notox_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoTOX\", t5_ud_da_notox_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Semantic Similarity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_nosem_val_preds, t5_ud_da_nosem_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_nosim\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoSEM\", t5_ud_da_nosem_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoSEM\", t5_ud_da_nosem_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Fluency Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_noflu_val_preds, t5_ud_da_noflu_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_noaccept\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoFLU\", t5_ud_da_noflu_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoFLU\", t5_ud_da_noflu_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-UD-DA</td>\n",
       "      <td>0.204206</td>\n",
       "      <td>0.593218</td>\n",
       "      <td>0.916178</td>\n",
       "      <td>0.714662</td>\n",
       "      <td>0.925547</td>\n",
       "      <td>0.813157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5-UD-DA-NoTOX</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.598625</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.713698</td>\n",
       "      <td>0.925555</td>\n",
       "      <td>0.804659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T5-UD-DA-NoSEM</td>\n",
       "      <td>0.211811</td>\n",
       "      <td>0.591599</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.719671</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.814390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T5-UD-DA-NoFLU</td>\n",
       "      <td>0.192281</td>\n",
       "      <td>0.595025</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.709992</td>\n",
       "      <td>0.925311</td>\n",
       "      <td>0.807508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0          DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1            BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2           T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230\n",
       "3        T5-UD-DA  0.204206  0.593218  0.916178  0.714662  0.925547  0.813157\n",
       "4  T5-UD-DA-NoTOX  0.208252  0.598625  0.892707  0.713698  0.925555  0.804659\n",
       "5  T5-UD-DA-NoSEM  0.211811  0.591599  0.917854  0.719671  0.924972  0.814390\n",
       "6  T5-UD-DA-NoFLU  0.192281  0.595025  0.903604  0.709992  0.925311  0.807508\n",
       "7           T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional, Negative Lexically Constrained Decoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-fa9991aa4e1e4311.arrow\n",
      "100%|██████████| 38/38 [02:17<00:00,  3.62s/it]\n"
     ]
    }
   ],
   "source": [
    "def get_bad_word_ids(dataset,\n",
    "                     tokenizer_toxicity=tokenizer_toxicity,\n",
    "                     model_toxicity=model_toxicity,\n",
    "                     tokenizer_t5=tokenizer_t5_small,\n",
    "                     num_layers=3,\n",
    "                     top_k=3):\n",
    "    \n",
    "    # Get list of bad words as identified using attention from toxicity classifier\n",
    "    bad_words_list = get_bad_words_list(dataset, tokenizer_toxicity, model_toxicity, num_layers, top_k)\n",
    "\n",
    "    # Convert each list of bad words to a string\n",
    "    bad_words_str_list = [\" \".join(bad_words) for bad_words in bad_words_list]\n",
    "\n",
    "    # Encode the bad words using the T5 tokenizer encode\n",
    "    bad_word_ids = [tokenizer_t5.encode(bad_words, add_special_tokens=False) for bad_words in bad_words_str_list]\n",
    "\n",
    "    return bad_word_ids\n",
    "\n",
    "def get_preds_nlcd(use_validation,\n",
    "                   model_checkpoint,\n",
    "                   raw_datasets=raw_datasets,\n",
    "                   batch_size=32,  # Set your desired batch size here\n",
    "                   num_beams=NUM_BEAMS,\n",
    "                   max_length=MAX_OUTPUT_LENGTH,\n",
    "                   num_batches=None):\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(DEVICE)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define raw dataset\n",
    "    raw_sentences = raw_datasets[\"validation\"][\"source\"] if use_validation else raw_datasets[\"test\"][\"source\"]\n",
    "\n",
    "    # Define bad word ids\n",
    "    bad_word_ids = get_bad_word_ids(raw_sentences)\n",
    "\n",
    "    # Preprocess dataset\n",
    "    tokenized_sentences = preprocess_dataset(raw_datasets[\"validation\"], tokenizer) if use_validation else preprocess_dataset(raw_datasets[\"test\"], tokenizer)\n",
    "\n",
    "    # Determine the number of batches\n",
    "    if num_batches is None:\n",
    "        num_batches = (len(tokenized_sentences) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Initialize a list to store all predictions\n",
    "    all_preds = []\n",
    "\n",
    "    # Process data in batches\n",
    "    for batch_idx in tqdm(range(num_batches)):  # Add tqdm here\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "        # Extract a batch of tokenized sentences\n",
    "        batch_tokenized_sentences = tokenized_sentences[start_idx:end_idx]\n",
    "\n",
    "        # Convert from Dataset to a list of dictionaries\n",
    "        batch_tokenized_sentences_list = []\n",
    "        for i in range(len(batch_tokenized_sentences)):\n",
    "            tokenized_sentence_dict = {\n",
    "                'input_ids': batch_tokenized_sentences['input_ids'][i],\n",
    "                'attention_mask': batch_tokenized_sentences['attention_mask'][i],\n",
    "                'labels': batch_tokenized_sentences['labels'][i]\n",
    "            }\n",
    "            batch_tokenized_sentences_list.append(tokenized_sentence_dict)\n",
    "\n",
    "        # Apply data collator to the batch\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "        input_ids = data_collator(batch_tokenized_sentences_list)[\"input_ids\"].to(DEVICE)\n",
    "\n",
    "        # Generate predictions for the batch\n",
    "        encoded_preds = model.generate(\n",
    "            inputs=input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            pad_token_id=model.config.pad_token_id,\n",
    "            decoder_start_token_id=model.config.pad_token_id,\n",
    "            bad_words_ids=bad_word_ids\n",
    "        )\n",
    "\n",
    "        # Decode the predictions for the batch\n",
    "        decoded_preds = tokenizer.batch_decode(encoded_preds, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process the predictions for the batch\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "\n",
    "        # Append the batch predictions to the list of all predictions\n",
    "        all_preds.extend(decoded_preds)\n",
    "\n",
    "    return all_preds\n",
    "\n",
    "t5_ud_nlcd_preds = get_preds_nlcd(use_validation=True, model_checkpoint=model_checkpoints['t5_small_unidir'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I love it tho',\n",
       " 'Redditor for 26 minutes leave',\n",
       " \"Yup then i'll unfollow you, just to be bad\",\n",
       " 'He said you were bieng.',\n",
       " 'Your shxt is just flat like a piece of paper ma niggah',\n",
       " \"As always i don't get you lol it's summer enjoy that only comes once a yr\",\n",
       " \"You get a warrant!!! it's not hard dude.\",\n",
       " \"Someone put these towers up right under everyone's noses.\",\n",
       " 'Hillary was not a bad candidate at least when compared to him.',\n",
       " 'Not to mention the amount of people that are going to get removed.',\n",
       " 'Republicans just have the thing removed.',\n",
       " \"He's not disagreeing, he's just saying it's bad.\",\n",
       " 'I wish I was going to belfast',\n",
       " 'Ohhhhh ok lmao idk about dogs so.',\n",
       " 'What year are we living in again?',\n",
       " 'At you local diner getting burgers and shakes with coupons',\n",
       " \"It's hot man\",\n",
       " \"I will lol I'm very observant\",\n",
       " 'Anybody who does anything with the anc in business or anything else must know they are being punished!',\n",
       " 'I just hit a jay, and forget about that.',\n",
       " \"Texas - they don't put up with anything.\",\n",
       " 'I call this \"the pass \".',\n",
       " 'You voted for that person,Trumpski.',\n",
       " 'The rebel has no respect for them.',\n",
       " 'That credit card was the funniest',\n",
       " 'You are a liberal',\n",
       " 'We can cure some bacterial infections, we can also cure some ham with that.',\n",
       " 'Just called me a \"fantastic \"',\n",
       " 'We get these people pushed to the forefront because america has a majority of ignorant nonreaders, eric norstog',\n",
       " \"You're just bored just like me.\",\n",
       " 'Klinton lost because she is a major and joe average figured that out quickly.',\n",
       " 'This makes me feel so helpless.',\n",
       " 'Your name is accurate.',\n",
       " \"Slightly off topic - drew carry's jokes were some of my favorite jokes ever.\",\n",
       " 'Like a man ever left up out this guy',\n",
       " 'Once born they stop giving.',\n",
       " \"Don't care what people tweet.\",\n",
       " 'But in order to protect myself and my loved ones I will dokill them, and legally.',\n",
       " 'Has written some good stuff in the past but this is just bad.',\n",
       " \"I'm constantly surprised by the amount of dubious things my cousins that age get up to.\",\n",
       " 'Said that first drip was the problem!!!',\n",
       " \"It's obvious that he's gonna win! lol i voted off already d\",\n",
       " 'Nasty liberal media is picking on donald by reporting what hes said.',\n",
       " 'I want to mess her up!!',\n",
       " 'And all alabama fans are real talk',\n",
       " 'Get rid of them and other likeminded people and well get somewhere.',\n",
       " 'These people never realize we dont have free speech.',\n",
       " 'Our entire reputation would go bad',\n",
       " 'You lie like a bad dog',\n",
       " \"I have an interview next week and they've specifically requested a print based portfolio\",\n",
       " 'Stop sending me nonsense',\n",
       " \"Man i had a whole rant how i hate when it's pronounced like that.\",\n",
       " 'China is behind yelling calm down Kimmy.',\n",
       " 'They good play bad play!',\n",
       " 'My lashes are already bad! just thin',\n",
       " 'You still missed like half the thing ive changed.',\n",
       " 'How ya been? i see you been tweetin some inspirational things lately lol.',\n",
       " \"I'm starving can i come over?!\",\n",
       " \"Though if you have ebola you're basically spitting and throwing up blood so\",\n",
       " 'Will nobody rid me of this director?',\n",
       " 'Your wimp is traveling across canada instead of building trade strategy.',\n",
       " 'Lucky, this morons parents could afford high priced lawyers to allow him to get slapped on the wrist',\n",
       " 'I wish I could sleep right now',\n",
       " 'Right, is why.',\n",
       " \"It's really bad right now.\",\n",
       " 'Needs to happen by 27 or we are in trouble',\n",
       " \"I'm not yo man dont ring my cellular that early!\",\n",
       " 'We do things like that.',\n",
       " 'Megyn kelly brought that same dude on again so she could punish him a second time',\n",
       " 'Take the time to do the research, and stop posting online.',\n",
       " 'You gotta be messing with me.',\n",
       " 'He murdered 27 innocent people, who cares what his motivations were?',\n",
       " \"Because every thing i see on facebook makes me sad i couldn't help it.\",\n",
       " 'That grass messed up my nose',\n",
       " 'Fired over a joke',\n",
       " 'But it gets old up here sometimes too.',\n",
       " 'Why do schools continue to do this when a family has already successfully sued for a similar situation?',\n",
       " 'All of the above but i ate lol u got class today?',\n",
       " 'Avakov does not care about agreements made by poroshenko.',\n",
       " \"You gunna park close to my car again so I can't get in?\",\n",
       " 'I sware i put music on it i dunno wer it went bad!!',\n",
       " 'Racism is played out and over pointed',\n",
       " 'Why not move to russia?',\n",
       " 'Henry you deserve my sympathy as you are totally unaware of your ignorance.',\n",
       " 'That sick person is going to be out in 54 years.',\n",
       " 'There is no excuse for having nothing is more visually appealing to the imagination',\n",
       " 'Thank you for clearly demonstrating your lack of a functioning brain to everybody.',\n",
       " \"You don't know my life!\",\n",
       " \"You can't wear glasses like those and be completely unsuitable.\",\n",
       " 'I read that as more of a \"what, i\\'ve done it again \"',\n",
       " 'Hey guys who cares',\n",
       " 'Get yourself back here before owning yourself!',\n",
       " 'So murkowski and sullivan have been bad for years.',\n",
       " 'Great that we only start playing with 88 mins left! the service into the box was bad',\n",
       " 'Probably because the source, content, html layout and title of that website is incomplete.',\n",
       " 'What did you think would happen?',\n",
       " \"Yeahhh right husband you always mess me up wen I'm at work pay back  29\",\n",
       " 'Who cares?',\n",
       " 'People get lost or stranded because they are not smart.',\n",
       " \"That's a huge stretch and you know it.\",\n",
       " \"When you're emotional like a pmsing, boy, you cannot analyze any information correctly.\",\n",
       " 'chillen fresh out the shower',\n",
       " 'Well you too, cbs.',\n",
       " 'Should have thought out a few more moves ahead of their tactics.',\n",
       " \"I'm mean, he's got to have at least 71 already because he cares so much.\",\n",
       " 'Stop playing good cop bad cop with your dahn hack forestgarden',\n",
       " 'Meet me n da streets its going down last ones standing baddest woman wins da crown',\n",
       " 'Nah yu seen how i act n public sober',\n",
       " \"Yet we oppose automated cars so much because we think we're bad.\",\n",
       " \"I don't mess with them red bull wizards lol\",\n",
       " 'Yeah, shes like one of those pretty can whores!',\n",
       " 'Im not going to admit to somehting i didnt do.',\n",
       " 'She was so mad.',\n",
       " 'Im trying to keep my mentions clean keep your mentions out']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t5_ud_nlcd_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (114) does not match length of index (1193)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb Cell 63\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m df_val_preds \u001b[39m=\u001b[39m add_preds_to_df(\u001b[39m\"\u001b[39;49m\u001b[39mT5-UD-NLCD\u001b[39;49m\u001b[39m\"\u001b[39;49m, t5_ud_nlcd_preds, use_validation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, load_csv\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb Cell 63\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mColumn \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m_preds already exists in the dataframe.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m df[\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mmodel_name\u001b[39m}\u001b[39;49;00m\u001b[39m_preds\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m preds\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m df\u001b[39m.\u001b[39mto_csv(save_path, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y135sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4094\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4091\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   4092\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4093\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 4094\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:4303\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4293\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4294\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4295\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4301\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4302\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4303\u001b[0m     value, refs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4305\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4306\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4307\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4308\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value\u001b[39m.\u001b[39mdtype, ExtensionDtype)\n\u001b[1;32m   4309\u001b[0m     ):\n\u001b[1;32m   4310\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4311\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5042\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   5039\u001b[0m     \u001b[39mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex)\n\u001b[1;32m   5041\u001b[0m \u001b[39mif\u001b[39;00m is_list_like(value):\n\u001b[0;32m-> 5042\u001b[0m     com\u001b[39m.\u001b[39;49mrequire_length_match(value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex)\n\u001b[1;32m   5043\u001b[0m \u001b[39mreturn\u001b[39;00m sanitize_array(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_2d\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m), \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    557\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[39mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(data) \u001b[39m!=\u001b[39m \u001b[39mlen\u001b[39m(index):\n\u001b[0;32m--> 561\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLength of values \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(data)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdoes not match length of index \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(index)\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    566\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (114) does not match length of index (1193)"
     ]
    }
   ],
   "source": [
    "df_val_preds = add_preds_to_df(\"T5-UD-NLCD\", t5_ud_nlcd_preds, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
