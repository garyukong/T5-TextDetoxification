{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 16:30:36.014269: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-23 16:30:36.014321: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-23 16:30:36.020193: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "from numba import cuda\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "from typing import Dict, Union, Optional, Tuple, List, Any\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Default parameters for T5 model fine-tuning\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 64\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "# Include BLEURT score in evaluation\n",
    "INCLUDE_BLEURT = True\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "AUG_DATASET_ALL_FILTERS_PATH = \"../data/processed/aug_datasets_all_filters\"\n",
    "AUG_DATASET_NO_TOXICITY_FILTER_PATH = \"../data/processed/aug_datasets_no_toxicity_filter\"\n",
    "AUG_DATASET_NO_SIMILARITY_FILTER_PATH = \"../data/processed/aug_datasets_no_similarity_filter\"\n",
    "AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH = \"../data/processed/aug_datasets_no_acceptability_filter\"\n",
    "\n",
    "# Set path for txt file containing best model checkpoints\n",
    "BEST_MODEL_CHECKPOINT_PATH = \"../models/best_model_checkpoints.txt\"\n",
    "\n",
    "# Set path to save evaluation outputs to\n",
    "VAL_PREDS_PATH = \"../data/interim/val_preds.csv\"\n",
    "VAL_METRICS_PATH = \"../data/interim/val_metrics.csv\"\n",
    "TEST_PREDS_PATH = \"../data/final/test_preds.csv\"\n",
    "TEST_METRICS_PATH = \"../data/final/test_metrics.csv\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and models\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(DEVICE)\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "aug_datasets_all_filters = DatasetDict.load_from_disk(AUG_DATASET_ALL_FILTERS_PATH)\n",
    "aug_datasets_no_acceptability_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH)\n",
    "aug_datasets_no_similarity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_SIMILARITY_FILTER_PATH)\n",
    "aug_datasets_no_toxicity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_TOXICITY_FILTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_val_preds and df_val_metrics\n",
    "df_val_preds = pd.read_csv(VAL_PREDS_PATH)\n",
    "df_val_metrics = pd.read_csv(VAL_METRICS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model checkpoints\n",
    "def get_model_checkpoints():\n",
    "    with open(BEST_MODEL_CHECKPOINT_PATH, \"r\") as f:\n",
    "        best_model_checkpoints = f.readlines()\n",
    "\n",
    "    best_model_checkpoints_dict = {}\n",
    "    for line in best_model_checkpoints:\n",
    "        model_name, checkpoint_path = line.split(\": \")\n",
    "        best_model_checkpoints_dict[model_name] = checkpoint_path.strip()\n",
    "\n",
    "    return best_model_checkpoints_dict\n",
    "\n",
    "model_checkpoints = get_model_checkpoints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the time it takes to run a function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "    return gpu.memoryUsed\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model functions\n",
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    y_pred_delete = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        y_pred_delete.append(text)\n",
    "\n",
    "    return y_pred_delete\n",
    "\n",
    "def bart_detoxifier(text_list):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text using BART\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "    y_pred_bart = pipe_bart(text_list, max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    \n",
    "    return y_pred_bart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=INCLUDE_BLEURT\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA\": sta_pred,\n",
    "        \"FLU\": acc_pred,\n",
    "        \"SEM\": bert_score_f1,\n",
    "        \"J\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preds_to_df(model_name, preds, raw_datasets=raw_datasets, use_validation=True, load_csv=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add model predictions to a pandas dataframe\n",
    "\n",
    "    Args:\n",
    "    - model_name: name of the model\n",
    "    - preds: list of predictions\n",
    "    - test_data: whether the data is test data or validation data (True for test data, False for validation data)\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    - replace_existing: whether to replace an existing column if it already exists\n",
    "\n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_PREDS_PATH\n",
    "        source = raw_datasets[\"validation\"][\"source\"]\n",
    "        target = raw_datasets[\"validation\"][\"target\"]\n",
    "    else:\n",
    "        save_path = TEST_PREDS_PATH\n",
    "        source = raw_datasets[\"test\"][\"source\"]\n",
    "        target = raw_datasets[\"test\"][\"target\"]\n",
    "\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "                \"source\": source,\n",
    "                \"target\": target,\n",
    "            })\n",
    "    \n",
    "    # If replace existing, remove the existing column with the same model name\n",
    "    if f\"{model_name}_preds\" in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[f\"{model_name}_preds\"])\n",
    "    \n",
    "    df[f\"{model_name}_preds\"] = preds\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_metric_cols_to_preds(preds_col_name, use_validation=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add metric columns to the dataframe\n",
    "\n",
    "    Args:\n",
    "    - preds_col_name: name of the column containing the predictions\n",
    "    - use_validation: whether to use validation data or test data\n",
    "\n",
    "    Returns:\n",
    "    - updated dataframe\n",
    "    \"\"\"\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_PREDS_PATH\n",
    "    else:\n",
    "        save_path = TEST_PREDS_PATH\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(save_path)\n",
    "\n",
    "    # Dynamically create column names\n",
    "    model_name = preds_col_name.split(\"_\")[0]\n",
    "    bleu_col_name = f\"{model_name}_BLEU\"\n",
    "    bleurt_col_name = f\"{model_name}_BLEURT\"\n",
    "    sta_col_name = f\"{model_name}_STA\"\n",
    "    flu_col_name = f\"{model_name}_FLU\"\n",
    "    sem_col_name = f\"{model_name}_SEM\"\n",
    "\n",
    "    # If replace existing, remove the existing columns\n",
    "    if bleu_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[bleu_col_name])\n",
    "    if bleurt_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[bleurt_col_name])\n",
    "    if sta_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[sta_col_name])\n",
    "    if flu_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[flu_col_name])\n",
    "    if sem_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[sem_col_name])\n",
    "\n",
    "    # Calculate metrics\n",
    "    df[bleu_col_name] = df.apply(lambda row: calc_sacrebleu([row[\"target\"]], [row[preds_col_name]]), axis=1)\n",
    "    df[bleurt_col_name] = calc_bleurt(df[\"target\"], df[preds_col_name], output_mean=False)\n",
    "    df[sta_col_name] = 1 - calc_tox_acceptability(df[preds_col_name].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    df[flu_col_name] = calc_tox_acceptability(df[preds_col_name].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "    df[sem_col_name] = calc_bert_score(df[\"target\"], df[preds_col_name], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(model_name, metrics, use_validation=True, load_csv=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    - test_data: whether the data is test data or validation data (True for test data, False for validation data)\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    - replace_existing: whether to replace an existing column if it already exists\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_METRICS_PATH\n",
    "    else:\n",
    "        save_path = TEST_METRICS_PATH\n",
    "        \n",
    "    # Load the existing dataframe if it exists\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"J\"])\n",
    "\n",
    "    # If replace existing, remove the existing row with the same model name\n",
    "    if model_name in df[\"Model\"].values and replace_existing:\n",
    "        df = df[df[\"Model\"] != model_name]\n",
    "\n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"J\": [metrics[\"J\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Object Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    \"\"\"Adds a prefix to the source sequence in the dataset.\"\"\"\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def create_bidirectional_dataset(datasets, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    if shuffle:\n",
    "        extended_datasets[\"train\"] = extended_datasets[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "        \n",
    "    return extended_datasets\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    \"\"\"Preprocesses a dataset using a tokenizer.\"\"\"\n",
    "    def preprocess_function(examples, tokenizer):\n",
    "        \"\"\"Preprocess function for T5.\"\"\"\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"source\"],\n",
    "            text_target=examples[\"target\"],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "        batched=True,\n",
    "        remove_columns=[\"source\", \"target\"],\n",
    "    )\n",
    "\n",
    "def post_process(preds, refs, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5.\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        refs (list): list of reference sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "        decoded_refs (list): list of decoded reference sequences\n",
    "    \"\"\"\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    return decoded_preds, decoded_refs\n",
    "\n",
    "def post_process_preds(preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5 (only for predictions)\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "    \"\"\"\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "\n",
    "    return decoded_preds\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset to use for testing created using create_bidirectional_datasets\n",
    "                                  For example, raw_datasets_bd[\"validation\"] or raw_datasets_bd[\"test\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = [i for i, input_sentence in enumerate(bd_dataset['source']) if input_sentence.startswith(\"to_neutral\")]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                compute_metrics,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                report_to=\"wandb\",\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Evaluation dataset.\n",
    "        compute_metrics (function): Function to compute metrics. Change this to compute_metrics_bd if using a bi-directional model.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "        report_to (str): Where to report results to. Either \"wandb\" or \"none\".\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    gen_config_path = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(gen_config_path)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=gen_config_path,\n",
    "        fp16=True,\n",
    "        report_to=report_to,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "   \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def training_pipeline(model_name, project_name=\"t5-detox\", model_checkpoint=\"t5-small\", use_validation=True, raw_datasets=raw_datasets, bidirectional=False, shuffle=False, do_train=True):\n",
    "    \"\"\"\n",
    "    Pipeline for training a T5 model. Saves the best model checkpoint to a txt file. Can also be used for evaluating a model (use test set instead of validation set).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to name the output directory and wandb run.\n",
    "        project_name (str): Name of the wandb project.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        use_validation (bool): Whether to use the validation set or not.\n",
    "        raw_datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        bidirectional (bool): Whether to use a bi-directional model or not.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "        do_train (bool): Whether to train the model or not. Keep as false if you just want to evaluate the model.\n",
    "\n",
    "    Returns:\n",
    "        trainer (Seq2SeqTrainer): Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess dataset (add prefixes / make bidirectional)\n",
    "    if bidirectional and do_train:\n",
    "        raw_datasets = create_bidirectional_dataset(raw_datasets, shuffle=shuffle)\n",
    "    else:\n",
    "        raw_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = preprocess_dataset(raw_datasets, tokenizer_t5_small)\n",
    "\n",
    "    # Define compute_metrics function depending on bidirectional or not\n",
    "    if bidirectional and do_train and use_validation:\n",
    "        bd_dataset = raw_datasets[\"validation\"]\n",
    "    elif bidirectional and do_train and not use_validation:\n",
    "        bd_dataset = raw_datasets[\"test\"]\n",
    "    else:\n",
    "        bd_dataset = None\n",
    "\n",
    "    compute_metrics_fn = partial(compute_metrics_bd, bd_dataset=bd_dataset, shuffled_data=shuffle) if bd_dataset else compute_metrics\n",
    "\n",
    "    # Setup trainer\n",
    "    trainer = setup_trainer(\n",
    "        output_dir_name=model_name,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"] if use_validation else tokenized_datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    if do_train:\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=project_name, name=model_name)\n",
    "        trainer.train()\n",
    "        wandb.finish()\n",
    "\n",
    "        # Get the best checkpoint path for the model\n",
    "        checkpoint_path = trainer.state.best_model_checkpoint\n",
    "\n",
    "        # Save the checkpoint path for the best model\n",
    "        with open(BEST_MODEL_CHECKPOINT_PATH, \"a\") as file:\n",
    "            file.write(f\"{model_name}: {checkpoint_path}\\n\")\n",
    "\n",
    "    return trainer, tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Lexically Constrained Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_words_list(dataset, tokenizer=tokenizer_toxicity, model=model_toxicity, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Gets the top k bad words for each sentence in the dataset based on attention scores from the toxicity classifier.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of sentences.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use (toxicity classifier).\n",
    "        model (PreTrainedModel): Model to use (toxicity classifier).\n",
    "        num_layers (int): Number of top n layers in the model to use for attention-based bad word identification.\n",
    "        top_k (int): Number of bad words to return.\n",
    "\n",
    "    Returns:\n",
    "        bad_words_list (list): List of lists of bad words.\n",
    "    \"\"\"    \n",
    "    bad_words_list = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        # Tokenize sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        # Get attention scores\n",
    "        attention = model(input_ids, output_attentions=True)['attentions']\n",
    "\n",
    "        # Get the last num_layers layer attention scores and average them\n",
    "        attention = torch.stack(attention[-num_layers:]).mean(0)\n",
    "\n",
    "        # Average across each head\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Sum each row to get the attention score for each token\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Exclude separator tokens and punctuation\n",
    "        token_list = input_ids.squeeze().tolist()\n",
    "        punctuation_ids = {tokenizer.convert_tokens_to_ids(token) for token in string.punctuation}\n",
    "        exclude_ids = set([tokenizer.bos_token_id, tokenizer.eos_token_id]) | punctuation_ids\n",
    "\n",
    "        valid_indices = [i for i, token_id in enumerate(token_list) if token_id not in exclude_ids]\n",
    "\n",
    "        # Filter out the valid indices from the attention scores\n",
    "        valid_attention = attention.squeeze()[valid_indices]\n",
    "\n",
    "        # Get the indices of the top k tokens with the highest attention scores among valid tokens\n",
    "        top_indices = valid_attention.topk(top_k).indices.tolist()\n",
    "        top_token_indices = [valid_indices[i] for i in top_indices]\n",
    "\n",
    "        # Decode the tokens\n",
    "        bad_words = [tokenizer.decode(token_list[index]).strip() for index in top_token_indices]\n",
    "\n",
    "        bad_words_list.append(bad_words)\n",
    "\n",
    "    return bad_words_list\n",
    "\n",
    "def generate_pred_nlcd(raw_sentence, model, tokenizer, tokenizer_prefix_space, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Generate predictions for a single raw sentence using the NLCD (Negative Lexically Constrained Decoding) method.\n",
    "\n",
    "    Args:\n",
    "        raw_sentence (str): The raw sentence to generate predictions for.\n",
    "        model (T5ForConditionalGeneration): The T5 model for generation.\n",
    "        tokenizer (T5Tokenizer): The tokenizer for tokenizing the input.\n",
    "        tokenizer_prefix_space (T5Tokenizer): The tokenizer for tokenizing the input with prefix space.\n",
    "        num_layers (int): The number of layers to consider for generating bad words.\n",
    "        top_k (int): The number of top bad words to consider.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prediction for the input sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer([raw_sentence], return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    def get_tokens_as_list(word_list):\n",
    "        \"Converts a sequence of words into a list of tokens\"\n",
    "        tokens_list = []\n",
    "        for word in word_list:\n",
    "            tokenized_word = tokenizer_prefix_space([word], add_special_tokens=False).input_ids[0]\n",
    "            tokens_list.append(tokenized_word)\n",
    "        return tokens_list\n",
    "\n",
    "    # Get toxic words\n",
    "    bad_words = get_bad_words_list([raw_sentence], tokenizer_toxicity, model_toxicity, num_layers=num_layers, top_k=top_k)[0]\n",
    "    bad_words_ids = get_tokens_as_list(word_list=bad_words)\n",
    "\n",
    "    # Generate text while enforcing the bad words to not appear\n",
    "    encoded_preds = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        bad_words_ids=bad_words_ids,\n",
    "        max_length=MAX_OUTPUT_LENGTH,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    decoded_preds = tokenizer.batch_decode(encoded_preds, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds][0]\n",
    "\n",
    "    return decoded_preds\n",
    "\n",
    "def generate_preds_nlcd(raw_sentences,\n",
    "                        model_checkpoint,\n",
    "                        num_layers=3,\n",
    "                        top_k=3):\n",
    "    \"\"\"\n",
    "    Generate predictions for a list of raw sentences using the NLCD (Negative Lexically Constrained Decoding) method.\n",
    "\n",
    "    Args:\n",
    "        raw_sentences (list): A list of raw sentences to generate predictions for.\n",
    "        model_checkpoint (str): The model checkpoint to use for generation.        \n",
    "        num_layers (int): The number of layers to \n",
    "        top_k (int): The number of top bad words to consider.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated predictions for the input sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model and tokenizers based on the checkpoint\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, add_prefix_space=False)\n",
    "    tokenizer_prefix_space = T5Tokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "    # Create a list to store the predictions\n",
    "    preds = []\n",
    "\n",
    "    # Iterate through each raw sentence\n",
    "    for raw_sentence in tqdm(raw_sentences):\n",
    "        # Add prefix to the raw sentence\n",
    "        raw_sentence = f\"to_neutral: {raw_sentence}\"\n",
    "\n",
    "        # Generate predictions with NLCD\n",
    "        decoded_preds = generate_pred_nlcd(raw_sentence, model, tokenizer, tokenizer_prefix_space, num_layers, top_k)\n",
    "\n",
    "        # Append the predictions to the list of predictions\n",
    "        preds.append(decoded_preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t5_preds_metrics(model_checkpoint,\n",
    "                         raw_datasets=raw_datasets,\n",
    "                         bidirectional=False,\n",
    "                         shuffle=False,\n",
    "                         use_validation=True,\n",
    "                         do_train=False,\n",
    "                         tokenizer=tokenizer_t5_small\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Returns the predictions and metrics for a fine-tuned T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training pipeline\n",
    "    trainer, trainer_tokenized_ds = training_pipeline(\n",
    "        model_name=\"n/a\",\n",
    "        project_name=\"n/a\",\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        use_validation=use_validation,\n",
    "        raw_datasets=raw_datasets,\n",
    "        bidirectional=bidirectional,\n",
    "        shuffle=shuffle,\n",
    "        do_train=do_train\n",
    "    )\n",
    "\n",
    "    # Get raw predictions\n",
    "    if use_validation:\n",
    "        trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"validation\"])\n",
    "    else:\n",
    "        trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"test\"])\n",
    "\n",
    "    # Get encoded predictions and metrics\n",
    "    trainer_preds_encoded, trainer_metrics = trainer_preds_raw.predictions, trainer_preds_raw.metrics\n",
    "\n",
    "    # Post-process predictions\n",
    "    if isinstance(trainer_preds_encoded, tuple):\n",
    "        trainer_preds_encoded = trainer_preds_encoded[0]\n",
    "\n",
    "    trainer_preds_decoded = tokenizer.batch_decode(trainer_preds_encoded, skip_special_tokens=True)\n",
    "    trainer_preds_decoded = [pred.strip() for pred in trainer_preds_decoded]\n",
    "\n",
    "    # Return trainer metrics in the same format as evaluate_metrics\n",
    "    trainer_metrics = {\n",
    "        \"BLEU\": trainer_metrics[\"test_BLEU\"],\n",
    "        \"BLEURT\": trainer_metrics[\"test_BLEURT\"],\n",
    "        \"STA\": trainer_metrics[\"test_STA\"],\n",
    "        \"FLU\": trainer_metrics[\"test_FLU\"],\n",
    "        \"SEM\": trainer_metrics[\"test_SEM\"],\n",
    "        \"J\": trainer_metrics[\"test_J\"]\n",
    "    }\n",
    "        \n",
    "    # Return predictions and metrics\n",
    "    return trainer_preds_decoded, trainer_metrics\n",
    "\n",
    "def compare_outputs(\n",
    "    df,\n",
    "    cols_to_compare = [\"source\", \"target\", \"BART_preds\", \"T5-UD-DA_preds\", \"T5-UD-DA-MinLoss_preds\"],\n",
    "    bad_words_cols = ['source', 'T5-UD_preds'],\n",
    "    num_examples = 5,\n",
    "    num_layers = 5,\n",
    "    top_k = 3,\n",
    "    print_toxic_words = True,\n",
    "    random_state = RANDOM_SEED,\n",
    "):\n",
    "    \"\"\"\"\"\"\n",
    "    # Randomly sample the filtered dataset\n",
    "    df_sample = df.sample(n=num_examples, random_state=random_state)\n",
    "\n",
    "    # Get toxic words for in bad_words_col_1, bad_words_col_2, and bad_words_col_3\n",
    "    if print_toxic_words and bad_words_cols is not None:\n",
    "        ## Initialize a dictionary of bad words\n",
    "        bad_words = {}\n",
    "\n",
    "        ## Get bad words for each model\n",
    "        for bad_words_col in bad_words_cols:\n",
    "            bad_words[bad_words_col] = get_bad_words_list(df_sample[bad_words_col], num_layers=num_layers, top_k=top_k)\n",
    "\n",
    "    # Print every line for each col in cols_to_compare, and the bad words for each col in bad_words_cols\n",
    "    for i in range(num_examples):\n",
    "        if cols_to_compare is not None:\n",
    "            for col in cols_to_compare:\n",
    "                # Format the column name to remove _preds and capitalise first letter\n",
    "                if col in ['source', 'target']:\n",
    "                    col_name = col.capitalize()\n",
    "                elif col.endswith(\"_preds\"):\n",
    "                    col_name = col[:-len(\"_preds\")]\n",
    "                print(f\"{col_name}: {df_sample[col].iloc[i]}\")\n",
    "        if bad_words_cols is not None:\n",
    "            for bad_words_col in bad_words_cols:\n",
    "                # Format the column name to remove _preds and capitalise first letter\n",
    "                if bad_words_col in ['source', 'target']:\n",
    "                    bad_words_col_name = bad_words_col.capitalize()\n",
    "                elif bad_words_col.endswith(\"preds\"):\n",
    "                    bad_words_col_name = bad_words_col[:-len(\"_preds\")]\n",
    "                print(f\"'Toxic' words in {bad_words_col_name}: {bad_words[bad_words_col][i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Using Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], raw_datasets[\"validation\"][\"target\"])\n",
    "df_val_metrics = add_metrics_to_df(\"ref\", ref_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n",
      "/tmp/ipykernel_15355/1601193926.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, model_metrics_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "delete_val_preds = baseline_detoxifier(raw_datasets[\"validation\"][\"source\"])\n",
    "df_val_preds = add_preds_to_df(\"DELETE\", delete_val_preds, use_validation=True, load_csv=False)\n",
    "\n",
    "delete_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], delete_val_preds)\n",
    "df_val_metrics = add_metrics_to_df(\"DELETE\", delete_val_metrics, use_validation=True, load_csv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bart_val_preds = bart_detoxifier(raw_datasets[\"validation\"][\"source\"])\n",
    "df_val_preds = add_preds_to_df(\"BART\", bart_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "bart_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], bart_val_preds)\n",
    "df_val_metrics = add_metrics_to_df(\"BART\", bart_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_preds = add_metric_cols_to_preds(\"BART_preds\", use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0  DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1    BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f92a3a0a683402c8e23ff8b8bc56d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb75a0816ea04b19ae99df6c8023c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609ad31c1d9145199b1a3c52e1a41321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_val_preds, t5_ud_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe and per-row metrics\n",
    "df_val_preds = add_preds_to_df(\"T5-UD\", t5_ud_val_preds, use_validation=True, load_csv=True)\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD_preds\", use_validation=True)\n",
    "\n",
    "# Add aggregated metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD\", t5_ud_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this example, the largest differences between BART and T5-Small-UD are in relation to BLEURT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of BLEURT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Most people understand trumps comments for what they were the whining of a sore loser.\n",
      "Target: Most people understand trumps comments were result of his defeat.\n",
      "BART: Most people understand trumps comments for what they were.\n",
      "T5-UD: Most people understand trumps comments for what they were the bad of a loser.\n",
      "\n",
      "Source: But then the incredibly stupid can continue to ignore the obvious.\n",
      "Target: But then those who don't know anything can continue to not pay attention to the obvious.\n",
      "BART: But then they can continue to ignore the obvious.\n",
      "T5-UD: But then the incredibly bad can continue to ignore the obvious.\n",
      "\n",
      "Source: Rt ima agree with you on that his shit is so bland and shallow\n",
      "Target: I agree with you that his performance is bland and shallow\n",
      "BART: I agree with you that his performance is bland and shallow\n",
      "T5-UD: Rt ima agree with you on that his thing is so bland and shallow\n",
      "\n",
      "Source: No bitch i miss being 80 you on the other hand are still a \" teen\n",
      "Target: No girl i miss being 80 you on the other hand are still a \" teen\n",
      "BART: No girl I miss being 80 you on the other hand are still a \"teen\n",
      "T5-UD: I miss being 80 you on the other hand are still a \" teen\n",
      "\n",
      "Source: Try thinking before you type, it makes you appear less foolish.\n",
      "Target: Try thinking before you type, it makes you more smart.\n",
      "BART: Try thinking before you type, it makes you more smart.\n",
      "T5-UD: Try thinking before you type, it makes you appear less foolish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART has better BLEURT score than T5-UD\n",
    "df_BLEURT_BART_vs_T5_UD = df_val_preds[(df_val_preds[\"BART_BLEURT\"] >= df_val_preds[\"T5-UD_BLEURT\"] * 1.2) & (df_val_preds[\"BART_BLEURT\"] != df_val_preds[\"T5-UD_BLEURT\"])]\n",
    "\n",
    "# Compmare outputs\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_BART_vs_T5_UD,\n",
    "    cols_to_compare=['source', 'target', 'BART_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Who the fuck cuts out part of the state of the union?\n",
      "Target: Who cuts out part of the state of tge union.\n",
      "BART: Who cuts out part of the state of the union?\n",
      "T5-UD: Who cuts out part of the state of the union?\n",
      "'Toxic' words in ourc: ['fuck', 'the', 'part']\n",
      "'Toxic' words in T5-UD: ['state', 'cuts', 'part']\n",
      "\n",
      "Source: Nah yu seen how i act n public sober n shit\n",
      "Target: Nah youve seen how i act in public, sober and all.\n",
      "BART: Nah yu seen how i act in public sober\n",
      "T5-UD: Nah you have seen how i act n public sober\n",
      "'Toxic' words in ourc: ['shit', 'i', 'u']\n",
      "'Toxic' words in T5-UD: ['i', 'sober', 'n']\n",
      "\n",
      "Source: Shit chillen fresh out the shower\n",
      "Target: Chillen fresh out of the shower\n",
      "BART: Chillen fresh out of the shower\n",
      "T5-UD: Coolen fresh out the shower\n",
      "'Toxic' words in ourc: ['it', 'Sh', 'chill']\n",
      "'Toxic' words in T5-UD: ['Cool', 'en', 'shower']\n",
      "\n",
      "Source: These repubicans are like cannibalistic piraas.\n",
      "Target: These republicans are taking advantage.\n",
      "BART: These republicans are not good.\n",
      "T5-UD: These repubicans are like cannibalistic people.\n",
      "'Toxic' words in ourc: ['cannibal', 'are', 'icans']\n",
      "'Toxic' words in T5-UD: ['cannibal', 'are', 'icans']\n",
      "\n",
      "Source: I mean very irritating to see females fightin over some dick\n",
      "Target: Its irritating to see females fighting over boys\n",
      "BART: It is very irritating to see females fighting over boys\n",
      "T5-UD: I mean very irritating to see females fightin over something\n",
      "'Toxic' words in ourc: ['dick', 'females', 'some']\n",
      "'Toxic' words in T5-UD: ['females', 'fight', 'irritating']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART is better than T5-Small-UD in terms of STA\n",
    "df_STA_BART_vs_T5_UD = df_val_preds[(df_val_preds[\"BART_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_BART_vs_T5_UD,\n",
    "    cols_to_compare=['source', 'target', 'BART_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=['source', 'T5-UD_preds'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c8c4a83c765452e6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-564d1ea8c297b791.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-d7d403f057f73219.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_bd_val_preds, t5_bd_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_bidir_shuf\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-BD\", t5_bd_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add per-row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-BD_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-BD\", t5_bd_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0  DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1    BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2   T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055\n",
       "3   T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of BLEURT vs. T5-Small-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Lol fuck im ever doing that formsprings, perfect exampleas why not to do it\n",
      "Target: Lol im ever doing that formsprings, perfect exampleas why not to do it\n",
      "T5-UD: I'm ever doing that formsprings, perfect exampleas why not to do it\n",
      "T5-BD: Lol im ever doing that formsprings, perfect exampleas why not to do it\n",
      "\n",
      "Source: Hey, respect to op for sticking around and not giving a shit\n",
      "Target: Hey, respect to op for sticking around and not caring about it.\n",
      "T5-UD: Hey, respect to op for sticking around and not giving a thing\n",
      "T5-BD: Hey, respect to op for sticking around and not caring\n",
      "\n",
      "Source: Im having the hardest time accepting that shit man!!! irefuse to believe this shit\n",
      "Target: I'm having the hardest time accepting that man, I refuse to believe this.\n",
      "T5-UD: Im having the hardest time accepting that man!!! irefuse to believe this man\n",
      "T5-BD: I am having the hardest time accepting that man!!! irefuse to believe this\n",
      "\n",
      "Source: Icantdateyou if ur hairy af - _ - fucks noo! lol\n",
      "Target: I cant date you if ur hairy\n",
      "T5-UD: I cant date you if you are hairy noo! lol\n",
      "T5-BD: I cant date you if ur hairy noo! lol\n",
      "\n",
      "Source: | smh they really be stashing and don't tell us shit\n",
      "Target: | smh they really be stashing and don't tell us a word\n",
      "T5-UD: They really be stashing and don't tell us anything\n",
      "T5-BD: | smh they really be stashing and don't tell us anything\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BLEURT of T5-Small-BD exceeds T5-Small-UD by 20% and are not identical\n",
    "df_BLEURT_T5_BD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-BD_BLEURT\"] >= df_val_preds[\"T5-UD_BLEURT\"] * 1.2) & (df_val_preds[\"T5-BD_BLEURT\"] != df_val_preds[\"T5-UD_BLEURT\"])]\n",
    "\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_T5_BD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD_preds','T5-BD_preds'],\n",
    "    bad_words_cols=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of STA vs. T5-Small-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: You already burned down to a crisp in fort mac by your own stupidity, already.\n",
      "Target: You already lost in fort mac.\n",
      "T5-UD: You already burned down to a crisp in fort mac by your own ignorance, already.\n",
      "T5-BD: You already burned down to a crisp in fort mac by yourself, already.\n",
      "'Toxic' words in Source: ['stupidity', 'burned']\n",
      "'Toxic' words in T5-UD: ['You', 'burned']\n",
      "\n",
      "Source: True conservatives, that are not white racists, will have to find a new home.\n",
      "Target: Correct\n",
      "T5-UD: True conservatives, that are not white, will have to find a new home.\n",
      "T5-BD: True conservatives will have to find a new home.\n",
      "'Toxic' words in Source: ['white', 'racists']\n",
      "'Toxic' words in T5-UD: ['white', 'True']\n",
      "\n",
      "Source: So you would love to see the commys win over a duly elected democratic president, man you are sick\n",
      "Target: So you would love to see the communist win over a duly elected democratic president\n",
      "T5-UD: So you would love to see the commys win over a duly elected democratic president, man you are sick\n",
      "T5-BD: So you would love to see the commys win over a duly elected democratic president\n",
      "'Toxic' words in Source: ['sick', 'you']\n",
      "'Toxic' words in T5-UD: ['sick', 'you']\n",
      "\n",
      "Source: Shut the fuck up, geek!\n",
      "Target: Please, keep quiet!\n",
      "T5-UD: Shut up.\n",
      "T5-BD: Keep quiet, geek!\n",
      "'Toxic' words in Source: ['fuck', 'Shut']\n",
      "'Toxic' words in T5-UD: ['Shut', 'up']\n",
      "\n",
      "Source: We are done with the unamerican, disrespectful denver broncos!\n",
      "Target: We are done with them\n",
      "T5-UD: We are done with the unamerican, disrespectful denver broncos!\n",
      "T5-BD: We are done with the unamerican denver broncos!\n",
      "'Toxic' words in Source: ['disrespectful', 'the']\n",
      "'Toxic' words in T5-UD: ['disrespectful', 'the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART is better than T5-Small-UD in terms of STA\n",
    "df_STA_T5_BD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-BD_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_BD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD_preds','T5-BD_preds'],\n",
    "    bad_words_cols=['source','T5-UD_preds'],\n",
    "    num_examples=5,\n",
    "    num_layers=5,\n",
    "    top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional, Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c8c4a83c765452e6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-564d1ea8c297b791.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-d7d403f057f73219.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_val_preds, t5_ud_da_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_all\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA\", t5_ud_da_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-DA_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA\", t5_ud_da_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Filters (Minimum Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e53273a7b74dac91313a3e0502e75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8d3533741b498a8c3e30d441e9b90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf3220e78364676b7054b3a5ca3d44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_minloss_val_preds, t5_ud_da_minloss_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=\"../models/t5_small_aug_all/checkpoint-972\",\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-MinLoss\", t5_ud_da_minloss_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-DA-MinLoss_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "# df_val_metrics = add_metrics_to_df(\"T5-Small-Aug-All-MinLoss\", t5_ud_da_minloss_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Outputs of All Filter vs. All Filter (Minimum Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Bitch was so mad lmfao!! smh\n",
      "Target: She was so mad!\n",
      "T5-UD-DA-MinLoss: Bitch was so mad\n",
      "T5-UD-DA: He was so mad.\n",
      "'Toxic' words in Source: ['itch', 'B', 'was']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['itch', 'was', 'B']\n",
      "\n",
      "Source: Pitiful choice scalia was a hypocrite!\n",
      "Target: Scalia was a bad person\n",
      "T5-UD-DA-MinLoss: Choice scalia was a hypocrite!\n",
      "T5-UD-DA: Choice scalia was not a good person!\n",
      "'Toxic' words in Source: ['sc', 'was', 'hypocr']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['was', 'sc', 'hypocr']\n",
      "\n",
      "Source: He's a fucking street thug.\n",
      "Target: He's a street gang member.\n",
      "T5-UD-DA-MinLoss: He's a street thug\n",
      "T5-UD-DA: He's a street guy.\n",
      "'Toxic' words in Source: ['fucking', 'He', \"'s\"]\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['He', 'thug', \"'s\"]\n",
      "\n",
      "Source: Doosh doosh doosh doosh doosh fucking cock sucker queir bate ass wipe dont tell me what the fuck to write\n",
      "Target: Don't tell me what to write.\n",
      "T5-UD-DA-MinLoss: Doosh doosh doosh doosh cock sucker queir bate wipe dont tell me what to write\n",
      "T5-UD-DA: Don't tell me what to write\n",
      "'Toxic' words in Source: ['fucking', 'dont', 'fuck']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['dont', 'sucker', 'cock']\n",
      "\n",
      "Source: True conservatives, that are not white racists, will have to find a new home.\n",
      "Target: Correct\n",
      "T5-UD-DA-MinLoss: True conservatives, that are not white, will have to find a new home.\n",
      "T5-UD-DA: True conservatives, that are not racists, will have to find a new home.\n",
      "'Toxic' words in Source: ['white', 'racists', 'True']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['white', 'True', 'are']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to rows where T5-UD-DA-MinLoss has STA == 0 and T5-UD-DA has STA == 1\n",
    "df_STA_T5_UD_DA_MinLoss_vs_T5_UD_DA = df_val_preds[(df_val_preds[\"T5-UD-DA-MinLoss_STA\"] == 0) & (df_val_preds[\"T5-UD-DA_STA\"] == 1)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_UD_DA_MinLoss_vs_T5_UD_DA,\n",
    "    cols_to_compare=['source','target','T5-UD-DA-MinLoss_preds', 'T5-UD-DA_preds'],\n",
    "    bad_words_cols=['source','T5-UD-DA-MinLoss_preds'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Toxicity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_notox_val_preds, t5_ud_da_notox_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_notox\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoTOX\", t5_ud_da_notox_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoTOX\", t5_ud_da_notox_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Semantic Similarity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_nosem_val_preds, t5_ud_da_nosem_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_nosim\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoSEM\", t5_ud_da_nosem_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoSEM\", t5_ud_da_nosem_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Fluency Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_noflu_val_preds, t5_ud_da_noflu_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_noaccept\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoFLU\", t5_ud_da_noflu_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoFLU\", t5_ud_da_noflu_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-UD-DA</td>\n",
       "      <td>0.204206</td>\n",
       "      <td>0.593218</td>\n",
       "      <td>0.916178</td>\n",
       "      <td>0.714662</td>\n",
       "      <td>0.925547</td>\n",
       "      <td>0.813157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5-UD-DA-NoTOX</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.598625</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.713698</td>\n",
       "      <td>0.925555</td>\n",
       "      <td>0.804659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T5-UD-DA-NoSEM</td>\n",
       "      <td>0.211811</td>\n",
       "      <td>0.591599</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.719671</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.814390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T5-UD-DA-NoFLU</td>\n",
       "      <td>0.192281</td>\n",
       "      <td>0.595025</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.709992</td>\n",
       "      <td>0.925311</td>\n",
       "      <td>0.807508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0          DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1            BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2           T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230\n",
       "3        T5-UD-DA  0.204206  0.593218  0.916178  0.714662  0.925547  0.813157\n",
       "4  T5-UD-DA-NoTOX  0.208252  0.598625  0.892707  0.713698  0.925555  0.804659\n",
       "5  T5-UD-DA-NoSEM  0.211811  0.591599  0.917854  0.719671  0.924972  0.814390\n",
       "6  T5-UD-DA-NoFLU  0.192281  0.595025  0.903604  0.709992  0.925311  0.807508\n",
       "7           T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5-Small (UD, NLCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that the functions works by filtering to cases where T5-UD_STA == 0. Theoretically NLCD should lead to the number of sentences with toxic words reducing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 116/116 [00:38<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of sentences with toxic words in preds_no_nlcd: 105\n",
      "Success rate of no_nlcd: 0.09482758620689655\n",
      "Count of sentences with toxic words in preds_nlcd: 32\n",
      "Success rate of nlcd: 0.7241379310344828\n",
      "\n",
      "Source: to_neutral: You straighten out that chicago problem and then well tell our cops to stop killing unarmed blacks.\n",
      "Target: There are problems in Chicago that need to be dealt with as well as police brutality.\n",
      "Bad words: ['blacks', 'killing']\n",
      "Prediction without NLCD: You straighten out that chicago problem and then well tell our cops to stop killing unarmed blacks\n",
      "Prediction with NLCD: You straighten out that chicago problem and then well tell our cops to stop shooting unarmed people.\n",
      "\n",
      "Source: to_neutral: You have cold dead hands.\n",
      "Target: You have cold hands.\n",
      "Bad words: ['You', 'have']\n",
      "Prediction without NLCD: You have cold dead hands\n",
      "Prediction with NLCD: Your hands are not good.\n",
      "\n",
      "Source: to_neutral: This is ridiculous and looks terrible if true.\n",
      "Target: It's ridiculously terrible if true.\n",
      "Bad words: ['This', 'ridiculous']\n",
      "Prediction without NLCD: This is ridiculous and looks terrible if true.\n",
      "Prediction with NLCD: It looks terrible if true.\n",
      "\n",
      "Source: to_neutral: That is the most ridiculous, insane, yet funny comment ever by kruger.\n",
      "Target: That is the most ridiculous, insane, yet funny comment ever by kruger.\n",
      "Bad words: ['ridiculous', 'comment']\n",
      "Prediction without NLCD: That is the most ridiculous, insane, yet funny comment ever by Kruger\n",
      "Prediction with NLCD: That is the most absurd, insane, yet funny comments ever by kruger.\n",
      "\n",
      "Source: to_neutral: Gads, i hate your cruelty.\n",
      "Target: I don't like your cruelty.\n",
      "Bad words: ['hate', 'your']\n",
      "Prediction without NLCD: I hate your cruelty.\n",
      "Prediction with NLCD: I don't like you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def nlcd_test():\n",
    "    # Prefix the dataset\n",
    "    prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Get the raw sentences\n",
    "    ## Get indices of rows where T5-UD has STA == 0\n",
    "    t5_ud_sta_0_indices = df_val_preds[df_val_preds[\"T5-UD_STA\"] == 0].index.tolist()\n",
    "\n",
    "    ## Get predictions from T5-UD where STA == 0\n",
    "    preds_no_nlcd = df_val_preds[df_val_preds[\"T5-UD_STA\"] == 0][\"T5-UD_preds\"].tolist()\n",
    "\n",
    "    ## Filter the raw sentences to just the ones where T5-UD has STA == 0\n",
    "    input_sentences = [prefixed_datasets[\"validation\"][\"source\"][i] for i in t5_ud_sta_0_indices]\n",
    "\n",
    "    ## Get the references\n",
    "    refs = [prefixed_datasets[\"validation\"][\"target\"][i] for i in t5_ud_sta_0_indices]\n",
    "\n",
    "    # Get toxic words for each sentence\n",
    "    toxic_words_per_sentence = get_bad_words_list(input_sentences, tokenizer_toxicity, model_toxicity, num_layers=3, top_k=2)\n",
    "\n",
    "    # Get predictions using NLCD\n",
    "    preds_nlcd = generate_preds_nlcd(input_sentences, model_checkpoints[\"t5_small_unidir\"], num_layers=3, top_k=2)\n",
    "\n",
    "    # Count the number of cases where preds_no_nlcd contains a toxic word and calculate success rate\n",
    "    num_toxic_words_no_nlcd = 0\n",
    "    for i in range(len(preds_no_nlcd)):\n",
    "        if any(toxic_word in preds_no_nlcd[i] for toxic_word in toxic_words_per_sentence[i]):\n",
    "            num_toxic_words_no_nlcd += 1\n",
    "    success_rate_no_nlcd = (len(preds_no_nlcd) - num_toxic_words_no_nlcd) / len(preds_no_nlcd)\n",
    "    print(f\"Count of sentences with toxic words in preds_no_nlcd: {num_toxic_words_no_nlcd}\")\n",
    "    print(f\"Success rate of no_nlcd: {success_rate_no_nlcd}\")\n",
    "\n",
    "    # Count the number of cases where preds_nlcd contains a toxic word and calculate success rate\n",
    "    num_toxic_words_nlcd = 0\n",
    "    for i in range(len(preds_nlcd)):\n",
    "        if any(toxic_word in preds_nlcd[i] for toxic_word in toxic_words_per_sentence[i]):\n",
    "            num_toxic_words_nlcd += 1\n",
    "    success_rate_nlcd = (len(preds_nlcd) - num_toxic_words_nlcd) / len(preds_nlcd)\n",
    "    print(f\"Count of sentences with toxic words in preds_nlcd: {num_toxic_words_nlcd}\")\n",
    "    print(f\"Success rate of nlcd: {success_rate_nlcd}\")\n",
    "\n",
    "    # Print out the first 5 predictions\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print(f\"Source: {input_sentences[i]}\")\n",
    "        print(f\"Target: {refs[i]}\")\n",
    "        print(f\"Bad words: {toxic_words_per_sentence[i]}\")\n",
    "        print(f\"Prediction without NLCD: {preds_no_nlcd[i]}\")\n",
    "        print(f\"Prediction with NLCD: {preds_nlcd[i]}\")\n",
    "\n",
    "nlcd_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that bad words as identified by attention scores in the toxicity classifier have been removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get predictions based on num_layers = 3 and top_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 1193/1193 [06:11<00:00,  3.21it/s]\n"
     ]
    }
   ],
   "source": [
    "# Get predictions based on default hyperparameters\n",
    "t5_ud_nlcd_val_preds = generate_preds_nlcd(raw_sentences=raw_datasets[\"validation\"][\"source\"],\n",
    "                                             model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "                                             num_layers=3,\n",
    "                                             top_k=3)\n",
    "\n",
    "t5_ud_nlcd_val_metrics = evaluate_metrics(refs=raw_datasets[\"validation\"][\"target\"],\n",
    "                                          preds=t5_ud_nlcd_val_preds,\n",
    "                                          include_bleurt=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-NLCD\", t5_ud_nlcd_val_preds, use_validation=True, load_csv=True, replace_existing=True)\n",
    "\n",
    "# Calculate per-row metrics\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-NLCD_preds\", use_validation=True, replace_existing=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-NLCD\", t5_ud_nlcd_val_metrics, use_validation=True, load_csv=True, replace_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-UD-DA</td>\n",
       "      <td>0.204206</td>\n",
       "      <td>0.593218</td>\n",
       "      <td>0.916178</td>\n",
       "      <td>0.714662</td>\n",
       "      <td>0.925547</td>\n",
       "      <td>0.813157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5-UD-DA-NoTOX</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.598625</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.713698</td>\n",
       "      <td>0.925555</td>\n",
       "      <td>0.804659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T5-UD-DA-NoSEM</td>\n",
       "      <td>0.211811</td>\n",
       "      <td>0.591599</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.719671</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.814390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T5-UD-DA-NoFLU</td>\n",
       "      <td>0.192281</td>\n",
       "      <td>0.595025</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.709992</td>\n",
       "      <td>0.925311</td>\n",
       "      <td>0.807508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ref</td>\n",
       "      <td>0.989226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953898</td>\n",
       "      <td>0.716085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.924776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>T5-UD-NLCD</td>\n",
       "      <td>-0.046128</td>\n",
       "      <td>0.440937</td>\n",
       "      <td>0.960604</td>\n",
       "      <td>0.711287</td>\n",
       "      <td>0.892896</td>\n",
       "      <td>0.793266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0          DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1            BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2           T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230\n",
       "3        T5-UD-DA  0.204206  0.593218  0.916178  0.714662  0.925547  0.813157\n",
       "4  T5-UD-DA-NoTOX  0.208252  0.598625  0.892707  0.713698  0.925555  0.804659\n",
       "5  T5-UD-DA-NoSEM  0.211811  0.591599  0.917854  0.719671  0.924972  0.814390\n",
       "6  T5-UD-DA-NoFLU  0.192281  0.595025  0.903604  0.709992  0.925311  0.807508\n",
       "7           T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055\n",
       "8             ref  0.989226  1.000000  0.953898  0.716085  1.000000  0.924776\n",
       "9      T5-UD-NLCD -0.046128  0.440937  0.960604  0.711287  0.892896  0.793266"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just using layers = 3 and top_k = 3 shows that STA increases significantly (in fact higher than ref) but at the cost of BLEURT, BLEU and SEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: We get these ignorant losers pushed to the forefront because america has a majority of ignorant nonreaders, eric norstog\n",
      "Target: We get these guys pushed to the forefront because America has a majority of non readers who are not knowledgeable. Eric Norstog\n",
      "T5-UD-NLCD: We get these people pushed to the forefront because america has a majority of nonreaders, eric norstog\n",
      "T5-UD: We get these people pushed to the forefront because america has a majority of ignorant nonreaders, eric norstog\n",
      "'Toxic' words in Source: ['losers', 'ignorant', 'americ']\n",
      "'Toxic' words in T5-UD: ['ignorant', 'americ', 'of']\n",
      "\n",
      "Source: You have cold dead hands.\n",
      "Target: You have cold hands.\n",
      "T5-UD-NLCD: Your hands are not good.\n",
      "T5-UD: You have cold dead hands\n",
      "'Toxic' words in Source: ['You', 'dead', 'hands']\n",
      "'Toxic' words in T5-UD: ['You', 'hands', 'dead']\n",
      "\n",
      "Source: They should all be whipped and quartered!\n",
      "Target: They should all be punished\n",
      "T5-UD-NLCD: All of them should have been punished.\n",
      "T5-UD: They should be punished\n",
      "'Toxic' words in Source: ['They', 'whipped', 'be']\n",
      "'Toxic' words in T5-UD: ['punished', 'They', 'should']\n",
      "\n",
      "Source: This pope is a fucking drongo.\n",
      "Target: This pope is unsuitable.\n",
      "T5-UD-NLCD: He is a drongo.\n",
      "T5-UD: This pope is a drongo\n",
      "'Toxic' words in Source: ['fucking', 'pope', 'is']\n",
      "'Toxic' words in T5-UD: ['dr', 'is', 'pope']\n",
      "\n",
      "Source: You had me right up until you called the president a goon.\n",
      "Target: You had me right up until you called the president some worse things\n",
      "T5-UD-NLCD: You had my right up until you called the president a republican.\n",
      "T5-UD: You had me right up until you called the president a goon.\n",
      "'Toxic' words in Source: ['on', 'go', 'a']\n",
      "'Toxic' words in T5-UD: ['on', 'go', 'a']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to rows where T5-UD-NLCD has STA == 1 and T5-UD has STA == 0\n",
    "df_STA_T5_UD_NLCD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-UD-NLCD_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_UD_NLCD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD-NLCD_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=['source', 'T5-UD_preds'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, the issue with nlcd using top k is that it picks up on words that aren't actually toxic. We may need to do some hyperparameter optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attention Layers: 3, Top K: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [01:02<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.5928528587928075, 'STA': 0.94, 'FLU': 0.6659169, 'SEM': 0.9224684408307076, 'J': 0.8122476438663534, 'BLEURT': 0.11041066324338317}\n",
      "Number of Attention Layers: 3, Top K: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2477/3724826859.py:67: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  metrics_df = pd.concat([metrics_df, pd.DataFrame(row_dict)], ignore_index=True)\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [01:00<00:00,  3.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.5213675280319027, 'STA': 0.97, 'FLU': 0.65486234, 'SEM': 0.9087645810842514, 'J': 0.8049988906762277, 'BLEURT': -0.007674927059561014}\n",
      "Number of Attention Layers: 3, Top K: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [00:58<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.4523498106817375, 'STA': 0.975, 'FLU': 0.6672938, 'SEM': 0.8956054365634918, 'J': 0.7930498068495586, 'BLEURT': -0.0889124252460897}\n",
      "Number of Attention Layers: 3, Top K: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [00:59<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.3684404582440046, 'STA': 0.975, 'FLU': 0.67182654, 'SEM': 0.8818790996074677, 'J': 0.774429219855054, 'BLEURT': -0.20356815587729216}\n",
      "Number of Attention Layers: 3, Top K: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [01:02<00:00,  3.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.30464715593226105, 'STA': 0.98, 'FLU': 0.72159886, 'SEM': 0.860208820104599, 'J': 0.7692909679277089, 'BLEURT': -0.2930098811723292}\n",
      "Number of Attention Layers: 1, Top K: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [00:59<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.5635054717288045, 'STA': 0.94, 'FLU': 0.66143477, 'SEM': 0.9156330397725105, 'J': 0.8041146562263495, 'BLEURT': 0.07538189258426428}\n",
      "Number of Attention Layers: 2, Top K: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 200/200 [00:59<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'BLEU': 0.5865926624629054, 'STA': 0.93, 'FLU': 0.6690722, 'SEM': 0.9196343222260475, 'J': 0.8070598390955361, 'BLEURT': 0.07289028305560351}\n",
      "Number of Attention Layers: 3, Top K: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 10%|         | 19/200 [00:05<00:43,  4.14it/s]"
     ]
    }
   ],
   "source": [
    "def nlcd_hyperparameter_optimization(raw_datasets=raw_datasets,\n",
    "                                     model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "                                     bw_top_k_list=[1, 2, 3, 4, 5],\n",
    "                                     num_bw_layers_list=[1, 2, 3, 4, 5],\n",
    "                                     default_num_layers=3,\n",
    "                                     include_bleurt=True,\n",
    "                                     num_validation_rows=200):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization for NLCD by trying different values for num_bw_layers and bw_top_k.\n",
    "\n",
    "    Args:\n",
    "    - raw_datasets: The raw datasets dictionary object containing the source and target sentences.\n",
    "    - model_checkpoint: The checkpoint of the T5 model to use.\n",
    "    - bw_top_k_list: A list of values to try for bw_top_k.\n",
    "    - num_bw_layers_list: A list of values to try for num_bw_layers.\n",
    "    - default_num_layers: The default number of layers to use for bw_top_k.\n",
    "    - include_bleurt: Whether to include BLEURT in the evaluation metrics.\n",
    "\n",
    "    - num_validation_rows: Number of rows to use from raw_datasets[\"validation\"].\n",
    "\n",
    "    Returns:\n",
    "    - metrics_df: A DataFrame containing the evaluation metrics for different hyperparameter combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Filter the validation dataset to the first num_validation_rows rows\n",
    "    if num_validation_rows is not None:\n",
    "        validation_source = raw_datasets[\"validation\"][\"source\"][:num_validation_rows]\n",
    "        validation_target = raw_datasets[\"validation\"][\"target\"][:num_validation_rows]\n",
    "    else:\n",
    "        validation_source = raw_datasets[\"validation\"][\"source\"]\n",
    "        validation_target = raw_datasets[\"validation\"][\"target\"]\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    metrics_df = pd.DataFrame(columns=[\"num_layers\", \"top_k\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"J\"])\n",
    "    if include_bleurt:\n",
    "        metrics_df[\"BLEURT\"] = None  # Initialize the BLEURT column if include_bleurt is True\n",
    "\n",
    "    best_J_score = -1\n",
    "    best_num_layers = None\n",
    "    best_top_k = None\n",
    "\n",
    "    # Iterate through bw_top_k while keeping default_num_layers constant\n",
    "    for bw_top_k in bw_top_k_list:\n",
    "        print(f\"Number of Attention Layers: {default_num_layers}, Top K: {bw_top_k}\")\n",
    "        t5_ud_nlcd_preds = generate_preds_nlcd(raw_sentences=validation_source,\n",
    "                                                model_checkpoint=model_checkpoint,\n",
    "                                                num_layers=default_num_layers,\n",
    "                                                top_k=bw_top_k)\n",
    "        t5_ud_nlcd_metrics = evaluate_metrics(validation_target, t5_ud_nlcd_preds, include_bleurt=include_bleurt)\n",
    "        print(t5_ud_nlcd_metrics)\n",
    "        \n",
    "        J_score = t5_ud_nlcd_metrics[\"J\"]\n",
    "\n",
    "        row_dict = {\n",
    "            \"num_layers\": [default_num_layers],\n",
    "            \"top_k\": [bw_top_k],\n",
    "            \"BLEU\": [t5_ud_nlcd_metrics[\"BLEU\"]],\n",
    "            \"STA\": [t5_ud_nlcd_metrics[\"STA\"]],\n",
    "            \"FLU\": [t5_ud_nlcd_metrics[\"FLU\"]],\n",
    "            \"SEM\": [t5_ud_nlcd_metrics[\"SEM\"]],\n",
    "            \"J\": [J_score]\n",
    "        }\n",
    "\n",
    "        if include_bleurt:\n",
    "            row_dict[\"BLEURT\"] = [t5_ud_nlcd_metrics[\"BLEURT\"]]  # Add BLEURT score if include_bleurt is True\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame(row_dict)], ignore_index=True)\n",
    "\n",
    "        if J_score > best_J_score:\n",
    "            best_J_score = J_score\n",
    "            best_top_k = bw_top_k\n",
    "\n",
    "    # Try different values for num_bw_layers while keeping best_top_k constant (the best top_k from the previous step)\n",
    "    for num_bw_layers in num_bw_layers_list:\n",
    "        print(f\"Number of Attention Layers: {num_bw_layers}, Top K: {best_top_k}\")\n",
    "        t5_ud_nlcd_preds = generate_preds_nlcd(raw_sentences=validation_source,\n",
    "                                                model_checkpoint=model_checkpoint,\n",
    "                                                num_layers=num_bw_layers,\n",
    "                                                top_k=best_top_k)\n",
    "        t5_ud_nlcd_metrics = evaluate_metrics(validation_target, t5_ud_nlcd_preds, include_bleurt=include_bleurt)\n",
    "        print(t5_ud_nlcd_metrics)\n",
    "        \n",
    "        J_score = t5_ud_nlcd_metrics[\"J\"]\n",
    "\n",
    "        row_dict = {\n",
    "            \"num_layers\": [num_bw_layers],\n",
    "            \"top_k\": [best_top_k],\n",
    "            \"BLEU\": [t5_ud_nlcd_metrics[\"BLEU\"]],\n",
    "            \"STA\": [t5_ud_nlcd_metrics[\"STA\"]],\n",
    "            \"FLU\": [t5_ud_nlcd_metrics[\"FLU\"]],\n",
    "            \"SEM\": [t5_ud_nlcd_metrics[\"SEM\"]],\n",
    "            \"J\": [J_score]\n",
    "        }\n",
    "\n",
    "        if include_bleurt:\n",
    "            row_dict[\"BLEURT\"] = [t5_ud_nlcd_metrics[\"BLEURT\"]]  # Add BLEURT score if include_bleurt is True\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame(row_dict)], ignore_index=True)\n",
    "\n",
    "        if J_score > best_J_score:\n",
    "            best_J_score = J_score\n",
    "            best_num_layers = num_bw_layers\n",
    "\n",
    "    return metrics_df, best_num_layers, best_top_k\n",
    "\n",
    "metrics_df, best_num_layers, best_top_k = nlcd_hyperparameter_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try using threshold instead of top k for selecting words to exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_words_list(dataset, tokenizer=tokenizer_toxicity, model=model_toxicity, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Gets the top k bad words for each sentence in the dataset based on attention scores from the toxicity classifier.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of sentences.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use (toxicity classifier).\n",
    "        model (PreTrainedModel): Model to use (toxicity classifier).\n",
    "        num_layers (int): Number of top n layers in the model to use for attention-based bad word identification.\n",
    "        top_k (int): Number of bad words to return.\n",
    "\n",
    "    Returns:\n",
    "        bad_words_list (list): List of lists of bad words.\n",
    "    \"\"\"    \n",
    "    bad_words_list = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        # Tokenize sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        # Get attention scores\n",
    "        attention = model(input_ids, output_attentions=True)['attentions']\n",
    "\n",
    "        # Get the last num_layers layer attention scores and average them\n",
    "        attention = torch.stack(attention[-num_layers:]).mean(0)\n",
    "\n",
    "        # Average across each head\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Sum each row to get the attention score for each token\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Exclude separator tokens and punctuation\n",
    "        token_list = input_ids.squeeze().tolist()\n",
    "        punctuation_ids = {tokenizer.convert_tokens_to_ids(token) for token in string.punctuation}\n",
    "        exclude_ids = set([tokenizer.bos_token_id, tokenizer.eos_token_id]) | punctuation_ids\n",
    "\n",
    "        valid_indices = [i for i, token_id in enumerate(token_list) if token_id not in exclude_ids]\n",
    "\n",
    "        # Filter out the valid indices from the attention scores\n",
    "        valid_attention = attention.squeeze()[valid_indices]\n",
    "\n",
    "        # Get the indices of the top k tokens with the highest attention scores among valid tokens\n",
    "        top_indices = valid_attention.topk(top_k).indices.tolist()\n",
    "        top_token_indices = [valid_indices[i] for i in top_indices]\n",
    "\n",
    "        # Decode the tokens\n",
    "        bad_words = [tokenizer.decode(token_list[index]).strip() for index in top_token_indices]\n",
    "\n",
    "        bad_words_list.append(bad_words)\n",
    "\n",
    "    return bad_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_words_list_2(dataset, tokenizer=tokenizer_toxicity, model=model_toxicity, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Gets the top k bad words for each sentence in the dataset based on attention scores from the toxicity classifier.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of sentences.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use (toxicity classifier).\n",
    "        model (PreTrainedModel): Model to use (toxicity classifier).\n",
    "        num_layers (int): Number of top n layers in the model to use for attention-based bad word identification.\n",
    "        top_k (int): Number of bad words to return.\n",
    "\n",
    "    Returns:\n",
    "        bad_words_list (list): List of lists of bad words.\n",
    "    \"\"\"    \n",
    "    # Tokenize the entire dataset\n",
    "    inputs = tokenizer(dataset, return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "    # Get attention scores\n",
    "    attention = model(input_ids, output_attentions=True)['attentions']\n",
    "\n",
    "    # Get the last num_layers layer attention scores and average them\n",
    "    attention = torch.stack(attention[-num_layers:]).mean(0)\n",
    "\n",
    "    # Average across each head\n",
    "    attention = attention.mean(1)\n",
    "\n",
    "    # Sum each row to get the attention score for each token\n",
    "    attention = attention.mean(1)\n",
    "\n",
    "    # Exclude separator tokens and punctuation\n",
    "    punctuation_ids = {tokenizer.convert_tokens_to_ids(token) for token in string.punctuation}\n",
    "    exclude_ids = set([tokenizer.bos_token_id, tokenizer.eos_token_id]) | punctuation_ids\n",
    "\n",
    "    valid_indices = []\n",
    "    for i, token_list in enumerate(input_ids):\n",
    "        token_list = token_list.tolist()\n",
    "        valid_indices.append([j for j, token_id in enumerate(token_list) if token_id not in exclude_ids])\n",
    "\n",
    "    # Filter out the valid indices from the attention scores\n",
    "    valid_attention = attention.squeeze()[valid_indices]\n",
    "\n",
    "    # Get the indices of the top k tokens with the highest attention scores among valid tokens\n",
    "    top_indices = valid_attention.topk(top_k, dim=1).indices.tolist()\n",
    "\n",
    "    bad_words_list = []\n",
    "    for i, token_list in enumerate(input_ids):\n",
    "        token_list = token_list.tolist()\n",
    "        top_token_indices = [valid_indices[i][j] for j in top_indices[i]]\n",
    "\n",
    "        # Decode the tokens\n",
    "        bad_words = [tokenizer.decode(token_list[index]).strip() for index in top_token_indices]\n",
    "\n",
    "        bad_words_list.append(bad_words)\n",
    "\n",
    "    return bad_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_list_1 = get_bad_words_list(raw_datasets[\"validation\"][\"source\"][:10], num_layers=3, top_k=3)\n",
    "bad_words_list_2 = get_bad_words_list_2(raw_datasets[\"validation\"][\"source\"][:10], num_layers=3, top_k=3)\n",
    "\n",
    "assert bad_words_list_1 == bad_words_list_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
