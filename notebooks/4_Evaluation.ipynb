{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-23 12:07:50.901659: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-23 12:07:50.901718: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-23 12:07:50.901763: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "from numba import cuda\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "from typing import Dict, Union, Optional, Tuple, List, Any\n",
    "import pandas as pd\n",
    "import string\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Default parameters for T5 model fine-tuning\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 64\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "# Include BLEURT score in evaluation\n",
    "INCLUDE_BLEURT = True\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "AUG_DATASET_ALL_FILTERS_PATH = \"../data/processed/aug_datasets_all_filters\"\n",
    "AUG_DATASET_NO_TOXICITY_FILTER_PATH = \"../data/processed/aug_datasets_no_toxicity_filter\"\n",
    "AUG_DATASET_NO_SIMILARITY_FILTER_PATH = \"../data/processed/aug_datasets_no_similarity_filter\"\n",
    "AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH = \"../data/processed/aug_datasets_no_acceptability_filter\"\n",
    "\n",
    "# Set path for txt file containing best model checkpoints\n",
    "BEST_MODEL_CHECKPOINT_PATH = \"../models/best_model_checkpoints.txt\"\n",
    "\n",
    "# Set path to save evaluation outputs to\n",
    "VAL_PREDS_PATH = \"../data/interim/val_preds.csv\"\n",
    "VAL_METRICS_PATH = \"../data/interim/val_metrics.csv\"\n",
    "TEST_PREDS_PATH = \"../data/final/test_preds.csv\"\n",
    "TEST_METRICS_PATH = \"../data/final/test_metrics.csv\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and models\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(DEVICE)\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "aug_datasets_all_filters = DatasetDict.load_from_disk(AUG_DATASET_ALL_FILTERS_PATH)\n",
    "aug_datasets_no_acceptability_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH)\n",
    "aug_datasets_no_similarity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_SIMILARITY_FILTER_PATH)\n",
    "aug_datasets_no_toxicity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_TOXICITY_FILTER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load df_val_preds and df_val_metrics\n",
    "df_val_preds = pd.read_csv(VAL_PREDS_PATH)\n",
    "df_val_metrics = pd.read_csv(VAL_METRICS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the time it takes to run a function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "    return gpu.memoryUsed\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model functions\n",
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    y_pred_delete = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        y_pred_delete.append(text)\n",
    "\n",
    "    return y_pred_delete\n",
    "\n",
    "def bart_detoxifier(text_list):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text using BART\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "    y_pred_bart = pipe_bart(text_list, max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    \n",
    "    return y_pred_bart\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(df, model_name, metrics, save_path=\"../data/processed/model_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a df if the input df is empty\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"Overall\"])\n",
    "\n",
    "    # Check if the model name already exists in the dataframe\n",
    "    if model_name in df[\"Model\"].values:\n",
    "        print(f\"Model {model_name} already exists in the dataframe.\")\n",
    "        return df\n",
    "    \n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"Overall\": [metrics[\"Overall\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=INCLUDE_BLEURT\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA\": sta_pred,\n",
    "        \"FLU\": acc_pred,\n",
    "        \"SEM\": bert_score_f1,\n",
    "        \"J\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preds_to_df(model_name, preds, raw_datasets=raw_datasets, use_validation=True, load_csv=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add model predictions to a pandas dataframe\n",
    "\n",
    "    Args:\n",
    "    - model_name: name of the model\n",
    "    - preds: list of predictions\n",
    "    - test_data: whether the data is test data or validation data (True for test data, False for validation data)\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    - replace_existing: whether to replace an existing column if it already exists\n",
    "\n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_PREDS_PATH\n",
    "        source = raw_datasets[\"validation\"][\"source\"]\n",
    "        target = raw_datasets[\"validation\"][\"target\"]\n",
    "    else:\n",
    "        save_path = TEST_PREDS_PATH\n",
    "        source = raw_datasets[\"test\"][\"source\"]\n",
    "        target = raw_datasets[\"test\"][\"target\"]\n",
    "\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "                \"source\": source,\n",
    "                \"target\": target,\n",
    "            })\n",
    "    \n",
    "    # If replace existing, remove the existing column with the same model name\n",
    "    if f\"{model_name}_preds\" in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[f\"{model_name}_preds\"])\n",
    "    \n",
    "    df[f\"{model_name}_preds\"] = preds\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "def add_metric_cols_to_preds(preds_col_name, use_validation=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add metric columns to the dataframe\n",
    "\n",
    "    Args:\n",
    "    - preds_col_name: name of the column containing the predictions\n",
    "    - use_validation: whether to use validation data or test data\n",
    "\n",
    "    Returns:\n",
    "    - updated dataframe\n",
    "    \"\"\"\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_PREDS_PATH\n",
    "    else:\n",
    "        save_path = TEST_PREDS_PATH\n",
    "\n",
    "    # Load CSV\n",
    "    df = pd.read_csv(save_path)\n",
    "\n",
    "    # Dynamically create column names\n",
    "    model_name = preds_col_name.split(\"_\")[0]\n",
    "    bleu_col_name = f\"{model_name}_BLEU\"\n",
    "    bleurt_col_name = f\"{model_name}_BLEURT\"\n",
    "    sta_col_name = f\"{model_name}_STA\"\n",
    "    flu_col_name = f\"{model_name}_FLU\"\n",
    "    sem_col_name = f\"{model_name}_SEM\"\n",
    "\n",
    "    # If replace existing, remove the existing columns\n",
    "    if bleu_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[bleu_col_name])\n",
    "    if bleurt_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[bleurt_col_name])\n",
    "    if sta_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[sta_col_name])\n",
    "    if flu_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[flu_col_name])\n",
    "    if sem_col_name in df.columns and replace_existing:\n",
    "        df = df.drop(columns=[sem_col_name])\n",
    "\n",
    "    # Calculate metrics\n",
    "    df[bleu_col_name] = df.apply(lambda row: calc_sacrebleu([row[\"target\"]], [row[preds_col_name]]), axis=1)\n",
    "    df[bleurt_col_name] = calc_bleurt(df[\"target\"], df[preds_col_name], output_mean=False)\n",
    "    df[sta_col_name] = 1 - calc_tox_acceptability(df[preds_col_name].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    df[flu_col_name] = calc_tox_acceptability(df[preds_col_name].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "    df[sem_col_name] = calc_bert_score(df[\"target\"], df[preds_col_name], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(model_name, metrics, use_validation=True, load_csv=True, replace_existing=True):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    - test_data: whether the data is test data or validation data (True for test data, False for validation data)\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    - replace_existing: whether to replace an existing column if it already exists\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Set save path\n",
    "    if use_validation:\n",
    "        save_path = VAL_METRICS_PATH\n",
    "    else:\n",
    "        save_path = TEST_METRICS_PATH\n",
    "        \n",
    "    # Load the existing dataframe if it exists\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"J\"])\n",
    "\n",
    "    # Check if the model name already exists in the dataframe\n",
    "    if model_name in df[\"Model\"].values and not replace_existing:\n",
    "        print(f\"Model {model_name} already exists in the dataframe.\")\n",
    "        return df\n",
    "\n",
    "    # If replace existing, remove the existing row with the same model name\n",
    "    if model_name in df[\"Model\"].values and replace_existing:\n",
    "        df = df[df[\"Model\"] != model_name]\n",
    "\n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"J\": [metrics[\"J\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer Object Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    \"\"\"Adds a prefix to the source sequence in the dataset.\"\"\"\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def create_bidirectional_dataset(datasets, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    if shuffle:\n",
    "        extended_datasets[\"train\"] = extended_datasets[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "        \n",
    "    return extended_datasets\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    \"\"\"Preprocesses a dataset using a tokenizer.\"\"\"\n",
    "    def preprocess_function(examples, tokenizer):\n",
    "        \"\"\"Preprocess function for T5.\"\"\"\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"source\"],\n",
    "            text_target=examples[\"target\"],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "        batched=True,\n",
    "        remove_columns=[\"source\", \"target\"],\n",
    "    )\n",
    "\n",
    "def post_process(preds, refs, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5.\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        refs (list): list of reference sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "        decoded_refs (list): list of decoded reference sequences\n",
    "    \"\"\"\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    return decoded_preds, decoded_refs\n",
    "\n",
    "def post_process_preds(preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5 (only for predictions)\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "    \"\"\"\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "\n",
    "    return decoded_preds\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset to use for testing created using create_bidirectional_datasets\n",
    "                                  For example, raw_datasets_bd[\"validation\"] or raw_datasets_bd[\"test\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = [i for i, input_sentence in enumerate(bd_dataset['source']) if input_sentence.startswith(\"to_neutral\")]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                compute_metrics,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                report_to=\"wandb\",\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Evaluation dataset.\n",
    "        compute_metrics (function): Function to compute metrics. Change this to compute_metrics_bd if using a bi-directional model.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "        report_to (str): Where to report results to. Either \"wandb\" or \"none\".\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    gen_config_path = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(gen_config_path)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=gen_config_path,\n",
    "        fp16=True,\n",
    "        report_to=report_to,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "   \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def training_pipeline(model_name, project_name=\"t5-detox\", model_checkpoint=\"t5-small\", use_validation=True, raw_datasets=raw_datasets, bidirectional=False, shuffle=False, do_train=True):\n",
    "    \"\"\"\n",
    "    Pipeline for training a T5 model. Saves the best model checkpoint to a txt file. Can also be used for evaluating a model (use test set instead of validation set).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to name the output directory and wandb run.\n",
    "        project_name (str): Name of the wandb project.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        use_validation (bool): Whether to use the validation set or not.\n",
    "        raw_datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        bidirectional (bool): Whether to use a bi-directional model or not.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "        do_train (bool): Whether to train the model or not.\n",
    "\n",
    "    Returns:\n",
    "        trainer (Seq2SeqTrainer): Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess dataset (add prefixes / make bidirectional)\n",
    "    if bidirectional:\n",
    "        raw_datasets = create_bidirectional_dataset(raw_datasets, shuffle=shuffle)\n",
    "    else:\n",
    "        raw_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = preprocess_dataset(raw_datasets, tokenizer_t5_small)\n",
    "\n",
    "    # Define compute_metrics function depending on bidirectional or not\n",
    "    if bidirectional and use_validation:\n",
    "        bd_dataset = raw_datasets[\"validation\"]\n",
    "    elif bidirectional and not use_validation:\n",
    "        bd_dataset = raw_datasets[\"test\"]\n",
    "    else:\n",
    "        bd_dataset = None\n",
    "\n",
    "    compute_metrics_fn = partial(compute_metrics_bd, bd_dataset=bd_dataset, shuffled_data=shuffle) if bd_dataset else compute_metrics\n",
    "\n",
    "    # Setup trainer\n",
    "    trainer = setup_trainer(\n",
    "        output_dir_name=model_name,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"] if use_validation else tokenized_datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    if do_train:\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=project_name, name=model_name)\n",
    "        trainer.train()\n",
    "        wandb.finish()\n",
    "\n",
    "        # Get the best checkpoint path for the model\n",
    "        checkpoint_path = trainer.state.best_model_checkpoint\n",
    "\n",
    "        # Save the checkpoint path for the best model\n",
    "        with open(BEST_MODEL_CHECKPOINT_PATH, \"a\") as file:\n",
    "            file.write(f\"{model_name}: {checkpoint_path}\\n\")\n",
    "\n",
    "    return trainer, tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Negative Lexically Constrained Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this documentation for bad_word_ids: https://github.com/huggingface/transformers/issues/14206\n",
    "\n",
    "def get_bad_words_list(dataset, tokenizer=tokenizer_toxicity, model=model_toxicity, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Gets the top k bad words for each sentence in the dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset (list): List of sentences.\n",
    "        tokenizer (PreTrainedTokenizer): Tokenizer to use (toxicity classifier).\n",
    "        model (PreTrainedModel): Model to use (toxicity classifier).\n",
    "        num_layers (int): Number of layers to use.\n",
    "        top_k (int): Number of bad words to return.\n",
    "\n",
    "    Returns:\n",
    "        bad_words_list (list): List of lists of bad words.\n",
    "    \"\"\"    \n",
    "    bad_words_list = []\n",
    "\n",
    "    for sentence in dataset:\n",
    "        # Tokenize sentence\n",
    "        inputs = tokenizer(sentence, return_tensors=\"pt\").to(DEVICE)\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "\n",
    "        # Get attention scores\n",
    "        attention = model(input_ids, output_attentions=True)['attentions']\n",
    "\n",
    "        # Get the last num_layers layer attention scores and average them\n",
    "        attention = torch.stack(attention[-num_layers:]).mean(0)\n",
    "\n",
    "        # Average across each head\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Sum each row to get the attention score for each token\n",
    "        attention = attention.mean(1)\n",
    "\n",
    "        # Exclude separator tokens and punctuation\n",
    "        token_list = input_ids.squeeze().tolist()\n",
    "        punctuation_ids = {tokenizer.convert_tokens_to_ids(token) for token in string.punctuation}\n",
    "        exclude_ids = set([tokenizer.bos_token_id, tokenizer.eos_token_id]) | punctuation_ids\n",
    "\n",
    "        valid_indices = [i for i, token_id in enumerate(token_list) if token_id not in exclude_ids]\n",
    "\n",
    "        # Filter out the valid indices from the attention scores\n",
    "        valid_attention = attention.squeeze()[valid_indices]\n",
    "\n",
    "        # Get the indices of the top k tokens with the highest attention scores among valid tokens\n",
    "        top_indices = valid_attention.topk(top_k).indices.tolist()\n",
    "        top_token_indices = [valid_indices[i] for i in top_indices]\n",
    "\n",
    "        # Decode the tokens\n",
    "        bad_words = [tokenizer.decode(token_list[index]).strip() for index in top_token_indices]\n",
    "\n",
    "        bad_words_list.append(bad_words)\n",
    "\n",
    "    return bad_words_list\n",
    "\n",
    "def get_bad_word_ids(dataset,\n",
    "                     tokenizer_toxicity=tokenizer_toxicity,\n",
    "                     model_toxicity=model_toxicity,\n",
    "                     tokenizer_t5=tokenizer_t5_small,\n",
    "                     num_layers=3,\n",
    "                     top_k=3):\n",
    "    \"\"\"\n",
    "    Get the bad word IDs for a given dataset using the toxicity classifier.\n",
    "\n",
    "    Args:\n",
    "    - dataset: The dataset to get the bad word IDs for.\n",
    "    - tokenizer_toxicity: The tokenizer for the toxicity classifier.\n",
    "    - model_toxicity: The toxicity classifier model.\n",
    "    - tokenizer_t5: The tokenizer for the T5 model.\n",
    "    - num_layers: The number of layers to use for the attention-based bad word identification.\n",
    "    - top_k: The number of top words to select from each layer.\n",
    "\n",
    "    Returns:\n",
    "    - bad_word_ids: A list of lists, where each inner list contains the bad word IDs for a sentence in the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get list of bad words as identified using attention from toxicity classifier\n",
    "    bad_words_list = get_bad_words_list(dataset, tokenizer_toxicity, model_toxicity, num_layers, top_k)\n",
    "\n",
    "    # Convert each list of bad words to a string\n",
    "    bad_words_str_list = [\" \".join(bad_words) for bad_words in bad_words_list]\n",
    "\n",
    "    # Encode the bad words using the T5 tokenizer encode\n",
    "    bad_word_ids = [tokenizer_t5.encode(bad_words, add_special_tokens=False) for bad_words in bad_words_str_list]\n",
    "\n",
    "    return bad_word_ids\n",
    "\n",
    "def get_preds_nlcd(use_validation,\n",
    "                   model_checkpoint,\n",
    "                   raw_datasets=raw_datasets,\n",
    "                   batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                   num_beams=NUM_BEAMS,\n",
    "                   max_length=MAX_OUTPUT_LENGTH,\n",
    "                   num_batches=None,\n",
    "                   num_bw_layers=3,\n",
    "                   bw_top_k=3):\n",
    "    \"\"\"\n",
    "    Generate predictions using Negative Lexially Constrained Decoding.\n",
    "\n",
    "    Args:\n",
    "    - use_validation: Whether to use the validation dataset or the test dataset.\n",
    "    - model_checkpoint: The checkpoint of the T5 model to use.\n",
    "    - raw_datasets: The raw datasets dictionary object containing the source sentences.\n",
    "    - batch_size: The batch size for processing the data.\n",
    "    - num_beams: The number of beams for beam search.\n",
    "    - max_length: The maximum length of the generated output.\n",
    "    - num_batches: The number of batches to process. If None, it will be calculated based on the dataset size.\n",
    "    - num_bw_layers: The number of layers to use for the attention-based bad word identification.\n",
    "    - bw_top_k: The number of top words to select from each layer.\n",
    "\n",
    "    Returns:\n",
    "    - all_preds: A list of predicted sentences for the NLCD task.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(DEVICE)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "    # Prefix the dataset\n",
    "    prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Get the raw sentences\n",
    "    raw_sentences = prefixed_datasets[\"validation\"][\"source\"] if use_validation else prefixed_datasets[\"test\"][\"source\"]\n",
    "\n",
    "    # Tokenize the raw sentences\n",
    "    input_ids = tokenizer(raw_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_INPUT_LENGTH, add_special_tokens=False)['input_ids'].to(DEVICE)\n",
    "\n",
    "    # Define bad word ids\n",
    "    bad_word_ids = get_bad_word_ids(raw_sentences, num_layers=num_bw_layers, top_k=bw_top_k)\n",
    "\n",
    "    # Initialize a list to store all predictions\n",
    "    all_preds = []\n",
    "\n",
    "    # Determine the number of batches\n",
    "    if num_batches is None:\n",
    "        num_batches = (len(raw_sentences) + batch_size - 1) // batch_size\n",
    "\n",
    "    # Process data in batches\n",
    "    for batch_idx in tqdm(range(num_batches)):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = (batch_idx + 1) * batch_size\n",
    "\n",
    "        # Extract a batch of tokenized sentences\n",
    "        batch_input_ids = input_ids[start_idx:end_idx]\n",
    "\n",
    "        # Generate predictions for the batch\n",
    "        encoded_preds = model.generate(\n",
    "            inputs=batch_input_ids,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            pad_token_id=model.config.pad_token_id,\n",
    "            decoder_start_token_id=model.config.pad_token_id,\n",
    "            bad_words_ids=bad_word_ids\n",
    "        )\n",
    "\n",
    "        # Decode the predictions for the batch\n",
    "        decoded_preds = tokenizer.batch_decode(encoded_preds, skip_special_tokens=True)\n",
    "\n",
    "        # Post-process the predictions for the batch\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "\n",
    "        # Append the batch predictions to the list of all predictions\n",
    "        all_preds.extend(decoded_preds)\n",
    "\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5 Evaluation Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_checkpoints():\n",
    "    # Get checkpoint values for the best models\n",
    "    with open(BEST_MODEL_CHECKPOINT_PATH, \"r\") as f:\n",
    "        best_model_checkpoints = f.readlines()\n",
    "\n",
    "    # Convert to a dictionary\n",
    "    best_model_checkpoints_dict = {}\n",
    "    for line in best_model_checkpoints:\n",
    "        model_name, checkpoint_path = line.split(\": \")\n",
    "        best_model_checkpoints_dict[model_name] = checkpoint_path.strip()\n",
    "\n",
    "    return best_model_checkpoints_dict\n",
    "\n",
    "model_checkpoints = get_model_checkpoints()\n",
    "\n",
    "def get_t5_preds_metrics(model_checkpoint,\n",
    "                         raw_datasets=raw_datasets,\n",
    "                         bidirectional=False,\n",
    "                         shuffle=False,\n",
    "                         use_validation=True,\n",
    "                         do_train=False,\n",
    "                         tokenizer=tokenizer_t5_small\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Returns the predictions and metrics for a fine-tuned T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training pipeline\n",
    "    trainer, trainer_tokenized_ds = training_pipeline(\n",
    "        model_name=\"n/a\",\n",
    "        project_name=\"n/a\",\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        use_validation=use_validation,\n",
    "        raw_datasets=raw_datasets,\n",
    "        bidirectional=bidirectional,\n",
    "        shuffle=shuffle,\n",
    "        do_train=do_train\n",
    "    )\n",
    "\n",
    "    # Get raw predictions\n",
    "    if use_validation:\n",
    "        trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"validation\"])\n",
    "    else:\n",
    "        trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"test\"])\n",
    "\n",
    "    # Get encoded predictions and metrics\n",
    "    trainer_preds_encoded, trainer_metrics = trainer_preds_raw.predictions, trainer_preds_raw.metrics\n",
    "\n",
    "    # Post-process predictions\n",
    "    if isinstance(trainer_preds_encoded, tuple):\n",
    "        trainer_preds_encoded = trainer_preds_encoded[0]\n",
    "\n",
    "    trainer_preds_decoded = tokenizer.batch_decode(trainer_preds_encoded, skip_special_tokens=True)\n",
    "    trainer_preds_decoded = [pred.strip() for pred in trainer_preds_decoded]\n",
    "\n",
    "    #Return trainer metrics in the same format as evaluate_metrics\n",
    "    trainer_metrics = {\n",
    "        \"BLEU\": trainer_metrics[\"test_BLEU\"],\n",
    "        \"BLEURT\": trainer_metrics[\"test_BLEURT\"],\n",
    "        \"STA\": trainer_metrics[\"test_STA\"],\n",
    "        \"FLU\": trainer_metrics[\"test_FLU\"],\n",
    "        \"SEM\": trainer_metrics[\"test_SEM\"],\n",
    "        \"J\": trainer_metrics[\"test_J\"]\n",
    "    }\n",
    "        \n",
    "    # Return predictions and metrics\n",
    "    return trainer_preds_decoded, trainer_metrics\n",
    "\n",
    "def compare_outputs(\n",
    "    df,\n",
    "    cols_to_compare = [\"source\", \"target\", \"BART_preds\", \"T5-UD-DA_preds\", \"T5-UD-DA-MinLoss_preds\"],\n",
    "    bad_words_cols = ['source', 'T5-UD_preds'],\n",
    "    num_examples = 5,\n",
    "    num_layers = 5,\n",
    "    top_k = 3,\n",
    "    print_toxic_words = True,\n",
    "    random_state = RANDOM_SEED,\n",
    "):\n",
    "    \"\"\"\"\"\"\n",
    "    # Randomly sample the filtered dataset\n",
    "    df_sample = df.sample(n=num_examples, random_state=random_state)\n",
    "\n",
    "    # Get toxic words for in bad_words_col_1, bad_words_col_2, and bad_words_col_3\n",
    "    if print_toxic_words and bad_words_cols is not None:\n",
    "        ## Initialize a dictionary of bad words\n",
    "        bad_words = {}\n",
    "\n",
    "        ## Get bad words for each model\n",
    "        for bad_words_col in bad_words_cols:\n",
    "            bad_words[bad_words_col] = get_bad_words_list(df_sample[bad_words_col], num_layers=num_layers, top_k=top_k)\n",
    "\n",
    "    # Print every line for each col in cols_to_compare, and the bad words for each col in bad_words_cols\n",
    "    for i in range(num_examples):\n",
    "        if cols_to_compare is not None:\n",
    "            for col in cols_to_compare:\n",
    "                # Format the column name to remove _preds and capitalise first letter\n",
    "                if col in ['source', 'target']:\n",
    "                    col_name = col.capitalize()\n",
    "                elif col.endswith(\"_preds\"):\n",
    "                    col_name = col[:-len(\"_preds\")]\n",
    "                print(f\"{col_name}: {df_sample[col].iloc[i]}\")\n",
    "        if bad_words_cols is not None:\n",
    "            for bad_words_col in bad_words_cols:\n",
    "                # Format the column name to remove _preds and capitalise first letter\n",
    "                if bad_words_col in ['source', 'target']:\n",
    "                    bad_words_col_name = bad_words_col.capitalize()\n",
    "                elif bad_words_col.endswith(\"preds\"):\n",
    "                    bad_words_col_name = bad_words_col[:-len(\"_preds\")]\n",
    "                print(f\"'Toxic' words in {bad_words_col_name}: {bad_words[bad_words_col][i]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Using Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'BLEU': 1.0000000000000004,\n",
       " 'STA': 0.9538977367979883,\n",
       " 'FLU': 0.7160852,\n",
       " 'SEM': 0.9999999969023571,\n",
       " 'J': 0.9247761332079433,\n",
       " 'BLEURT': 0.9892257596401237}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], raw_datasets[\"validation\"][\"target\"])\n",
    "human_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n",
      "/tmp/ipykernel_15355/1601193926.py:132: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df = pd.concat([df, model_metrics_df], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "delete_val_preds = baseline_detoxifier(raw_datasets[\"validation\"][\"source\"])\n",
    "df_val_preds = add_preds_to_df(\"DELETE\", delete_val_preds, use_validation=True, load_csv=False)\n",
    "\n",
    "delete_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], delete_val_preds)\n",
    "df_val_metrics = add_metrics_to_df(\"DELETE\", delete_val_metrics, use_validation=True, load_csv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bart_val_preds = bart_detoxifier(raw_datasets[\"validation\"][\"source\"])\n",
    "df_val_preds = add_preds_to_df(\"BART\", bart_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "bart_val_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], bart_val_preds)\n",
    "df_val_metrics = add_metrics_to_df(\"BART\", bart_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_preds = add_metric_cols_to_preds(\"BART_preds\", use_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0  DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1    BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f92a3a0a683402c8e23ff8b8bc56d5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb75a0816ea04b19ae99df6c8023c506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609ad31c1d9145199b1a3c52e1a41321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_val_preds, t5_ud_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe and per-row metrics\n",
    "df_val_preds = add_preds_to_df(\"T5-UD\", t5_ud_val_preds, use_validation=True, load_csv=True)\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD_preds\", use_validation=True)\n",
    "\n",
    "# Add aggregated metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD\", t5_ud_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this example, the largest differences between BART and T5-Small-UD are in relation to BLEURT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of BLEURT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Most people understand trumps comments for what they were the whining of a sore loser.\n",
      "Target: Most people understand trumps comments were result of his defeat.\n",
      "BART: Most people understand trumps comments for what they were.\n",
      "T5-UD: Most people understand trumps comments for what they were the bad of a loser.\n",
      "\n",
      "Source: But then the incredibly stupid can continue to ignore the obvious.\n",
      "Target: But then those who don't know anything can continue to not pay attention to the obvious.\n",
      "BART: But then they can continue to ignore the obvious.\n",
      "T5-UD: But then the incredibly bad can continue to ignore the obvious.\n",
      "\n",
      "Source: Rt ima agree with you on that his shit is so bland and shallow\n",
      "Target: I agree with you that his performance is bland and shallow\n",
      "BART: I agree with you that his performance is bland and shallow\n",
      "T5-UD: Rt ima agree with you on that his thing is so bland and shallow\n",
      "\n",
      "Source: No bitch i miss being 80 you on the other hand are still a \" teen\n",
      "Target: No girl i miss being 80 you on the other hand are still a \" teen\n",
      "BART: No girl I miss being 80 you on the other hand are still a \"teen\n",
      "T5-UD: I miss being 80 you on the other hand are still a \" teen\n",
      "\n",
      "Source: Try thinking before you type, it makes you appear less foolish.\n",
      "Target: Try thinking before you type, it makes you more smart.\n",
      "BART: Try thinking before you type, it makes you more smart.\n",
      "T5-UD: Try thinking before you type, it makes you appear less foolish.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART has better BLEURT score than T5-UD\n",
    "df_BLEURT_BART_vs_T5_UD = df_val_preds[(df_val_preds[\"BART_BLEURT\"] >= df_val_preds[\"T5-UD_BLEURT\"] * 1.2) & (df_val_preds[\"BART_BLEURT\"] != df_val_preds[\"T5-UD_BLEURT\"])]\n",
    "\n",
    "# Compmare outputs\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_BART_vs_T5_UD,\n",
    "    cols_to_compare=['source', 'target', 'BART_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of STA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Who the fuck cuts out part of the state of the union?\n",
      "Target: Who cuts out part of the state of tge union.\n",
      "BART: Who cuts out part of the state of the union?\n",
      "T5-UD: Who cuts out part of the state of the union?\n",
      "'Toxic' words in ourc: ['fuck', 'the', 'part']\n",
      "'Toxic' words in T5-UD: ['state', 'cuts', 'part']\n",
      "\n",
      "Source: Nah yu seen how i act n public sober n shit\n",
      "Target: Nah youve seen how i act in public, sober and all.\n",
      "BART: Nah yu seen how i act in public sober\n",
      "T5-UD: Nah you have seen how i act n public sober\n",
      "'Toxic' words in ourc: ['shit', 'i', 'u']\n",
      "'Toxic' words in T5-UD: ['i', 'sober', 'n']\n",
      "\n",
      "Source: Shit chillen fresh out the shower\n",
      "Target: Chillen fresh out of the shower\n",
      "BART: Chillen fresh out of the shower\n",
      "T5-UD: Coolen fresh out the shower\n",
      "'Toxic' words in ourc: ['it', 'Sh', 'chill']\n",
      "'Toxic' words in T5-UD: ['Cool', 'en', 'shower']\n",
      "\n",
      "Source: These repubicans are like cannibalistic piraas.\n",
      "Target: These republicans are taking advantage.\n",
      "BART: These republicans are not good.\n",
      "T5-UD: These repubicans are like cannibalistic people.\n",
      "'Toxic' words in ourc: ['cannibal', 'are', 'icans']\n",
      "'Toxic' words in T5-UD: ['cannibal', 'are', 'icans']\n",
      "\n",
      "Source: I mean very irritating to see females fightin over some dick\n",
      "Target: Its irritating to see females fighting over boys\n",
      "BART: It is very irritating to see females fighting over boys\n",
      "T5-UD: I mean very irritating to see females fightin over something\n",
      "'Toxic' words in ourc: ['dick', 'females', 'some']\n",
      "'Toxic' words in T5-UD: ['females', 'fight', 'irritating']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART is better than T5-Small-UD in terms of STA\n",
    "df_STA_BART_vs_T5_UD = df_val_preds[(df_val_preds[\"BART_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_BART_vs_T5_UD,\n",
    "    cols_to_compare=['source', 'target', 'BART_preds', 'T5-UD_preds'],\n",
    "    bad_words_cols=['source', 'T5-UD_preds'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Bidirectional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c8c4a83c765452e6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-564d1ea8c297b791.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-d7d403f057f73219.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_bd_val_preds, t5_bd_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_bidir_shuf\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-BD\", t5_bd_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add per-row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-BD_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-BD\", t5_bd_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0  DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1    BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2   T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055\n",
       "3   T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of BLEURT vs. T5-Small-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Lol fuck im ever doing that formsprings, perfect exampleas why not to do it\n",
      "Target: Lol im ever doing that formsprings, perfect exampleas why not to do it\n",
      "T5-UD: I'm ever doing that formsprings, perfect exampleas why not to do it\n",
      "T5-BD: Lol im ever doing that formsprings, perfect exampleas why not to do it\n",
      "\n",
      "Source: Hey, respect to op for sticking around and not giving a shit\n",
      "Target: Hey, respect to op for sticking around and not caring about it.\n",
      "T5-UD: Hey, respect to op for sticking around and not giving a thing\n",
      "T5-BD: Hey, respect to op for sticking around and not caring\n",
      "\n",
      "Source: Im having the hardest time accepting that shit man!!! irefuse to believe this shit\n",
      "Target: I'm having the hardest time accepting that man, I refuse to believe this.\n",
      "T5-UD: Im having the hardest time accepting that man!!! irefuse to believe this man\n",
      "T5-BD: I am having the hardest time accepting that man!!! irefuse to believe this\n",
      "\n",
      "Source: Icantdateyou if ur hairy af - _ - fucks noo! lol\n",
      "Target: I cant date you if ur hairy\n",
      "T5-UD: I cant date you if you are hairy noo! lol\n",
      "T5-BD: I cant date you if ur hairy noo! lol\n",
      "\n",
      "Source: | smh they really be stashing and don't tell us shit\n",
      "Target: | smh they really be stashing and don't tell us a word\n",
      "T5-UD: They really be stashing and don't tell us anything\n",
      "T5-BD: | smh they really be stashing and don't tell us anything\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BLEURT of T5-Small-BD exceeds T5-Small-UD by 20% and are not identical\n",
    "df_BLEURT_T5_BD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-BD_BLEURT\"] >= df_val_preds[\"T5-UD_BLEURT\"] * 1.2) & (df_val_preds[\"T5-BD_BLEURT\"] != df_val_preds[\"T5-UD_BLEURT\"])]\n",
    "\n",
    "compare_outputs(\n",
    "    df=df_BLEURT_T5_BD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD_preds','T5-BD_preds'],\n",
    "    bad_words_cols=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of STA vs. T5-Small-UD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: You already burned down to a crisp in fort mac by your own stupidity, already.\n",
      "Target: You already lost in fort mac.\n",
      "T5-UD: You already burned down to a crisp in fort mac by your own ignorance, already.\n",
      "T5-BD: You already burned down to a crisp in fort mac by yourself, already.\n",
      "'Toxic' words in Source: ['stupidity', 'burned']\n",
      "'Toxic' words in T5-UD: ['You', 'burned']\n",
      "\n",
      "Source: True conservatives, that are not white racists, will have to find a new home.\n",
      "Target: Correct\n",
      "T5-UD: True conservatives, that are not white, will have to find a new home.\n",
      "T5-BD: True conservatives will have to find a new home.\n",
      "'Toxic' words in Source: ['white', 'racists']\n",
      "'Toxic' words in T5-UD: ['white', 'True']\n",
      "\n",
      "Source: So you would love to see the commys win over a duly elected democratic president, man you are sick\n",
      "Target: So you would love to see the communist win over a duly elected democratic president\n",
      "T5-UD: So you would love to see the commys win over a duly elected democratic president, man you are sick\n",
      "T5-BD: So you would love to see the commys win over a duly elected democratic president\n",
      "'Toxic' words in Source: ['sick', 'you']\n",
      "'Toxic' words in T5-UD: ['sick', 'you']\n",
      "\n",
      "Source: Shut the fuck up, geek!\n",
      "Target: Please, keep quiet!\n",
      "T5-UD: Shut up.\n",
      "T5-BD: Keep quiet, geek!\n",
      "'Toxic' words in Source: ['fuck', 'Shut']\n",
      "'Toxic' words in T5-UD: ['Shut', 'up']\n",
      "\n",
      "Source: We are done with the unamerican, disrespectful denver broncos!\n",
      "Target: We are done with them\n",
      "T5-UD: We are done with the unamerican, disrespectful denver broncos!\n",
      "T5-BD: We are done with the unamerican denver broncos!\n",
      "'Toxic' words in Source: ['disrespectful', 'the']\n",
      "'Toxic' words in T5-UD: ['disrespectful', 'the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataset to rows where BART is better than T5-Small-UD in terms of STA\n",
    "df_STA_T5_BD_vs_T5_UD = df_val_preds[(df_val_preds[\"T5-BD_STA\"] == 1) & (df_val_preds[\"T5-UD_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_BD_vs_T5_UD,\n",
    "    cols_to_compare=['source','target','T5-UD_preds','T5-BD_preds'],\n",
    "    bad_words_cols=['source','T5-UD_preds'],\n",
    "    num_examples=5,\n",
    "    num_layers=5,\n",
    "    top_k=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Small (Unidirectional, Data Augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'t5_small_unidir': '../models/t5_small_unidir/checkpoint-840',\n",
       " 't5_small_bidir_noshuf': '../models/t5_small_bidir_noshuf/checkpoint-2352',\n",
       " 't5_small_bidir_shuf': '../models/t5_small_bidir_shuf/checkpoint-3024',\n",
       " 't5_small_aug_all': '../models/t5_small_aug_all/checkpoint-2592',\n",
       " 't5_small_aug_noaccept': '../models/t5_small_aug_noaccept/checkpoint-1620',\n",
       " 't5_small_aug_nosim': '../models/t5_small_aug_nosim/checkpoint-2592',\n",
       " 't5_small_aug_notox': '../models/t5_small_aug_notox/checkpoint-1944'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c8c4a83c765452e6.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-564d1ea8c297b791.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-d7d403f057f73219.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_val_preds, t5_ud_da_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_all\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA\", t5_ud_da_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-DA_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA\", t5_ud_da_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Filters (Minimum Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54e53273a7b74dac91313a3e0502e75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8d3533741b498a8c3e30d441e9b90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bf3220e78364676b7054b3a5ca3d44f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_minloss_val_preds, t5_ud_da_minloss_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=\"../models/t5_small_aug_all/checkpoint-972\",\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-MinLoss\", t5_ud_da_minloss_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add row metrics to dataframe\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-DA-MinLoss_preds\", use_validation=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "# df_val_metrics = add_metrics_to_df(\"T5-Small-Aug-All-MinLoss\", t5_ud_da_minloss_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Outputs of All Filter vs. All Filter (Minimum Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Bitch was so mad lmfao!! smh\n",
      "Target: She was so mad!\n",
      "T5-UD-DA-MinLoss: Bitch was so mad\n",
      "T5-UD-DA: He was so mad.\n",
      "'Toxic' words in Source: ['itch', 'B', 'was']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['itch', 'was', 'B']\n",
      "\n",
      "Source: Pitiful choice scalia was a hypocrite!\n",
      "Target: Scalia was a bad person\n",
      "T5-UD-DA-MinLoss: Choice scalia was a hypocrite!\n",
      "T5-UD-DA: Choice scalia was not a good person!\n",
      "'Toxic' words in Source: ['sc', 'was', 'hypocr']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['was', 'sc', 'hypocr']\n",
      "\n",
      "Source: He's a fucking street thug.\n",
      "Target: He's a street gang member.\n",
      "T5-UD-DA-MinLoss: He's a street thug\n",
      "T5-UD-DA: He's a street guy.\n",
      "'Toxic' words in Source: ['fucking', 'He', \"'s\"]\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['He', 'thug', \"'s\"]\n",
      "\n",
      "Source: Doosh doosh doosh doosh doosh fucking cock sucker queir bate ass wipe dont tell me what the fuck to write\n",
      "Target: Don't tell me what to write.\n",
      "T5-UD-DA-MinLoss: Doosh doosh doosh doosh cock sucker queir bate wipe dont tell me what to write\n",
      "T5-UD-DA: Don't tell me what to write\n",
      "'Toxic' words in Source: ['fucking', 'dont', 'fuck']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['dont', 'sucker', 'cock']\n",
      "\n",
      "Source: True conservatives, that are not white racists, will have to find a new home.\n",
      "Target: Correct\n",
      "T5-UD-DA-MinLoss: True conservatives, that are not white, will have to find a new home.\n",
      "T5-UD-DA: True conservatives, that are not racists, will have to find a new home.\n",
      "'Toxic' words in Source: ['white', 'racists', 'True']\n",
      "'Toxic' words in T5-UD-DA-MinLoss: ['white', 'True', 'are']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to rows where T5-UD-DA-MinLoss has STA == 0 and T5-UD-DA has STA == 1\n",
    "df_STA_T5_UD_DA_MinLoss_vs_T5_UD_DA = df_val_preds[(df_val_preds[\"T5-UD-DA-MinLoss_STA\"] == 0) & (df_val_preds[\"T5-UD-DA_STA\"] == 1)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_UD_DA_MinLoss_vs_T5_UD_DA,\n",
    "    cols_to_compare=['source','target','T5-UD-DA-MinLoss_preds', 'T5-UD-DA_preds'],\n",
    "    bad_words_cols=['source','T5-UD-DA-MinLoss_preds'],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Toxicity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_notox_val_preds, t5_ud_da_notox_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_notox\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoTOX\", t5_ud_da_notox_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoTOX\", t5_ud_da_notox_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Semantic Similarity Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_nosem_val_preds, t5_ud_da_nosem_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_nosim\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoSEM\", t5_ud_da_nosem_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoSEM\", t5_ud_da_nosem_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No Fluency Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-c5ec8683272f3fbe.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eccb671bff583efc.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-2b1be1a5e41615da.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions and metrics\n",
    "t5_ud_da_noflu_val_preds, t5_ud_da_noflu_val_metrics = get_t5_preds_metrics(\n",
    "    model_checkpoint=model_checkpoints[\"t5_small_aug_noaccept\"],\n",
    "    use_validation=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-DA-NoFLU\", t5_ud_da_noflu_val_preds, use_validation=True, load_csv=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-DA-NoFLU\", t5_ud_da_noflu_val_metrics, use_validation=True, load_csv=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>BLEURT</th>\n",
       "      <th>BLEU</th>\n",
       "      <th>STA</th>\n",
       "      <th>FLU</th>\n",
       "      <th>SEM</th>\n",
       "      <th>J</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DELETE</td>\n",
       "      <td>-0.227430</td>\n",
       "      <td>0.529101</td>\n",
       "      <td>0.659681</td>\n",
       "      <td>0.478651</td>\n",
       "      <td>0.911821</td>\n",
       "      <td>0.647787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BART</td>\n",
       "      <td>0.466564</td>\n",
       "      <td>0.701595</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.718025</td>\n",
       "      <td>0.945139</td>\n",
       "      <td>0.840093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T5-BD</td>\n",
       "      <td>0.217048</td>\n",
       "      <td>0.611682</td>\n",
       "      <td>0.912825</td>\n",
       "      <td>0.711991</td>\n",
       "      <td>0.926827</td>\n",
       "      <td>0.815230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T5-UD-DA</td>\n",
       "      <td>0.204206</td>\n",
       "      <td>0.593218</td>\n",
       "      <td>0.916178</td>\n",
       "      <td>0.714662</td>\n",
       "      <td>0.925547</td>\n",
       "      <td>0.813157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T5-UD-DA-NoTOX</td>\n",
       "      <td>0.208252</td>\n",
       "      <td>0.598625</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.713698</td>\n",
       "      <td>0.925555</td>\n",
       "      <td>0.804659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>T5-UD-DA-NoSEM</td>\n",
       "      <td>0.211811</td>\n",
       "      <td>0.591599</td>\n",
       "      <td>0.917854</td>\n",
       "      <td>0.719671</td>\n",
       "      <td>0.924972</td>\n",
       "      <td>0.814390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>T5-UD-DA-NoFLU</td>\n",
       "      <td>0.192281</td>\n",
       "      <td>0.595025</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.709992</td>\n",
       "      <td>0.925311</td>\n",
       "      <td>0.807508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>T5-UD</td>\n",
       "      <td>0.206119</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model    BLEURT      BLEU       STA       FLU       SEM         J\n",
       "0          DELETE -0.227430  0.529101  0.659681  0.478651  0.911821  0.647787\n",
       "1            BART  0.466564  0.701595  0.917854  0.718025  0.945139  0.840093\n",
       "2           T5-BD  0.217048  0.611682  0.912825  0.711991  0.926827  0.815230\n",
       "3        T5-UD-DA  0.204206  0.593218  0.916178  0.714662  0.925547  0.813157\n",
       "4  T5-UD-DA-NoTOX  0.208252  0.598625  0.892707  0.713698  0.925555  0.804659\n",
       "5  T5-UD-DA-NoSEM  0.211811  0.591599  0.917854  0.719671  0.924972  0.814390\n",
       "6  T5-UD-DA-NoFLU  0.192281  0.595025  0.903604  0.709992  0.925311  0.807508\n",
       "7           T5-UD  0.206119  0.606150  0.902766  0.707668  0.925923  0.809055"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5-Small (UD, NLCD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pred_nlcd(raw_sentence, model, tokenizer, tokenizer_prefix_space, num_layers=3, top_k=3):\n",
    "    \"\"\"\n",
    "    Generate predictions for a single raw sentence using the NLCD (Negative Lexically Constrained Decoding) method.\n",
    "\n",
    "    Args:\n",
    "        raw_sentence (str): The raw sentence to generate predictions for.\n",
    "        model (T5ForConditionalGeneration): The T5 model for generation.\n",
    "        tokenizer (T5Tokenizer): The tokenizer for tokenizing the input.\n",
    "        tokenizer_prefix_space (T5Tokenizer): The tokenizer for tokenizing the input with prefix space.\n",
    "        num_layers (int): The number of layers to consider for generating bad words.\n",
    "        top_k (int): The number of top bad words to consider.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated prediction for the input sentence.\n",
    "    \"\"\"\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer([raw_sentence], return_tensors=\"pt\")\n",
    "\n",
    "    def get_tokens_as_list(word_list):\n",
    "        \"Converts a sequence of words into a list of tokens\"\n",
    "        tokens_list = []\n",
    "        for word in word_list:\n",
    "            tokenized_word = tokenizer_prefix_space([word], add_special_tokens=False).input_ids[0]\n",
    "            tokens_list.append(tokenized_word)\n",
    "        return tokens_list\n",
    "\n",
    "    # Get toxic words\n",
    "    bad_words = get_bad_words_list([raw_sentence], tokenizer_toxicity, model_toxicity, num_layers=num_layers, top_k=top_k)[0]\n",
    "    bad_words_ids = get_tokens_as_list(word_list=bad_words)\n",
    "\n",
    "    # Generate text while enforcing the bad words to not appear\n",
    "    encoded_preds = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        bad_words_ids=bad_words_ids,\n",
    "        max_length=MAX_OUTPUT_LENGTH,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    decoded_preds = tokenizer.batch_decode(encoded_preds, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds][0]\n",
    "\n",
    "    return decoded_preds\n",
    "\n",
    "def generate_preds_nlcd(raw_sentences,\n",
    "                        model_checkpoint,\n",
    "                        num_layers=3,\n",
    "                        top_k=3):\n",
    "    \"\"\"\n",
    "    Generate predictions for a list of raw sentences using the NLCD (Negative Lexically Constrained Decoding) method.\n",
    "\n",
    "    Args:\n",
    "        raw_sentences (list): A list of raw sentences to generate predictions for.\n",
    "        model (T5ForConditionalGeneration): The T5 model for generation.\n",
    "        tokenizer (T5Tokenizer): The tokenizer for tokenizing the input.\n",
    "        tokenizer_prefix_space (T5Tokenizer): The tokenizer for tokenizing the bad words with prefix space.\n",
    "        num_layers (int): The number of layers to consider for generating bad words.\n",
    "        top_k (int): The number of top bad words to consider.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of generated predictions for the input sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load model and tokenizers based on the checkpoint\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint, add_prefix_space=False)\n",
    "    tokenizer_prefix_space = T5Tokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "    # Create a list to store the predictions\n",
    "    preds = []\n",
    "\n",
    "    # Iterate through each raw sentence\n",
    "    for raw_sentence in tqdm(raw_sentences):\n",
    "        # Generate predictions with NLCD\n",
    "        decoded_preds = generate_pred_nlcd(raw_sentence, model, tokenizer, tokenizer_prefix_space, num_layers, top_k)\n",
    "\n",
    "        # Append the predictions to the list of predictions\n",
    "        preds.append(decoded_preds)\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test that this works by filtering to cases where T5-UD_STA == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "100%|| 116/116 [00:38<00:00,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of sentences with toxic words in preds_no_nlcd: 105\n",
      "Success rate of no_nlcd: 0.09482758620689655\n",
      "Count of sentences with toxic words in preds_nlcd: 32\n",
      "Success rate of nlcd: 0.7241379310344828\n",
      "\n",
      "Source: to_neutral: You straighten out that chicago problem and then well tell our cops to stop killing unarmed blacks.\n",
      "Target: There are problems in Chicago that need to be dealt with as well as police brutality.\n",
      "Bad words: ['blacks', 'killing']\n",
      "Prediction without NLCD: You straighten out that chicago problem and then well tell our cops to stop killing unarmed blacks\n",
      "Prediction with NLCD: You straighten out that chicago problem and then well tell our cops to stop shooting unarmed people.\n",
      "\n",
      "Source: to_neutral: You have cold dead hands.\n",
      "Target: You have cold hands.\n",
      "Bad words: ['You', 'have']\n",
      "Prediction without NLCD: You have cold dead hands\n",
      "Prediction with NLCD: Your hands are not good.\n",
      "\n",
      "Source: to_neutral: This is ridiculous and looks terrible if true.\n",
      "Target: It's ridiculously terrible if true.\n",
      "Bad words: ['This', 'ridiculous']\n",
      "Prediction without NLCD: This is ridiculous and looks terrible if true.\n",
      "Prediction with NLCD: It looks terrible if true.\n",
      "\n",
      "Source: to_neutral: That is the most ridiculous, insane, yet funny comment ever by kruger.\n",
      "Target: That is the most ridiculous, insane, yet funny comment ever by kruger.\n",
      "Bad words: ['ridiculous', 'comment']\n",
      "Prediction without NLCD: That is the most ridiculous, insane, yet funny comment ever by Kruger\n",
      "Prediction with NLCD: That is the most absurd, insane, yet funny comments ever by kruger.\n",
      "\n",
      "Source: to_neutral: Gads, i hate your cruelty.\n",
      "Target: I don't like your cruelty.\n",
      "Bad words: ['hate', 'your']\n",
      "Prediction without NLCD: I hate your cruelty.\n",
      "Prediction with NLCD: I don't like you.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def nlcd_test():\n",
    "    # Prefix the dataset\n",
    "    prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Get the raw sentences\n",
    "    ## Get indices of rows where T5-UD has STA == 0\n",
    "    t5_ud_sta_0_indices = df_val_preds[df_val_preds[\"T5-UD_STA\"] == 0].index.tolist()\n",
    "\n",
    "    ## Get predictions from T5-UD where STA == 0\n",
    "    preds_no_nlcd = df_val_preds[df_val_preds[\"T5-UD_STA\"] == 0][\"T5-UD_preds\"].tolist()\n",
    "\n",
    "    ## Filter the raw sentences to just the ones where T5-UD has STA == 0\n",
    "    input_sentences = [prefixed_datasets[\"validation\"][\"source\"][i] for i in t5_ud_sta_0_indices]\n",
    "\n",
    "    ## Get the references\n",
    "    refs = [prefixed_datasets[\"validation\"][\"target\"][i] for i in t5_ud_sta_0_indices]\n",
    "\n",
    "    # Get toxic words for each sentence\n",
    "    toxic_words_per_sentence = get_bad_words_list(input_sentences, tokenizer_toxicity, model_toxicity, num_layers=3, top_k=2)\n",
    "\n",
    "    # Get predictions using NLCD\n",
    "    preds_nlcd = generate_preds_nlcd(input_sentences, model_checkpoints[\"t5_small_unidir\"], num_layers=3, top_k=2)\n",
    "\n",
    "    # Count the number of cases where preds_no_nlcd contains a toxic word and calculate success rate\n",
    "    num_toxic_words_no_nlcd = 0\n",
    "    for i in range(len(preds_no_nlcd)):\n",
    "        if any(toxic_word in preds_no_nlcd[i] for toxic_word in toxic_words_per_sentence[i]):\n",
    "            num_toxic_words_no_nlcd += 1\n",
    "    success_rate_no_nlcd = (len(preds_no_nlcd) - num_toxic_words_no_nlcd) / len(preds_no_nlcd)\n",
    "    print(f\"Count of sentences with toxic words in preds_no_nlcd: {num_toxic_words_no_nlcd}\")\n",
    "    print(f\"Success rate of no_nlcd: {success_rate_no_nlcd}\")\n",
    "\n",
    "    # Count the number of cases where preds_nlcd contains a toxic word and calculate success rate\n",
    "    num_toxic_words_nlcd = 0\n",
    "    for i in range(len(preds_nlcd)):\n",
    "        if any(toxic_word in preds_nlcd[i] for toxic_word in toxic_words_per_sentence[i]):\n",
    "            num_toxic_words_nlcd += 1\n",
    "    success_rate_nlcd = (len(preds_nlcd) - num_toxic_words_nlcd) / len(preds_nlcd)\n",
    "    print(f\"Count of sentences with toxic words in preds_nlcd: {num_toxic_words_nlcd}\")\n",
    "    print(f\"Success rate of nlcd: {success_rate_nlcd}\")\n",
    "\n",
    "    # Print out the first 5 predictions\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print(f\"Source: {input_sentences[i]}\")\n",
    "        print(f\"Target: {refs[i]}\")\n",
    "        print(f\"Bad words: {toxic_words_per_sentence[i]}\")\n",
    "        print(f\"Prediction without NLCD: {preds_no_nlcd[i]}\")\n",
    "        print(f\"Prediction with NLCD: {preds_nlcd[i]}\")\n",
    "\n",
    "nlcd_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below is to do hyperparameter optimization. This hasn't been done in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Attention Layers: 2, Top K: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 29%|       | 349/1193 [02:02<04:55,  2.86it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb Cell 64\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=75'>76</a>\u001b[0m             best_top_k \u001b[39m=\u001b[39m bw_top_k\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m metrics_df, best_num_layers, best_top_k\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m nlcd_metrics_df, best_num_layers, best_top_k \u001b[39m=\u001b[39m nlcd_hyperparameter_optimization()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39mprint\u001b[39m(nlcd_metrics_df)\n",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb Cell 64\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mfor\u001b[39;00m num_bw_layers \u001b[39min\u001b[39;00m num_bw_layers_list:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNumber of Attention Layers: \u001b[39m\u001b[39m{\u001b[39;00mnum_bw_layers\u001b[39m}\u001b[39;00m\u001b[39m, Top K: \u001b[39m\u001b[39m{\u001b[39;00mdefault_bw_top_k\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m     t5_ud_nlcd_preds \u001b[39m=\u001b[39m generate_preds_nlcd(raw_sentences\u001b[39m=\u001b[39;49mraw_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39msource\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m                                             model_checkpoint\u001b[39m=\u001b[39;49mmodel_checkpoint,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m                                             num_layers\u001b[39m=\u001b[39;49mnum_bw_layers,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m                                             top_k\u001b[39m=\u001b[39;49mdefault_bw_top_k)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m     t5_ud_nlcd_metrics \u001b[39m=\u001b[39m evaluate_metrics(raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m], t5_ud_nlcd_preds, include_bleurt\u001b[39m=\u001b[39minclude_bleurt)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mprint\u001b[39m(t5_ud_nlcd_metrics)\n",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb Cell 64\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39m# Iterate through each raw sentence\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=78'>79</a>\u001b[0m \u001b[39mfor\u001b[39;00m raw_sentence \u001b[39min\u001b[39;00m tqdm(raw_sentences):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39m# Generate predictions with NLCD\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     decoded_preds \u001b[39m=\u001b[39m generate_pred_nlcd(raw_sentence, model, tokenizer, tokenizer_prefix_space, num_layers, top_k)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39m# Append the predictions to the list of predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=83'>84</a>\u001b[0m     preds\u001b[39m.\u001b[39mappend(decoded_preds)\n",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb Cell 64\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m bad_words_ids \u001b[39m=\u001b[39m get_tokens_as_list(word_list\u001b[39m=\u001b[39mbad_words)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39m# Generate text while enforcing the bad words to not appear\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m encoded_preds \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=33'>34</a>\u001b[0m     inputs[\u001b[39m\"\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m     bad_words_ids\u001b[39m=\u001b[39;49mbad_words_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m     max_length\u001b[39m=\u001b[39;49mMAX_OUTPUT_LENGTH,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m     num_beams\u001b[39m=\u001b[39;49mNUM_BEAMS,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m     early_stopping\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m     eos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m     bos_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mbos_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m     pad_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m     decoder_start_token_id\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mpad_token_id\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Decode the generated text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/4_Evaluation.ipynb#Y234sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m decoded_preds \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mbatch_decode(encoded_preds, skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1496\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1488\u001b[0m         logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m   1489\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1490\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mgeneration results, please set `padding_side=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m\u001b[39m` when initializing the tokenizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1491\u001b[0m         )\n\u001b[1;32m   1493\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1494\u001b[0m     \u001b[39m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1495\u001b[0m     \u001b[39m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1496\u001b[0m     model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prepare_encoder_decoder_kwargs_for_generation(\n\u001b[1;32m   1497\u001b[0m         inputs_tensor, model_kwargs, model_input_name\n\u001b[1;32m   1498\u001b[0m     )\n\u001b[1;32m   1500\u001b[0m \u001b[39m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:661\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    659\u001b[0m encoder_kwargs[\u001b[39m\"\u001b[39m\u001b[39mreturn_dict\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    660\u001b[0m encoder_kwargs[model_input_name] \u001b[39m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 661\u001b[0m model_kwargs[\u001b[39m\"\u001b[39m\u001b[39mencoder_outputs\u001b[39m\u001b[39m\"\u001b[39m]: ModelOutput \u001b[39m=\u001b[39m encoder(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoder_kwargs)\n\u001b[1;32m    663\u001b[0m \u001b[39mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:1123\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1110\u001b[0m     layer_outputs \u001b[39m=\u001b[39m checkpoint(\n\u001b[1;32m   1111\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1112\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1120\u001b[0m         \u001b[39mNone\u001b[39;00m,  \u001b[39m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     )\n\u001b[1;32m   1122\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1123\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m   1124\u001b[0m         hidden_states,\n\u001b[1;32m   1125\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1126\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m   1127\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1128\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1129\u001b[0m         encoder_decoder_position_bias\u001b[39m=\u001b[39;49mencoder_decoder_position_bias,\n\u001b[1;32m   1130\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m   1131\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m   1132\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   1133\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1134\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1135\u001b[0m     )\n\u001b[1;32m   1137\u001b[0m \u001b[39m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[39m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:695\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    693\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer[\u001b[39m0\u001b[39;49m](\n\u001b[1;32m    696\u001b[0m     hidden_states,\n\u001b[1;32m    697\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    698\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    699\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    700\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    701\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    702\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    703\u001b[0m )\n\u001b[1;32m    704\u001b[0m hidden_states, present_key_value_state \u001b[39m=\u001b[39m self_attention_outputs[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    705\u001b[0m attention_outputs \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m2\u001b[39m:]  \u001b[39m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:602\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    592\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    593\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    599\u001b[0m     output_attentions\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    600\u001b[0m ):\n\u001b[1;32m    601\u001b[0m     normed_hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 602\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mSelfAttention(\n\u001b[1;32m    603\u001b[0m         normed_hidden_states,\n\u001b[1;32m    604\u001b[0m         mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    605\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    606\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49mlayer_head_mask,\n\u001b[1;32m    607\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    608\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    609\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(attention_output[\u001b[39m0\u001b[39m])\n\u001b[1;32m    612\u001b[0m     outputs \u001b[39m=\u001b[39m (hidden_states,) \u001b[39m+\u001b[39m attention_output[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:544\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    542\u001b[0m         position_bias\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 544\u001b[0m     position_bias \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_bias(real_seq_length, key_length, device\u001b[39m=\u001b[39;49mscores\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m    546\u001b[0m \u001b[39m# if key and values are already calculated\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[39m# we want only the last query position bias\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:444\u001b[0m, in \u001b[0;36mT5Attention.compute_bias\u001b[0;34m(self, query_length, key_length, device)\u001b[0m\n\u001b[1;32m    442\u001b[0m memory_position \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39marange(key_length, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)[\u001b[39mNone\u001b[39;00m, :]\n\u001b[1;32m    443\u001b[0m relative_position \u001b[39m=\u001b[39m memory_position \u001b[39m-\u001b[39m context_position  \u001b[39m# shape (query_length, key_length)\u001b[39;00m\n\u001b[0;32m--> 444\u001b[0m relative_position_bucket \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_relative_position_bucket(\n\u001b[1;32m    445\u001b[0m     relative_position,  \u001b[39m# shape (query_length, key_length)\u001b[39;49;00m\n\u001b[1;32m    446\u001b[0m     bidirectional\u001b[39m=\u001b[39;49m(\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_decoder),\n\u001b[1;32m    447\u001b[0m     num_buckets\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelative_attention_num_buckets,\n\u001b[1;32m    448\u001b[0m     max_distance\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelative_attention_max_distance,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelative_attention_bias(relative_position_bucket)  \u001b[39m# shape (query_length, key_length, num_heads)\u001b[39;00m\n\u001b[1;32m    451\u001b[0m values \u001b[39m=\u001b[39m values\u001b[39m.\u001b[39mpermute([\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)  \u001b[39m# shape (1, num_heads, query_length, key_length)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/t5/modeling_t5.py:429\u001b[0m, in \u001b[0;36mT5Attention._relative_position_bucket\u001b[0;34m(relative_position, bidirectional, num_buckets, max_distance)\u001b[0m\n\u001b[1;32m    422\u001b[0m is_small \u001b[39m=\u001b[39m relative_position \u001b[39m<\u001b[39m max_exact\n\u001b[1;32m    424\u001b[0m \u001b[39m# The other half of the buckets are for logarithmically bigger bins in positions up to max_distance\u001b[39;00m\n\u001b[1;32m    425\u001b[0m relative_position_if_large \u001b[39m=\u001b[39m max_exact \u001b[39m+\u001b[39m (\n\u001b[1;32m    426\u001b[0m     torch\u001b[39m.\u001b[39;49mlog(relative_position\u001b[39m.\u001b[39;49mfloat() \u001b[39m/\u001b[39;49m max_exact)\n\u001b[1;32m    427\u001b[0m     \u001b[39m/\u001b[39;49m math\u001b[39m.\u001b[39;49mlog(max_distance \u001b[39m/\u001b[39;49m max_exact)\n\u001b[1;32m    428\u001b[0m     \u001b[39m*\u001b[39;49m (num_buckets \u001b[39m-\u001b[39;49m max_exact)\n\u001b[0;32m--> 429\u001b[0m )\u001b[39m.\u001b[39;49mto(torch\u001b[39m.\u001b[39;49mlong)\n\u001b[1;32m    430\u001b[0m relative_position_if_large \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmin(\n\u001b[1;32m    431\u001b[0m     relative_position_if_large, torch\u001b[39m.\u001b[39mfull_like(relative_position_if_large, num_buckets \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m    432\u001b[0m )\n\u001b[1;32m    434\u001b[0m relative_buckets \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(is_small, relative_position, relative_position_if_large)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def nlcd_hyperparameter_optimization(raw_datasets=raw_datasets,\n",
    "                                     model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "                                     num_bw_layers_list=[2, 3, 4],\n",
    "                                     bw_top_k_list=[1, 2, 3, 4, 5],\n",
    "                                     default_bw_top_k=3,\n",
    "                                     include_bleurt=False):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization for NLCD by trying different values for num_bw_layers and bw_top_k.\n",
    "\n",
    "    Args:\n",
    "    - model_checkpoint: The checkpoint of the T5 model to use.\n",
    "    - raw_datasets: The raw datasets dictionary object containing the source and target sentences.\n",
    "    - num_bw_layers_list: A list of values to try for num_bw_layers.\n",
    "    - bw_top_k_list: A list of values to try for bw_top_k.\n",
    "\n",
    "    Returns:\n",
    "    - metrics_df: A DataFrame containing the evaluation metrics for different hyperparameter combinations.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty DataFrame\n",
    "    metrics_df = pd.DataFrame(columns=[\"num_layers\", \"top_k\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"J\"])\n",
    "    best_J_score = -1\n",
    "    best_num_layers = None\n",
    "    best_top_k = None\n",
    "\n",
    "    # Try different values for num_bw_layers\n",
    "    for num_bw_layers in num_bw_layers_list:\n",
    "        print(f\"Number of Attention Layers: {num_bw_layers}, Top K: {default_bw_top_k}\")\n",
    "        t5_ud_nlcd_preds = generate_preds_nlcd(raw_sentences=raw_datasets[\"validation\"][\"source\"],\n",
    "                                                model_checkpoint=model_checkpoint,\n",
    "                                                num_layers=num_bw_layers,\n",
    "                                                top_k=default_bw_top_k)\n",
    "        t5_ud_nlcd_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], t5_ud_nlcd_preds, include_bleurt=include_bleurt)\n",
    "        print(t5_ud_nlcd_metrics)\n",
    "        \n",
    "        J_score = t5_ud_nlcd_metrics[\"J\"]\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            \"num_layers\": [num_bw_layers],\n",
    "            \"top_k\": [default_bw_top_k],\n",
    "            \"BLEU\": [t5_ud_nlcd_metrics[\"BLEU\"]],\n",
    "            \"STA\": [t5_ud_nlcd_metrics[\"STA\"]],\n",
    "            \"FLU\": [t5_ud_nlcd_metrics[\"FLU\"]],\n",
    "            \"SEM\": [t5_ud_nlcd_metrics[\"SEM\"]],\n",
    "            \"J\": [J_score]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        if J_score > best_J_score:\n",
    "            best_J_score = J_score\n",
    "            best_num_layers = num_bw_layers\n",
    "\n",
    "    # Iterate through bw_top_k while keeping the best num_bw_layers\n",
    "    for bw_top_k in bw_top_k_list:\n",
    "        print(f\"Number of Attention Layers: {best_num_layers}, Top K: {bw_top_k}\")\n",
    "        t5_ud_nlcd_preds = generate_preds_nlcd(raw_sentences=raw_datasets[\"validation\"][\"source\"],\n",
    "                                                model_checkpoint=model_checkpoint,\n",
    "                                                num_layers=best_num_layers,\n",
    "                                                top_k=bw_top_k)\n",
    "        t5_ud_nlcd_metrics = evaluate_metrics(raw_datasets[\"validation\"][\"target\"], t5_ud_nlcd_preds, include_bleurt=include_bleurt)\n",
    "        print(t5_ud_nlcd_metrics)\n",
    "        \n",
    "        J_score = t5_ud_nlcd_metrics[\"J\"]\n",
    "\n",
    "        metrics_df = pd.concat([metrics_df, pd.DataFrame({\n",
    "            \"num_layers\": [best_num_layers],\n",
    "            \"top_k\": [bw_top_k],\n",
    "            \"BLEU\": [t5_ud_nlcd_metrics[\"BLEU\"]],\n",
    "            \"STA\": [t5_ud_nlcd_metrics[\"STA\"]],\n",
    "            \"FLU\": [t5_ud_nlcd_metrics[\"FLU\"]],\n",
    "            \"SEM\": [t5_ud_nlcd_metrics[\"SEM\"]],\n",
    "            \"J\": [t5_ud_nlcd_metrics[\"J\"]]\n",
    "        })], ignore_index=True)\n",
    "\n",
    "        if J_score > best_J_score:\n",
    "            best_J_score = J_score\n",
    "            best_top_k = bw_top_k\n",
    "\n",
    "    return metrics_df, best_num_layers, best_top_k\n",
    "\n",
    "nlcd_metrics_df, best_num_layers, best_top_k = nlcd_hyperparameter_optimization()\n",
    "print(nlcd_metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get predictions based on num_layers = 3 and top_k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      " 13%|        | 155/1193 [00:50<05:28,  3.16it/s]"
     ]
    }
   ],
   "source": [
    "# Get predictions based on default hyperparameters\n",
    "t5_ud_nlcd_val_preds = generate_preds_nlcd(raw_sentences=raw_datasets[\"validation\"][\"source\"],\n",
    "                                             model_checkpoint=model_checkpoints[\"t5_small_unidir\"],\n",
    "                                             num_layers=3,\n",
    "                                             top_k=3)\n",
    "\n",
    "t5_ud_nlcd_val_metrics = evaluate_metrics(refs=raw_datasets[\"validation\"][\"target\"],\n",
    "                                          preds=t5_ud_nlcd_val_preds,\n",
    "                                          include_bleurt=True)\n",
    "\n",
    "# Add predictions to dataframe\n",
    "df_val_preds = add_preds_to_df(\"T5-UD-NLCD\", t5_ud_nlcd_val_preds, use_validation=True, load_csv=True, replace_existing=True)\n",
    "\n",
    "# Calculate per-row metrics\n",
    "df_val_preds = add_metric_cols_to_preds(\"T5-UD-NLCD_preds\", use_validation=True, replace_existing=True)\n",
    "\n",
    "# Add metrics to dataframe\n",
    "df_val_metrics = add_metrics_to_df(\"T5-UD-NLCD\", t5_ud_nlcd_val_metrics, use_validation=True, load_csv=True, replace_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: Should have thought out a few more moves ahead of their arrogant bully tactics.\n",
      "Target: Should have thought out a few more moves ahead of their bully tactics.\n",
      "T5-UD-NLCD: Should have thought out a few more moves ahead of their tactics.\n",
      "T5-UD-DA: Should have thought a few more moves ahead of their arrogant tactics.\n",
      "'Toxic' words in Source: ['their', 'arrogant', 'bully']\n",
      "\n",
      "Source: Your post is no less trollish than old soul.\n",
      "Target: Your post is not good enough than vintage soul.\n",
      "T5-UD-NLCD: Your post is no less bad\n",
      "T5-UD-DA: Your post is no less bad than old soul.\n",
      "'Toxic' words in Source: ['troll', 'Your', 'post']\n",
      "\n",
      "Source: Lola is a fucking beast ok\n",
      "Target: Lola is a beast ok\n",
      "T5-UD-NLCD: Lola is ok\n",
      "T5-UD-DA: Lola is a beast\n",
      "'Toxic' words in Source: ['fucking', 'ola', 'is']\n",
      "\n",
      "Source: Pence is a despicable toady who fails every time to stand up to trump.\n",
      "Target: Pence cant stand up to Trump\n",
      "T5-UD-NLCD: Pence fails every time to stand up to Trump.\n",
      "T5-UD-DA: Pence is a naive person who fails every time to stand up to Trump.\n",
      "'Toxic' words in Source: ['despicable', 'ence', 'is']\n",
      "\n",
      "Source: And trump has admitted to both, and either or, both are despicable.\n",
      "Target: And trump has admitted to both, and either or, both are bad.\n",
      "T5-UD-NLCD: And trump has admitted to both, and either or, both are despicable.\n",
      "T5-UD-DA: And Trump has admitted to both, and either or both are despicable.\n",
      "'Toxic' words in Source: ['despicable', 'are', 'trump']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter to rows where T5-UD-NLCD has STA == 1 and T5-UD-DA has STA == 0\n",
    "df_STA_T5_UD_NLCD_vs_T5_UD_DA = df_val_preds[(df_val_preds[\"T5-UD-NLCD_STA\"] == 1) & (df_val_preds[\"T5-UD-DA_STA\"] == 0)]\n",
    "\n",
    "# Compare outputs\n",
    "compare_outputs(\n",
    "    df=df_STA_T5_UD_NLCD_vs_T5_UD_DA,\n",
    "    cols_to_compare=['source','target','T5-UD-NLCD_preds', 'T5-UD-DA_preds'],\n",
    "    bad_words_cols=['source'],\n",
    "    random_state=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
