{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "from numba import cuda\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle\n",
    "import optuna\n",
    "from typing import Dict, Union, Optional, Tuple, List, Any\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Default parameters for T5 model fine-tuning\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 64\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "EARLY_STOPPING_PATIENCE = 2\n",
    "NUM_BEAMS = 4\n",
    "\n",
    "# Include BLEURT score in evaluation\n",
    "INCLUDE_BLEURT = True\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "AUG_DATASET_ALL_FILTERS_PATH = \"../data/processed/aug_datasets_all_filters\"\n",
    "AUG_DATASET_NO_TOXICITY_FILTER_PATH = \"../data/processed/aug_datasets_no_toxicity_filter\"\n",
    "AUG_DATASET_NO_SIMILARITY_FILTER_PATH = \"../data/processed/aug_datasets_no_similarity_filter\"\n",
    "AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH = \"../data/processed/aug_datasets_no_acceptability_filter\"\n",
    "\n",
    "# Set path for txt file containing best model checkpoints\n",
    "BEST_MODEL_CHECKPOINT_PATH = \"../models/best_model_checkpoints.txt\"\n",
    "\n",
    "# Set path to save evaluation outputs to\n",
    "EVAL_PREDS_PATH = \"../data/final/model_preds.csv\"\n",
    "EVAL_METRICS_PATH = \"../data/final/model_metrics.csv\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and models\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(DEVICE)\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "aug_datasets_all_filters = DatasetDict.load_from_disk(AUG_DATASET_ALL_FILTERS_PATH)\n",
    "aug_datasets_no_acceptability_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_ACCEPTABILITY_FILTER_PATH)\n",
    "aug_datasets_no_similarity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_SIMILARITY_FILTER_PATH)\n",
    "aug_datasets_no_toxicity_filter = DatasetDict.load_from_disk(AUG_DATASET_NO_TOXICITY_FILTER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the time it takes to run a function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "    return gpu.memoryUsed\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model functions\n",
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    y_pred_delete = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        y_pred_delete.append(text)\n",
    "\n",
    "    return y_pred_delete\n",
    "\n",
    "def bart_detoxifier(text_list):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text using BART\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "    y_pred_bart = pipe_bart(text_list, max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    \n",
    "    return y_pred_bart\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(df, model_name, metrics, save_path=\"../data/processed/model_metrics.csv\"):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a df if the input df is empty\n",
    "    if df is None:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"Overall\"])\n",
    "\n",
    "    # Check if the model name already exists in the dataframe\n",
    "    if model_name in df[\"Model\"].values:\n",
    "        print(f\"Model {model_name} already exists in the dataframe.\")\n",
    "        return df\n",
    "    \n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"Overall\": [metrics[\"Overall\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"  \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=INCLUDE_BLEURT\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA\": sta_pred,\n",
    "        \"FLU\": acc_pred,\n",
    "        \"SEM\": bert_score_f1,\n",
    "        \"Overall\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_preds_to_df(model_name, preds, raw_datasets=raw_datasets, save_path=EVAL_PREDS_PATH, load_csv=True):\n",
    "    \"\"\"\n",
    "    Add model predictions to a pandas dataframe\n",
    "\n",
    "    Args:\n",
    "    - model_name: name of the model\n",
    "    - preds: list of predictions\n",
    "    - save_path: csv file to save the dataframe to\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    \"\"\"\n",
    "\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame({\n",
    "                \"source\": raw_datasets[\"test\"][\"source\"],\n",
    "                \"target\": raw_datasets[\"test\"][\"target\"],\n",
    "            })\n",
    "        \n",
    "    df[f\"{model_name}_preds\"] = preds\n",
    "    df.to_csv(save_path, index=False)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Helper function to add metrics to the dataframe\n",
    "def add_metrics_to_df(model_name, metrics, save_path=EVAL_METRICS_PATH, load_csv=True):\n",
    "    \"\"\"\n",
    "    Add model metrics to a pandas dataframe\n",
    "    \n",
    "    Args:\n",
    "    - df: pandas dataframe to add metrics to\n",
    "    - model_name: name of the model\n",
    "    - metrics: dictionary of evaluation metrics\n",
    "    - save_path: csv file to save the dataframe to\n",
    "    - load_csv: whether to load the existing csv file. If False, a new dataframe will be created.\n",
    "    \n",
    "    Returns:\n",
    "    - updated pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the existing dataframe if it exists\n",
    "    if load_csv:\n",
    "        df = pd.read_csv(save_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=[\"Model\", \"BLEURT\", \"BLEU\", \"STA\", \"FLU\", \"SEM\", \"Overall\"])\n",
    "\n",
    "    # Check if the model name already exists in the dataframe\n",
    "    if model_name in df[\"Model\"].values:\n",
    "        print(f\"Model {model_name} already exists in the dataframe.\")\n",
    "        return df\n",
    "    \n",
    "    # Add the new row to the dataframe\n",
    "    model_metrics_df = pd.DataFrame({\n",
    "        \"Model\": [model_name],\n",
    "        \"BLEURT\": [metrics[\"BLEURT\"]],\n",
    "        \"BLEU\": [metrics[\"BLEU\"]],\n",
    "        \"STA\": [metrics[\"STA\"]],\n",
    "        \"FLU\": [metrics[\"FLU\"]],\n",
    "        \"SEM\": [metrics[\"SEM\"]],\n",
    "        \"Overall\": [metrics[\"Overall\"]]\n",
    "    })\n",
    "\n",
    "    # Save the dataframe to a csv file\n",
    "    df = pd.concat([df, model_metrics_df], ignore_index=True)\n",
    "    df.to_csv(save_path, index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    \"\"\"Adds a prefix to the source sequence in the dataset.\"\"\"\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def create_bidirectional_dataset(datasets, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    if shuffle:\n",
    "        extended_datasets[\"train\"] = extended_datasets[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "        \n",
    "    return extended_datasets\n",
    "\n",
    "def preprocess_dataset(dataset, tokenizer):\n",
    "    \"\"\"Preprocesses a dataset using a tokenizer.\"\"\"\n",
    "    def preprocess_function(examples, tokenizer):\n",
    "        \"\"\"Preprocess function for T5.\"\"\"\n",
    "        model_inputs = tokenizer(\n",
    "            examples[\"source\"],\n",
    "            text_target=examples[\"target\"],\n",
    "            max_length=MAX_INPUT_LENGTH,\n",
    "            truncation=True,\n",
    "        )\n",
    "        return model_inputs\n",
    "\n",
    "    return dataset.map(\n",
    "        preprocess_function,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "        batched=True,\n",
    "        remove_columns=[\"source\", \"target\"],\n",
    "    )\n",
    "\n",
    "def post_process(preds, refs, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5.\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        refs (list): list of reference sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "        decoded_refs (list): list of decoded reference sequences\n",
    "    \"\"\"\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    return decoded_preds, decoded_refs\n",
    "\n",
    "def post_process_preds(preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5 (only for predictions)\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "    \"\"\"\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "\n",
    "    return decoded_preds\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset to use for testing created using create_bidirectional_datasets\n",
    "                                  For example, raw_datasets_bd[\"validation\"] or raw_datasets_bd[\"test\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = [i for i, input_sentence in enumerate(bd_dataset['source']) if input_sentence.startswith(\"to_neutral\")]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        include_bleurt=INCLUDE_BLEURT\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                compute_metrics,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                report_to=\"wandb\",\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Evaluation dataset.\n",
    "        compute_metrics (function): Function to compute metrics. Change this to compute_metrics_bd if using a bi-directional model.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "        report_to (str): Where to report results to. Either \"wandb\" or \"none\".\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    gen_config_path = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(gen_config_path)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=gen_config_path,\n",
    "        fp16=True,\n",
    "        report_to=report_to,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "   \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                compute_metrics,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=NUM_BEAMS,\n",
    "                early_stopping_patience=EARLY_STOPPING_PATIENCE,\n",
    "                report_to=\"wandb\",\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Evaluation dataset.\n",
    "        compute_metrics (function): Function to compute metrics. Change this to compute_metrics_bd if using a bi-directional model.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of the output sequence.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        early_stopping_patience (int): Number of epochs to wait before early stopping.\n",
    "        report_to (str): Where to report results to. Either \"wandb\" or \"none\".\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    gen_config_path = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(gen_config_path)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=gen_config_path,\n",
    "        fp16=True,\n",
    "        report_to=report_to,\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "   \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=partial(compute_metrics, tokenizer=tokenizer),\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=early_stopping_patience)]\n",
    "\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def training_pipeline(model_name, project_name=\"t5-detox\", model_checkpoint=\"t5-small\", use_validation=True, raw_datasets=raw_datasets, bidirectional=False, shuffle=False, do_train=True):\n",
    "    \"\"\"\n",
    "    Pipeline for training a T5 model. Saves the best model checkpoint to a txt file. Can also be used for evaluating a model (use test set instead of validation set).\n",
    "\n",
    "    Args:\n",
    "        model_name (str): Name of the model to name the output directory and wandb run.\n",
    "        project_name (str): Name of the wandb project.\n",
    "        model_checkpoint (str): Model checkpoint to use.\n",
    "        use_validation (bool): Whether to use the validation set or not.\n",
    "        raw_datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        bidirectional (bool): Whether to use a bi-directional model or not.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "        do_train (bool): Whether to train the model or not.\n",
    "\n",
    "    Returns:\n",
    "        trainer (Seq2SeqTrainer): Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Preprocess dataset (add prefixes / make bidirectional)\n",
    "    if bidirectional:\n",
    "        raw_datasets = create_bidirectional_dataset(raw_datasets, shuffle=shuffle)\n",
    "    else:\n",
    "        raw_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "    # Tokenize dataset\n",
    "    tokenized_datasets = preprocess_dataset(raw_datasets, tokenizer_t5_small)\n",
    "\n",
    "    # Define compute_metrics function depending on bidirectional or not\n",
    "    if bidirectional and use_validation:\n",
    "        bd_dataset = raw_datasets[\"validation\"]\n",
    "    elif bidirectional and not use_validation:\n",
    "        bd_dataset = raw_datasets[\"test\"]\n",
    "    else:\n",
    "        bd_dataset = None\n",
    "\n",
    "    compute_metrics_fn = partial(compute_metrics_bd, bd_dataset=bd_dataset, shuffled_data=shuffle) if bd_dataset else compute_metrics\n",
    "\n",
    "    # Setup trainer\n",
    "    trainer = setup_trainer(\n",
    "        output_dir_name=model_name,\n",
    "        model_checkpoint=model_checkpoint,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"validation\"] if use_validation else tokenized_datasets[\"test\"],\n",
    "        compute_metrics=compute_metrics_fn\n",
    "    )\n",
    "\n",
    "    if do_train:\n",
    "        # Initialize wandb\n",
    "        wandb.init(project=project_name, name=model_name)\n",
    "        trainer.train()\n",
    "        wandb.finish()\n",
    "\n",
    "        # Get the best checkpoint path for the model\n",
    "        checkpoint_path = trainer.state.best_model_checkpoint\n",
    "\n",
    "        # Save the checkpoint path for the best model\n",
    "        with open(BEST_MODEL_CHECKPOINT_PATH, \"a\") as file:\n",
    "            file.write(f\"{model_name}: {checkpoint_path}\\n\")\n",
    "\n",
    "    return trainer, tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_preds_test = baseline_detoxifier(raw_datasets[\"test\"][\"source\"])\n",
    "df_test = add_preds_to_df(\"DELETE\", delete_preds_test, load_csv=False)\n",
    "\n",
    "delete_preds_test_metrics = evaluate_metrics(raw_datasets[\"test\"][\"target\"], delete_preds_test)\n",
    "df_eval = add_metrics_to_df(\"DELETE\", delete_preds_test_metrics, load_csv=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline: BART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "bart_preds_test = bart_detoxifier(raw_datasets[\"test\"][\"source\"])\n",
    "df_test = add_preds_to_df(\"BART\", bart_preds_test, load_csv=True)\n",
    "\n",
    "bart_preds_test_metrics = evaluate_metrics(raw_datasets[\"test\"][\"target\"], bart_preds_test)\n",
    "df_eval = add_metrics_to_df(df_eval, \"BART\", bart_preds_test_metrics, load_csv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_checkpoints():\n",
    "    # Get checkpoint values for the best models\n",
    "    with open(BEST_MODEL_CHECKPOINT_PATH, \"r\") as f:\n",
    "        best_model_checkpoints = f.readlines()\n",
    "\n",
    "    # Convert to a dictionary\n",
    "    best_model_checkpoints_dict = {}\n",
    "    for line in best_model_checkpoints:\n",
    "        model_name, checkpoint_path = line.split(\": \")\n",
    "        best_model_checkpoints_dict[model_name] = checkpoint_path.strip()\n",
    "\n",
    "    return best_model_checkpoints_dict\n",
    "\n",
    "model_checkpoints = get_model_checkpoints()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_t5_preds_metrics(model_name,\n",
    "                         model_checkpoint_dict=model_checkpoints,\n",
    "                         raw_datasets=raw_datasets,\n",
    "                         bidirectional=False,\n",
    "                         shuffle=False,\n",
    "                         use_validation=False,\n",
    "                         do_train=False,\n",
    "                         tokenizer=tokenizer_t5_small\n",
    "                         ):\n",
    "    \"\"\"\n",
    "    Returns the predictions and metrics for a T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Setup training pipeline\n",
    "    trainer, trainer_tokenized_ds = training_pipeline(\n",
    "        model_name=\"n/a\",\n",
    "        project_name=\"n/a\",\n",
    "        model_checkpoint=model_checkpoint_dict[model_name],\n",
    "        use_validation=use_validation,\n",
    "        raw_datasets=raw_datasets,\n",
    "        bidirectional=bidirectional,\n",
    "        shuffle=shuffle,\n",
    "        do_train=do_train\n",
    "    )\n",
    "\n",
    "    # Get raw predictions\n",
    "    trainer_preds_raw = trainer.predict(trainer_tokenized_ds[\"test\"])\n",
    "\n",
    "    # Get encoded predictions and metrics\n",
    "    trainer_preds_encoded, trainer_metrics = trainer_preds_raw.predictions, trainer_preds_raw.metrics\n",
    "\n",
    "    # Post-process predictions\n",
    "    if isinstance(trainer_preds_encoded, tuple):\n",
    "        trainer_preds_encoded = trainer_preds_encoded[0]\n",
    "\n",
    "    trainer_preds_decoded = tokenizer.batch_decode(trainer_preds_encoded, skip_special_tokens=True)\n",
    "    trainer_preds_decoded = [pred.strip() for pred in trainer_preds_decoded]\n",
    "\n",
    "    #Return trainer metrics in the same format as evaluate_metrics\n",
    "    trainer_metrics = {\n",
    "        \"BLEU\": trainer_metrics[\"test_BLEU\"],\n",
    "        \"BLEURT\": trainer_metrics[\"test_BLEURT\"],\n",
    "        \"STA\": trainer_metrics[\"test_STA\"],\n",
    "        \"FLU\": trainer_metrics[\"test_FLU\"],\n",
    "        \"SEM\": trainer_metrics[\"test_SEM\"],\n",
    "        \"Overall\": trainer_metrics[\"test_Overall\"]\n",
    "    }\n",
    "        \n",
    "    # Return predictions and metrics\n",
    "    return trainer_preds_decoded, trainer_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ee8b2e7f190b6253.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-8afb5dbff4db4da4.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-f98992b3fb470fc3.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ee8b2e7f190b6253.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-8afb5dbff4db4da4.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-f98992b3fb470fc3.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ee8b2e7f190b6253.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-8afb5dbff4db4da4.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-f98992b3fb470fc3.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ee8b2e7f190b6253.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-8afb5dbff4db4da4.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-f98992b3fb470fc3.arrow\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loop through the model checkpoints and get predictions and metrics for each\n",
    "for model_name, model_path in model_checkpoints.items():\n",
    "    # Get predictions and metrics\n",
    "    preds, metrics = get_t5_preds_metrics(model_name)\n",
    "\n",
    "    # Add predictions to dataframe\n",
    "    df_test = add_preds_to_df(model_name, preds)\n",
    "\n",
    "    # Add metrics to dataframe\n",
    "    df_eval = add_metrics_to_df(model_name, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T5 Model With Negative Lexically Constrained Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentence: My thoughts are that our new guy might be an even bigger dirt bag.\n",
      "Input IDs shape: torch.Size([1, 17])\n",
      "Input IDs: tensor([[    0,  2387,  4312,    32,    14,    84,    92,  2173,   429,    28,\n",
      "            41,   190,  2671, 10667,  3298,     4,     2]])\n",
      "Attention shape after averaging across 3 layers: torch.Size([1, 12, 17, 17])\n",
      "Attention shape after averaging across each head: torch.Size([1, 17, 17])\n",
      "Attention shape after averaging across each row: torch.Size([1, 17])\n",
      "Attention: tensor([[0.0245, 0.0356, 0.0252, 0.0242, 0.0346, 0.0566, 0.0571, 0.0656, 0.0579,\n",
      "         0.0584, 0.0474, 0.0600, 0.0601, 0.0947, 0.0917, 0.1030, 0.1034]],\n",
      "       grad_fn=<MeanBackward1>)\n",
      "Top indices: [16, 15, 13, 14]\n",
      "Filtered input IDs: [2, 4, 10667, 3298]\n",
      "Bad words: ['</s>', '.', ' dirt', ' bag']\n",
      "\n",
      "Sentence: Getting stronger every month yeah i fucking wish.\n",
      "Input IDs shape: torch.Size([1, 11])\n",
      "Input IDs: tensor([[    0, 28750,  3651,   358,   353, 11380,   939, 23523,  2813,     4,\n",
      "             2]])\n",
      "Attention shape after averaging across 3 layers: torch.Size([1, 12, 11, 11])\n",
      "Attention shape after averaging across each head: torch.Size([1, 11, 11])\n",
      "Attention shape after averaging across each row: torch.Size([1, 11])\n",
      "Attention: tensor([[0.0496, 0.0487, 0.0606, 0.0202, 0.0297, 0.0897, 0.0887, 0.2209, 0.1068,\n",
      "         0.1423, 0.1429]], grad_fn=<MeanBackward1>)\n",
      "Top indices: [7, 10, 9, 8]\n",
      "Filtered input IDs: [23523, 2, 4, 2813]\n",
      "Bad words: [' fucking', '</s>', '.', ' wish']\n",
      "\n",
      "Sentence: That guy used to be a real dick.\n",
      "Input IDs shape: torch.Size([1, 11])\n",
      "Input IDs: tensor([[    0,  1711,  2173,   341,     7,    28,    10,   588, 38594,     4,\n",
      "             2]])\n",
      "Attention shape after averaging across 3 layers: torch.Size([1, 12, 11, 11])\n",
      "Attention shape after averaging across each head: torch.Size([1, 11, 11])\n",
      "Attention shape after averaging across each row: torch.Size([1, 11])\n",
      "Attention: tensor([[0.0361, 0.0989, 0.0631, 0.0966, 0.0705, 0.0656, 0.0592, 0.0629, 0.1109,\n",
      "         0.1676, 0.1686]], grad_fn=<MeanBackward1>)\n",
      "Top indices: [10, 9, 8, 1]\n",
      "Filtered input IDs: [2, 4, 38594, 1711]\n",
      "Bad words: ['</s>', '.', ' dick', 'That']\n",
      "\n",
      "Sentence: Why the fuck does everyone hate syria so much?.\n",
      "Input IDs shape: torch.Size([1, 14])\n",
      "Input IDs: tensor([[    0,  7608,     5, 26536,   473,   961,  4157, 13550,  6374,    98,\n",
      "           203,   116,     4,     2]])\n",
      "Attention shape after averaging across 3 layers: torch.Size([1, 12, 14, 14])\n",
      "Attention shape after averaging across each head: torch.Size([1, 14, 14])\n",
      "Attention shape after averaging across each row: torch.Size([1, 14])\n",
      "Attention: tensor([[0.0421, 0.0536, 0.0600, 0.0870, 0.0457, 0.0518, 0.0725, 0.0708, 0.0701,\n",
      "         0.0439, 0.0671, 0.0868, 0.1241, 0.1246]], grad_fn=<MeanBackward1>)\n",
      "Top indices: [13, 12, 3, 11]\n",
      "Filtered input IDs: [2, 4, 26536, 116]\n",
      "Bad words: ['</s>', '.', ' fuck', '?']\n",
      "\n",
      "Sentence: I think that s noble as fuck.\n",
      "Input IDs shape: torch.Size([1, 10])\n",
      "Input IDs: tensor([[    0,   100,   206,    14,   579, 25097,    25, 26536,     4,     2]])\n",
      "Attention shape after averaging across 3 layers: torch.Size([1, 12, 10, 10])\n",
      "Attention shape after averaging across each head: torch.Size([1, 10, 10])\n",
      "Attention shape after averaging across each row: torch.Size([1, 10])\n",
      "Attention: tensor([[0.0409, 0.0665, 0.0722, 0.0929, 0.1076, 0.0415, 0.1145, 0.1295, 0.1669,\n",
      "         0.1674]], grad_fn=<MeanBackward1>)\n",
      "Top indices: [9, 8, 7, 6]\n",
      "Filtered input IDs: [2, 4, 26536, 25]\n",
      "Bad words: ['</s>', '.', ' fuck', ' as']\n"
     ]
    }
   ],
   "source": [
    "for sentence in raw_datasets[\"test\"][\"source\"][:5]:\n",
    "    print()\n",
    "    print(f\"Sentence: {sentence}\")\n",
    "    \n",
    "    # Tokenize sentence\n",
    "    inputs = tokenizer_toxicity(sentence, return_tensors=\"pt\")\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "    print(f\"Input IDs: {input_ids}\")\n",
    "\n",
    "    # Get attention scores\n",
    "    attention = model_toxicity(input_ids, output_attentions=True)['attentions']\n",
    "    \n",
    "    # Get the last 3 layer attention scores\n",
    "    attention = attention[-3:]\n",
    "\n",
    "    # Get the mean attention scores for the last 3 layers\n",
    "    attention = torch.stack(attention).mean(0)\n",
    "    print(f\"Attention shape after averaging across 3 layers: {attention.shape}\")\n",
    "\n",
    "    # Average across each head\n",
    "    attention = attention.mean(1)\n",
    "    print(f\"Attention shape after averaging across each head: {attention.shape}\")\n",
    "\n",
    "    # Sum each row to get the attention score for each token\n",
    "    attention = attention.mean(1)\n",
    "    print(f\"Attention shape after averaging across each row: {attention.shape}\")\n",
    "    print(f\"Attention: {attention}\")\n",
    "\n",
    "    # Get the indices of the top 3 tokens with the highest attention scores\n",
    "    top_indices = attention.topk(4).indices.squeeze().tolist()\n",
    "    print(f\"Top indices: {top_indices}\")\n",
    "\n",
    "    # Filter input_ids to only include the top indices\n",
    "    filtered_input_ids = input_ids[:, top_indices].squeeze().tolist()\n",
    "    print(f\"Filtered input IDs: {filtered_input_ids}\")\n",
    "\n",
    "    # Decode the filtered input IDs, skipping special tokens and outputting as a list\n",
    "    bad_words = []\n",
    "    for input_id in filtered_input_ids:\n",
    "        bad_words.append(tokenizer_toxicity.decode(input_id, skip_special_tokens=True))\n",
    "    \n",
    "    print(f\"Bad words: {bad_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the RoBERTa toxicity classifier on each sentence of the test set to identify attention weights for each word\n",
    "tokenized_inputs = tokenizer_toxicity(raw_datasets[\"test\"][\"source\"], return_tensors=\"pt\", padding=True, truncation=True).to(DEVICE)\n",
    "encoded_outputs = model_toxicity(**tokenized_inputs, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,  2387,  4312,    32,    14,    84,    92,  2173,   429,    28,\n",
       "            41,   190,  2671, 10667,  3298,     4,     2,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1],\n",
       "        [    0, 28750,  3651,   358,   353, 11380,   939, 23523,  2813,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1],\n",
       "        [    0,  1711,  2173,   341,     7,    28,    10,   588, 38594,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1],\n",
       "        [    0,  7608,     5, 26536,   473,   961,  4157, 13550,  6374,    98,\n",
       "           203,   116,     4,     2,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1],\n",
       "        [    0,   100,   206,    14,   579, 25097,    25, 26536,     4,     2,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1,     1,     1]], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[\"input_ids\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My thoughts are that our new guy might be an even bigger dirt bag.',\n",
       " 'Getting stronger every month yeah i fucking wish.',\n",
       " 'That guy used to be a real dick.',\n",
       " 'Why the fuck does everyone hate syria so much?.',\n",
       " 'I think that s noble as fuck.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode first 5 inputs\n",
    "tokenizer_toxicity.batch_decode(tokenized_inputs[\"input_ids\"][:5], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8803, 0.0160, 0.0112, 0.0088, 0.0060, 0.0055, 0.0050, 0.0052, 0.0085,\n",
       "         0.0085, 0.0068, 0.0078, 0.0052, 0.0076, 0.0044, 0.0050, 0.0081, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
       "       device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the attention weights for each word in the sentence\n",
    "attentions = encoded_outputs[\"attentions\"]\n",
    "attentions = torch.cat(attentions, dim=0).view(-1, attentions[0].size(-1))\n",
    "attentions[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0160, 0.0112, 0.0088, 0.0060, 0.0055, 0.0050, 0.0052, 0.0085, 0.0085,\n",
       "         0.0068, 0.0078, 0.0052, 0.0076, 0.0044, 0.0050, 0.0081, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the first token from each attention weight\n",
    "attentions_nostart = attentions[:, 1:]\n",
    "attentions_nostart[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "RAW SENTENCE: My thoughts are that our new guy might be an even bigger dirt bag.\n",
      "TOKENIZED SENTENCE: tensor([ 2387,  4312,    32,    14,    84,    92,  2173,   429,    28,    41,\n",
      "          190,  2671, 10667,  3298,     4,     2,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1], device='cuda:0')\n",
      "ATTENTION WEIGHTS: tensor([0.8803, 0.0160, 0.0112, 0.0088, 0.0060, 0.0055, 0.0050, 0.0052, 0.0085,\n",
      "        0.0085, 0.0068, 0.0078, 0.0052, 0.0076, 0.0044, 0.0050, 0.0081, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n",
      "\n",
      "RAW SENTENCE: Getting stronger every month yeah i fucking wish.\n",
      "TOKENIZED SENTENCE: tensor([28750,  3651,   358,   353, 11380,   939, 23523,  2813,     4,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1], device='cuda:0')\n",
      "ATTENTION WEIGHTS: tensor([7.7698e-01, 9.2398e-02, 4.7191e-02, 2.7379e-02, 7.8028e-03, 4.4896e-03,\n",
      "        4.0587e-03, 7.8468e-03, 1.2314e-02, 6.3864e-03, 3.8618e-03, 2.2447e-03,\n",
      "        3.9564e-04, 2.2155e-03, 8.7681e-04, 1.0465e-03, 2.5155e-03, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00], device='cuda:0',\n",
      "       grad_fn=<SelectBackward0>)\n",
      "\n",
      "RAW SENTENCE: That guy used to be a real dick.\n",
      "TOKENIZED SENTENCE: tensor([ 1711,  2173,   341,     7,    28,    10,   588, 38594,     4,     2,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1], device='cuda:0')\n",
      "ATTENTION WEIGHTS: tensor([0.1397, 0.4354, 0.1660, 0.1048, 0.0198, 0.0173, 0.0170, 0.0190, 0.0288,\n",
      "        0.0135, 0.0231, 0.0049, 0.0021, 0.0039, 0.0018, 0.0007, 0.0019, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "       device='cuda:0', grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# For the first 5 sentences, print raw sentence, tokenized sentence, and attention weights\n",
    "for i in range(3):\n",
    "    print()\n",
    "    print(f\"RAW SENTENCE: {raw_datasets['test']['source'][i]}\")\n",
    "    print(f\"TOKENIZED SENTENCE: {tokenized_inputs['input_ids'][i][1:]}\")\n",
    "    print(f\"ATTENTION WEIGHTS: {attentions[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach A: Set top N tokens by attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.0160, 0.0112, 0.0088, 0.0085, 0.0085],\n",
       "        [0.0924, 0.0472, 0.0274, 0.0123, 0.0078],\n",
       "        [0.4354, 0.1660, 0.1048, 0.0288, 0.0231],\n",
       "        ...,\n",
       "        [0.3704, 0.1812, 0.0534, 0.0470, 0.0442],\n",
       "        [0.3704, 0.1812, 0.0534, 0.0470, 0.0442],\n",
       "        [0.3704, 0.1812, 0.0534, 0.0470, 0.0442]], device='cuda:0',\n",
       "       grad_fn=<TopkBackward0>),\n",
       "indices=tensor([[ 0,  1,  2,  7,  8],\n",
       "        [ 0,  1,  2,  7,  6],\n",
       "        [ 0,  1,  2,  7,  9],\n",
       "        ...,\n",
       "        [14,  7,  2,  6,  9],\n",
       "        [14,  7,  2,  6,  9],\n",
       "        [14,  7,  2,  6,  9]], device='cuda:0'))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the indices of the top 3 words with the highest attention weights, ignoring the first token\n",
    "top_indices = torch.topk(attentions_nostart, 5, dim=1)\n",
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  2,  3,  8,  9],\n",
       "        [ 1,  2,  3,  8,  7],\n",
       "        [ 1,  2,  3,  8, 10],\n",
       "        ...,\n",
       "        [15,  8,  3,  7, 10],\n",
       "        [15,  8,  3,  7, 10],\n",
       "        [15,  8,  3,  7, 10]], device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 1 to each index to account for the first token\n",
    "top_indices = top_indices.indices + 1\n",
    "top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['My', ' thoughts', ' are', ' might', ' be'],\n",
       " ['Getting', ' stronger', ' every', ' wish', ' fucking'],\n",
       " ['That', ' guy', ' used', ' dick', '</s>'],\n",
       " ['Why', ' the', ' fuck', ' does', 'ria'],\n",
       " [' think', 'I', ' that', ' s', ' noble'],\n",
       " [' still', ' wife', ' gives', 'My', ' me'],\n",
       " [' to', ' refreshing', ' see', ' s', 'It'],\n",
       " [' state', ' thinks', 'His', ' of', ' secretary'],\n",
       " [' of', ' its', ' cheap', ' but', ' such'],\n",
       " [' about', ' mean', ' actually', 'is', ' the'],\n",
       " [' big', ' stick', ' a', ' to', ' smack'],\n",
       " ['?', ' suck', '</s>', '<pad>', '<pad>'],\n",
       " ['<pad>', '<pad>', '<pad>', '</s>', '<pad>'],\n",
       " ['ry', ' har', ' and', ' ll', 'oyd'],\n",
       " [' played', 'd', ' i', 'yr', ' with'],\n",
       " [' than', ',', ' open', ' fool', ' it'],\n",
       " ['</s>', 'ria', '<pad>', '.', ' sy'],\n",
       " [' they', 'Well', '<pad>', '.', '<pad>'],\n",
       " [' other', 'In', ',', ' actually', ' justice'],\n",
       " [' the', 'Get', '<pad>', ' do', '<pad>'],\n",
       " ['ont', 'D', '<pad>', ' to', '<pad>'],\n",
       " [' kind', 'What', '.', ' put', ' first'],\n",
       " [' comment', 'My', ' at', 'at', ' be'],\n",
       " ['acy', 'Leg', '<pad>', ' shit', '<pad>'],\n",
       " [' do', 'Why', '<pad>', ' shit', '<pad>'],\n",
       " [' and', 'That', '<pad>', '.', '<pad>'],\n",
       " [' just', 'I', ' some', ' i', ' and'],\n",
       " ['eh', 'Y', '<pad>', 'uk', '</s>'],\n",
       " ['it', 'Sh', '.', ' video', ' and'],\n",
       " [' someone', 'Can', '<pad>', '.', '<pad>'],\n",
       " [' is', 'She', ' one', ' wh', ' split'],\n",
       " [' has', 'This', '<pad>', 'est', '</s>'],\n",
       " [' your', 'Take', '</s>', ' fuck', ' other'],\n",
       " [' it', 'Fuck', '</s>', ' the', ' go'],\n",
       " ['<pad>', '<pad>', 'ghan', '<pad>', ' has'],\n",
       " [' this', ' actually', '</s>', 'Do', ' people'],\n",
       " [' s', ' a', 'California', 'of', ' election'],\n",
       " [' and', ' you', ' to', ' asshole', ' will'],\n",
       " [',', ' shit', ' for', ' no', '<pad>'],\n",
       " [' liable', ' are', ' touch', ' something', ' break'],\n",
       " [' is', ' since', ' it', ' the', ','],\n",
       " [' m', ' t', ' i', ' one', 'Its'],\n",
       " [' i', ' can', ' democr', ' thinking', 'ats'],\n",
       " [' m', ' too', ' that', ',', ' her'],\n",
       " [' some', ' not', ',', ' a', ' no'],\n",
       " [' have', ' you', '<pad>', '<pad>', '.'],\n",
       " ['<pad>', '<pad>', 'ats', ' you', ' how'],\n",
       " [' ill', '<pad>', ' way', '<pad>', ' and'],\n",
       " ['<pad>', ' pants', '<pad>', '<pad>', 'Assad'],\n",
       " ['im', ' d', ' \"', ' i', ' one'],\n",
       " ['ching', ' bit', ' military', 'We', ' yet'],\n",
       " [' mark', ' there', ' teen', ' the', ' a'],\n",
       " [' it', ' s', 'a', ' get', ' cub'],\n",
       " ['<pad>', '<pad>', ' psych', '</s>', ' the'],\n",
       " ['<pad>', '<pad>', ' out', '</s>', ' shit'],\n",
       " ['?', '.', ' some', ' time', ' give'],\n",
       " ['.', '</s>', ' you', '?', ' are'],\n",
       " [' stupid', ' shall', ' its', ' least', ' take'],\n",
       " [' dip', 'shit', ' they', ' fuckin', ' but'],\n",
       " ['ney', ' s', ' two', ' che', ' wanted'],\n",
       " ['</s>', '<pad>', ' seeing', '.', ' of'],\n",
       " ['<pad>', '<pad>', '.', '<pad>', ' up'],\n",
       " [' time', ' to', ' gave', ' came', ' coworkers'],\n",
       " ['<pad>', '<pad>', ' so', '<pad>', ' kicks'],\n",
       " ['<pad>', '<pad>', ' for', '</s>', ' qualified'],\n",
       " ['<pad>', '<pad>', ' a', '<pad>', ' give'],\n",
       " ['<pad>', '<pad>', ' get', '</s>', 'uck'],\n",
       " ['<pad>', '<pad>', ' just', '<pad>', ' was'],\n",
       " ['<pad>', '<pad>', ' its', '<pad>', '<pad>'],\n",
       " [' again', 'Fuck', '<pad>', '</s>', '.'],\n",
       " ['ums', ' these', ' clown', ' actually', ' throw'],\n",
       " [' redd', 'But', ' b', ' too', 'itors'],\n",
       " [' think', ' fuck', ' not', ' police', ' re'],\n",
       " [' blew', 'Oh', ' shit', '<pad>', ' minds'],\n",
       " [' can', ' retarded', '.', ' edit', 'Anyone'],\n",
       " ['But', ' is', ' unwilling', 'raine', ' gas'],\n",
       " ['iar', '</s>', ' fire', '<pad>', '<pad>'],\n",
       " ['He', ' almost', ' half', ' mor', ' to'],\n",
       " [' river', ' claim', ' on', ' col', 'ia'],\n",
       " [' are', '<pad>', '<pad>', 'You', ' bit'],\n",
       " [' could', ' people', ' stop', ' acting', ' he'],\n",
       " [' nothing', 'per', '<pad>', '<pad>', '.'],\n",
       " [' do', ' and', ' to', ' anything', ' say'],\n",
       " [' out', ' comes', '<pad>', '.', ' crap'],\n",
       " [' story', '<pad>', ' the', 'What', '<pad>'],\n",
       " [' what', ' matter', ' does', ' it', ' community'],\n",
       " [' he', ' society', ' of', ' the', ' bitch'],\n",
       " ['<pad>', '<pad>', '</s>', '<pad>', ' the'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', '</s>'],\n",
       " [' tablet', ' enterprise', ' a', ' quality', ' new'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', '</s>'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', ' out'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', '</s>'],\n",
       " ['<pad>', '<pad>', '</s>', '<pad>', ','],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', '?'],\n",
       " ['<pad>', '</s>', ' to', '.', ' this'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', ' outlook'],\n",
       " [' generation', ' your', ' made', ' for', ' they'],\n",
       " ['</s>', '.', ' completely', ' ridiculous', ' some'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', '</s>'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', ' get'],\n",
       " ['</s>', '.', ' nobody', ' else', ' who'],\n",
       " [' people', ' these', 'I', '?', 'ards'],\n",
       " [' seems', ' go', ' there', 'anim', ' the'],\n",
       " [' the', ' about', '<pad>', ' talking', '<pad>'],\n",
       " [' almost', ' in', ' never', '<pad>', ' shit'],\n",
       " [' you', ' get', '</s>', '.', 'They'],\n",
       " ['</s>', ' piece', '!', '<pad>', '<pad>'],\n",
       " [' says', ' g', ' it', '</s>', 'He'],\n",
       " [' add', ' stupid', 'ont', ' vandalism', ' to'],\n",
       " [' they', ' fuck', ' term', ' a', ' effects'],\n",
       " ['.', ' party', ' a', '<pad>', ' crap'],\n",
       " [' keeping', ' these', ' though', ' thing', ' has'],\n",
       " [' shit', ' point', '<pad>', ' was', ' choice'],\n",
       " [' cold', ' hes', ' acted', ' per', ' found'],\n",
       " ['<pad>', '<pad>', ' while', '<pad>', ' drinking'],\n",
       " [' recl', ' others', 'unts', ' will', ' knowing'],\n",
       " ['<pad>', '<pad>', ' fuck', ' flying', '<pad>'],\n",
       " ['<pad>', '<pad>', 'Also', ' known', '<pad>'],\n",
       " ['<pad>', ' that', '.', ' is', '<pad>'],\n",
       " ['<pad>', ' shit', '.', ' mail', '<pad>'],\n",
       " [' mail', '!', ' \"', ' read', ' confirmed'],\n",
       " ['<pad>', ' basically', 'for', ' fuck', '</s>'],\n",
       " ['.', ' the', ' n', ' of', ' recent'],\n",
       " [' dropping', ' us', ' town', ' about', ' here'],\n",
       " ['<pad>', ' he', '</s>', ' real', '<pad>'],\n",
       " ['<pad>', 'aepernick', '<pad>', ' idiot', '<pad>'],\n",
       " [' voyage', ' said', ' the', ' goddamn', 'it'],\n",
       " ['<pad>', ' is', '<pad>', ' doing', '<pad>'],\n",
       " ['<pad>', ' shit', '<pad>', '.', '<pad>'],\n",
       " ['<pad>', ' in', '<pad>', '.', '<pad>'],\n",
       " ['<pad>', ' many', '</s>', ' comments', '<pad>'],\n",
       " ['<pad>', ' thought', '</s>', ' a', '<pad>'],\n",
       " ['<pad>', ' clown', '.', ' seriously', '<pad>'],\n",
       " ['ack', ' stupid', ' electing', ' our', ' wing'],\n",
       " ['<pad>', ' got', '<pad>', '.', '<pad>'],\n",
       " ['<pad>', ' one', '.', '<pad>', '<pad>'],\n",
       " ['<pad>', '</s>', '<pad>', '<pad>', 'ite'],\n",
       " ['ucking', ' to', ' barb', ' of', 'arian'],\n",
       " [' one', ' be', '</s>', '.', ' now'],\n",
       " [' government', ' who', ' this', ' there', ' fuck'],\n",
       " [' wait', ' the', ' storm', ' be', '!'],\n",
       " [' sund', ' i', ' then', ' afternoon', ' a'],\n",
       " [' shit', ' that', ' about', ' forgot', '<pad>'],\n",
       " ['.', ' day', ' to', ' cl', ' bullshit'],\n",
       " [',', ' sw', '\\\\', 'O', ' d'],\n",
       " [' i', ' entertaining', ' i', ' a', ' that'],\n",
       " [' he', ' and', ' makes', ' of', ' reporting'],\n",
       " ['ers', ' her', '.', ' hate', ' of'],\n",
       " ['<pad>', ' is', 'Sm', ' thug', '.'],\n",
       " ['</s>', '.', 'L', ' was', ','],\n",
       " [' from', ' health', ' together', ' the', ' get'],\n",
       " [' and', '<pad>', ' it', '<pad>', '<pad>'],\n",
       " ['<pad>', ',', ' on', ' p', ' republic'],\n",
       " ['<pad>', ' fuck', '?', '</s>', ' it'],\n",
       " ['<pad>', ' the', ' no', ' shit', ' report'],\n",
       " ['</s>', ' are', ' beans', ',', ' your'],\n",
       " ['<pad>', ' end', '<pad>', '<pad>', '!'],\n",
       " ['<pad>', ' are', ' shit', '.', ' pieces'],\n",
       " [' own', ' do', ' world', ' immigrants', ' accept'],\n",
       " ['<pad>', ' seeing', ' is', ' a', ' prime'],\n",
       " ['<pad>', ' shit', '.', '</s>', ' year'],\n",
       " [' still', ' of', 'a', ',', ' v'],\n",
       " ['<pad>', ' the', '<pad>', '<pad>', '</s>'],\n",
       " ['<pad>', ' ar', '</s>', '<pad>', ' are'],\n",
       " [' really', ' in', ' was', ' an', 'ard'],\n",
       " [' by', ' a', ' owned', ' by', ' wearing'],\n",
       " ['<pad>', ' you', ' mom', 'mas', ' with'],\n",
       " [' what', ' voted', ' accepted', ' and', ' r'],\n",
       " ['<pad>', ' things', ' to', ' build', ' has'],\n",
       " ['<pad>', '</s>', '<pad>', ' and', 'Mon'],\n",
       " ['ful', ' idiots', 'Use', ' r', ' )'],\n",
       " ['Making', ' distributing', ' cp', '<pad>', '<pad>'],\n",
       " ['ol', ',', ' yeah', 'L', ','],\n",
       " [' going', ' am', ' lose', ' to', '</s>'],\n",
       " [' cameras', ' light', ' red', ' and', ' the'],\n",
       " [' do', ' t', ' try', ' but', ' can'],\n",
       " [' did', '.', ' who', ' shit', ' this'],\n",
       " ['<pad>', '</s>', '<pad>', ' and', ' spend'],\n",
       " ['<pad>', '<pad>', '?', '<pad>', '<pad>'],\n",
       " ['!', '</s>', ' bitch', '<pad>', '<pad>'],\n",
       " [' and', ' afford', '?', ' houses', '</s>'],\n",
       " [' their', ' living', ' while', ' normal', ' are'],\n",
       " ['<pad>', ' would', '<pad>', ' bully', '</s>'],\n",
       " ['</s>', '<pad>', '<pad>', '.', ' if'],\n",
       " ['<pad>', '<pad>', '<pad>', ' to', ' need'],\n",
       " ['<pad>', '<pad>', '<pad>', ' fucking', '<pad>'],\n",
       " ['grand', ' ag', ' puff', ' always', ' blowing'],\n",
       " ['<pad>', '<pad>', ' think', ' t', ' a'],\n",
       " ['</s>', '.', ' them', ' have', ' shit'],\n",
       " ['.', ' dollars', ' they', ' shit', ' of'],\n",
       " ['<pad>', '<pad>', ' on', ' plastic', ' will'],\n",
       " [' on', ' sides', ' rep', ' truly', ' there'],\n",
       " [' can', ' weed', ' been', ' ever', ' seen'],\n",
       " ['uation', 'eval', ',', ' you', ' americ'],\n",
       " [' with', ' away', ' like', ' things', ' and'],\n",
       " [' fact', ' that', ' an', ' is', ' didnt'],\n",
       " ['.', ' hole', ' to', ' trying', ' is'],\n",
       " [' page', ' a', ' fucking', ' you', ' takes'],\n",
       " ['<pad>', '<pad>', ' is', ' fuck', '</s>'],\n",
       " ['<pad>', '<pad>', ' lazy', ' you', '?'],\n",
       " ['<pad>', '</s>', ' the', ' why', 'co'],\n",
       " [' see', ' or', ' want', ' t', ','],\n",
       " ['s', ' she', ' gave', ' he', ' p'],\n",
       " ['Holy', ' shit', '<pad>', '<pad>', '.'],\n",
       " [' m', ' glad', ' you', 'I', ' guys'],\n",
       " [' shit', ' of', ' post', ' is', 'What'],\n",
       " ['pe', ' of', ' trump', ' a', ' is'],\n",
       " [' shit', ' of', ' bit', ' on', ' a'],\n",
       " [' in', ' real', ' prison', ' him', ' minimum'],\n",
       " [' i', ' guess', ' it', ' would', ' you'],\n",
       " ['<pad>', '<pad>', '</s>', '<pad>', '.'],\n",
       " [' to', ' fix', ' the', ' fucking', ' an'],\n",
       " [' of', ' shit', ' piece', ' because', ' a'],\n",
       " ['ure', ' proceed', ',', ' nothing', ' more'],\n",
       " [' shit', ' about', ' international', ' laws', '.'],\n",
       " ['</s>', '.', '<pad>', '<pad>', ' public'],\n",
       " [' for', ' those', 'a', ' cor', ' requests'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', '.'],\n",
       " [' a', ' were', ' it', ' if', ' self'],\n",
       " [' to', ' have', ' you', ' that', ' hard'],\n",
       " [' seen', ' even', 'It', ' like', ' and'],\n",
       " [' blocked', ' be', 'Th', ' just', ','],\n",
       " ['</s>', '.', 'This', ' has', ' full'],\n",
       " ['<pad>', '<pad>', 'I', ' done', ' this'],\n",
       " ['<pad>', '<pad>', 'Holy', ' that', ' awesome'],\n",
       " [',', 'eme', 'Hi', ' my', ' is'],\n",
       " [' cover', ' would', 'If', ' think', ' town'],\n",
       " [' over', ' and', 'Dest', ' to', ' the'],\n",
       " ['</s>', '.', 'I', ' a', ' dumb'],\n",
       " ['<pad>', '<pad>', 'I', \"'t\", ' a'],\n",
       " [' say', ' to', 'So', ',', ' you'],\n",
       " ['</s>', '?', 'What', ' actual', ' hell'],\n",
       " [' and', ' teenagers', 'An', ' is', ' a'],\n",
       " [' to', ' needs', 'He', ' a', ' bar'],\n",
       " [',', ' years', 'They', ' done', ' shit'],\n",
       " [' with', ' begin', 'The', ' was', ' piece'],\n",
       " ['?', ' fucking', 'H', ' t', ' ever'],\n",
       " [' especially', 'Nobody', ' a', ' alone', ','],\n",
       " ['Get', 'f', '<pad>', '<pad>', '<pad>'],\n",
       " ['m', '<pad>', 'nn', '<pad>', ' the'],\n",
       " ['<pad>', ' do', ' anything', ' we', '<pad>'],\n",
       " [' hasn', ' -', '</s>', ' suck', ' this'],\n",
       " ['No', ' shit', ' already', '<pad>', '<pad>'],\n",
       " [' fuck', ',', 'But', ' so', ' all'],\n",
       " [' ever', 'ucked', 'S', '<pad>', '<pad>'],\n",
       " [' police', ' bitch', ' law', '<pad>', 'ities'],\n",
       " [' t', '<pad>', ' a', '<pad>', ' give'],\n",
       " ['<pad>', '<pad>', ' fuck', 'He', ' that'],\n",
       " [' down', ' was', ' to', ' from', ' going'],\n",
       " ['!', ' written', 'Never', 'ck', ' poorly'],\n",
       " [' this', ' awesome', 'I', ' it', ' and'],\n",
       " [' could', 'I', ' on', ' help', ' thinking'],\n",
       " ['I', ' ju', \"'m\", ' the', ' the'],\n",
       " ['C', ' care', 'ret', ' this', '</s>'],\n",
       " [' it', '?', '<pad>', '</s>', 'face'],\n",
       " [',', ' forget', '.', ' your', ' you'],\n",
       " [' water', ' accident', '<pad>', ' waiting', ' fatal'],\n",
       " [' shit', ' the', '<pad>', ' last', ' as'],\n",
       " [' wait', ' episode', '<pad>', ' about', ' an'],\n",
       " [' the', ' ton', ' its', ' beast', ' thousand'],\n",
       " [' more', ' the', '.', ' course', ' stay'],\n",
       " [' who', ' the', '</s>', ' most', ' privately'],\n",
       " [' of', ' agenda', '</s>', ' instead', ' your'],\n",
       " [' to', ' my', ' all', ' opinion', ','],\n",
       " [' had', ',', '</s>', ' or', ' team'],\n",
       " [' talk', ' reason', '<pad>', '.', ' fucking'],\n",
       " [' find', ' deny', ' piece', ' rights', ' to'],\n",
       " [' that', ' kind', '</s>', ' of', ' that'],\n",
       " [' don', ' they', '.', ' are', ' since'],\n",
       " [' son', ' are', '<pad>', ' you', ' where'],\n",
       " [' are', ' a', '<pad>', ' civil', ' start'],\n",
       " ['<pad>', ' so', '<pad>', '</s>', 'Man'],\n",
       " ['What', '<pad>', '<pad>', '!', ' a'],\n",
       " ['Stop', ' being', ' a', '<pad>', ' are'],\n",
       " [' wrote', ' uttered', ' crap', '.', ' that'],\n",
       " [' a', ' new', 'S', ' get', ','],\n",
       " [' communism', ' who', ' really', ' really', ' reality'],\n",
       " ['.', ' t', 'I', ' shit', ' m'],\n",
       " ['<pad>', '<pad>', '.', ' yourself', ' man'],\n",
       " [' way', ' s', ' that', 'Y', 'pe'],\n",
       " [' north', ' fuck', '?', \"'t\", ' haven'],\n",
       " [' it', 'Don', 'p', ' cunt', 'icky'],\n",
       " [' idea', ' and', ' the', ' solar', ' as'],\n",
       " [' heaven', ' are', ' but', ' enough', ' it'],\n",
       " ['<pad>', ' dumb', 'It', '<pad>', '<pad>'],\n",
       " [' cars', ' on', 'The', \"'\", ' <'],\n",
       " ['<pad>', ' state', '</s>', 'Just', '<pad>'],\n",
       " ['<pad>', ' cunt', ' miss', 'Well', ','],\n",
       " ['</s>', '<pad>', '<pad>', ' you', '<pad>'],\n",
       " ['</s>', '<pad>', '<pad>', ' to', '<pad>'],\n",
       " [' my', '</s>', ' asshole', ' the', '<pad>'],\n",
       " [' fellow', '<pad>', '<pad>', ',', '<pad>'],\n",
       " [' women', ' big', 'sa', ',', ' corp'],\n",
       " [' in', ' stain', ' a', ' them', '.'],\n",
       " [' crap', '<pad>', '<pad>', ' the', '<pad>'],\n",
       " ['<pad>', '<pad>', '<pad>', ' fucking', '<pad>'],\n",
       " ['</s>', '<pad>', '<pad>', ' have', '<pad>'],\n",
       " [' those', ' video', ' in', ' nobody', '.'],\n",
       " ['<pad>', '<pad>', '<pad>', ' to', '<pad>'],\n",
       " [' away', '.', ' ch', ',', '</s>'],\n",
       " ['?', '<pad>', '<pad>', ' is', '<pad>'],\n",
       " [' all', '</s>', ' cohorts', 'umb', '<pad>'],\n",
       " [' fucking', '<pad>', '</s>', ' all', '<pad>'],\n",
       " [' saying', ' do', ' won', \"'s\", ' shit'],\n",
       " [' should', ' the', ' by', ' colonies', ' officials'],\n",
       " ['G', '</s>', ' something', 't', ' fucking'],\n",
       " [' should', 'They', ' have', ' guys', ' are'],\n",
       " [' most', ' all', ' of', 'After', ' and'],\n",
       " [' big', ' a', ' deal', ' shit', ' s'],\n",
       " [' same', ' the', '.', ' route', ' go'],\n",
       " [' logic', '.', ' shit', '<pad>', '<pad>'],\n",
       " [' lying', ' sack', ' a', '<pad>', ' is'],\n",
       " [' does', ' fuck', ' everything', ' the', ' called'],\n",
       " ['</s>', '.', '<pad>', ' about', 'This'],\n",
       " [' it', ' since', ' up', \"'s\", 'I'],\n",
       " ['<pad>', '</s>', '.', ' other', '<pad>'],\n",
       " [' to', ' dec', ' love', ' yeah', ' world'],\n",
       " [' do', ' they', ' it', ' think', ' why'],\n",
       " ['</s>', '!.', '<pad>', 'b', 'aker'],\n",
       " [' starve', ' to', ' than', ' rather', ' cheap'],\n",
       " [' at', ' work', ' work', ' fucking', ' for'],\n",
       " ['</s>', ' with', '.', ' brilliance', ' them'],\n",
       " [' congr', 'D', 'ats', ',', ' week'],\n",
       " ['<pad>', 'Look', '<pad>', '<pad>', '</s>'],\n",
       " [' wake', 'H', ' up', ' to', ' need'],\n",
       " ['<pad>', 'Last', '<pad>', '<pad>', '</s>'],\n",
       " ['.', 'At', '</s>', ' article', ' fucking'],\n",
       " [' after', 'People', ' taking', ' hands', ' damn'],\n",
       " [' bribery', 'How', ' being', ' with', ','],\n",
       " [' like', 'I', ' this', ' shit', ' for'],\n",
       " [' vote', 'All', ' for', ' woman', ' any'],\n",
       " ['x', 'Not', 'ist', ' mar', ' hypocritical'],\n",
       " ['.', 'Jesus', '</s>', ' up', ' fucked'],\n",
       " [' and', 'I', ' no', ' ass', ' my'],\n",
       " ['.', 'Now', '</s>', ' shit', ' do'],\n",
       " [' when', 'Wow', ' they', ' thinking', ' they'],\n",
       " ['<pad>', 'He', '<pad>', '<pad>', '</s>'],\n",
       " ['<pad>', 'Par', '<pad>', '</s>', '.'],\n",
       " [' bitch', 'We', ',', ' this', ' in'],\n",
       " [' how', ' the', ' billions', ',', ' money'],\n",
       " [' that', ' correct', ' their', ' people', ' moments'],\n",
       " [' and', ' the', ' off', ' voters', ' office'],\n",
       " ['Instead', ' of', ' old', ' trying', ' ye'],\n",
       " [\"'s\", ' these', '<pad>', ' this', ' with'],\n",
       " ['ized', ' some', ' hierarch', ' saints', 'Quite'],\n",
       " [' this', ' still', ' value', 'So', ' but'],\n",
       " [' the', 'est', '<pad>', '</s>', '<pad>'],\n",
       " ['gt', ' one', ' days', '&', ' the'],\n",
       " ['<pad>', '<pad>', '.', '<pad>', '<pad>'],\n",
       " [' most', ' \"', ' \"', ' are', ' ten'],\n",
       " ['<pad>', '<pad>', '.', ' early', '<pad>'],\n",
       " [' shit', '<pad>', '</s>', ' nasty', '<pad>'],\n",
       " [' that', ' garbage', ' is', ' on', '<pad>'],\n",
       " [' they', 'ars', ' clueless', ' li', 'Are'],\n",
       " ['<pad>', ' i', ' hate', '<pad>', '<pad>'],\n",
       " ['<pad>', '<pad>', ' leave', '<pad>', '<pad>'],\n",
       " ['<pad>', ' himself', '<pad>', '<pad>', '*'],\n",
       " ['<pad>', ' awful', '<pad>', '<pad>', 'Price'],\n",
       " ['<pad>', ' -', '<pad>', '<pad>', 'Removed'],\n",
       " ['<pad>', ' happened', '<pad>', '<pad>', 'It'],\n",
       " ['.', ' a', '<pad>', '<pad>', 'I'],\n",
       " [' o', ',', '<pad>', '<pad>', 'Strong'],\n",
       " [' and', ' but', ' i', ' very', 'If'],\n",
       " ['.', ' hours', '<pad>', '<pad>', 'The'],\n",
       " ['</s>', ' that', '<pad>', '<pad>', 'How'],\n",
       " ['ons', ' what', '</s>', '<pad>', 'Americans'],\n",
       " ['ucks', 'bc', '</s>', '<pad>', 'They'],\n",
       " ['<pad>', '.', '<pad>', '<pad>', 'I'],\n",
       " ['</s>', ' pathetic', '<pad>', '<pad>', 'As'],\n",
       " [' down', ' try', '.', '<pad>', 'Never'],\n",
       " [' this', ' something', ',', ' can', 'I'],\n",
       " [' think', ' crap', ' our', 'ious', 'And'],\n",
       " [' sure', ' practice', ' never', ' same', 'Getting'],\n",
       " [' cant', '</s>', '.', 'I', ' this'],\n",
       " ['W', ' the', 'tf', ' catch', ' fuck'],\n",
       " ['No', ' shit', ' or', ',', ' bo'],\n",
       " [' fucking', ' way', ' is', ' number', 'No'],\n",
       " [' needs', ' to', 'per', '<pad>', ' get'],\n",
       " [' the', ' country', ' why', '<pad>', '<pad>'],\n",
       " [' here', ',', ' resident', ' this', '<pad>'],\n",
       " [' said', ' ill', ' i', ' be', '</s>'],\n",
       " ['.', '</s>', 'ome', '<pad>', '<pad>'],\n",
       " [' fucked', ' up', ' totally', '.', ' just'],\n",
       " [' t', ' vote', ' don', ' at', 'Sit'],\n",
       " [' have', ' loads', ' they', ' of', 'Yeah'],\n",
       " [' t', ' doesn', ' want', ' sick', ' to'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', ' that'],\n",
       " [' taxes', ' your', ' oil', ' all', ' representatives'],\n",
       " ['<pad>', '<pad>', '<pad>', ' a', '<pad>'],\n",
       " ['</s>', '<pad>', '.', ' shit', ' grand'],\n",
       " ['<pad>', '</s>', '<pad>', '!', 'Go'],\n",
       " [' just', ',', ' u', 'raine', 'It'],\n",
       " ['control', ' self', '.', ' lacks', 'H'],\n",
       " [' globally', ' compete', '.', ' to', 'Sh'],\n",
       " [' being', ' are', ' a', ' you', 'No'],\n",
       " ['ist', 'rolog', '!.', ' vi', 'Not'],\n",
       " [' understand', ' not', '.', ' does', 'Sp'],\n",
       " ['</s>', '!', '<pad>', ' lawyer', 'Get'],\n",
       " ['<pad>', '<pad>', '<pad>', '</s>', 'This'],\n",
       " ['.', ' too', '</s>', 'nw', 'Same'],\n",
       " [' you', ' when', ' can', ' economy', 'How'],\n",
       " ['<pad>', '<pad>', '<pad>', '<pad>', 'Read'],\n",
       " [' in', ' caught', ' today', ' get', 'They'],\n",
       " ['?', ' questions', '</s>', ' stupid', 'Do'],\n",
       " [' credit', ' guy', ' for', ' the', 'Why'],\n",
       " [' was', ' page', ' not', ' the', 'Call'],\n",
       " [' as', ' crazy', ' fuck', ' their', 'Eh'],\n",
       " ['P', ' all', ' own', ' those', ' s'],\n",
       " [' quadru', 'Yeah', ' citizenship', ',', ' americ'],\n",
       " ['E', 'li', '5', ' a', ' should'],\n",
       " ['The', ' fact', ' we', ' are', ' doing'],\n",
       " [' k', ' fucked', 'Get', 'idd', 'o'],\n",
       " [' s', 'Org', ' wik', ' there', 'ipedia'],\n",
       " [' hell', ' you', ' do', 'Who', ' feminists'],\n",
       " [' remembers', ' what', ' happened', ' somebody', ','],\n",
       " [' number', 'Red', ' from', ' down', ' to'],\n",
       " [' they', ' give', ' a', ' shit', ' should'],\n",
       " [' air', ' unless', ' off', ' shit', ' you'],\n",
       " [' any', ' insane', ' stupid', ' are', ' and'],\n",
       " ['<pad>', '?', ' fucking', ' you', '<pad>'],\n",
       " [' attorney', ' get', ' hope', ' i', ' great'],\n",
       " ['</s>', ' realize', ' ignorant', ' too', '.'],\n",
       " [' place', ' in', ' doing', ' for', ' first'],\n",
       " [' saying', 'ents', ' my', ' put', ' too'],\n",
       " [' to', ' a', ' to', ' option', 'id'],\n",
       " ['<pad>', '.', ' fucking', ' great', '<pad>'],\n",
       " ['<pad>', '.', ' sick', ' a', '<pad>'],\n",
       " ['<pad>', '.', ' fucking', ' s', '<pad>'],\n",
       " [' up', ' never', ' rabb', 'i', ' fucked'],\n",
       " ['</s>', ' ass', ' for', ' time', '.'],\n",
       " ['q', ' with', ' fucking', ' the', 'b'],\n",
       " [' times', ' to', ' ball', ' a', ' ten'],\n",
       " [' be', ' is', ' the', ' who', ' to'],\n",
       " [' i', ' a', ' to', ' trying', ' but'],\n",
       " ['<pad>', '</s>', ' zone', ' twilight', '<pad>'],\n",
       " [' that', ' gonna', 'ans', ' americ', ' at'],\n",
       " [' on', ' jerk', ',', ' off', ' get'],\n",
       " ['.', ' i', ' fucked', ' is', ' this'],\n",
       " [' aside', ' are', ' number', ' least', ' leave'],\n",
       " ['<pad>', ' it', ' might', ' i', '<pad>'],\n",
       " [' people', ' shit', ' killed', \"'s\", ' of'],\n",
       " ['</s>', '<pad>', ' is', ' suggest', ' stupid'],\n",
       " [' letter', 'oz', 'un', 's', ' the'],\n",
       " [' find', ' that', ' had', ' for', ' to'],\n",
       " [' the', ' why', 'And', ' tell', ' did'],\n",
       " [' $', 'IG', 'For', ' gives', 'D'],\n",
       " ['R', ' troll', ' is', ' a', ' john'],\n",
       " [' hell', ' from', ' me', ' and', ' stay'],\n",
       " [' s', ' great', ' idea', 'It', ' actually'],\n",
       " [' known', ' no', ' hypocrisy', 'His', ' bounds'],\n",
       " [' thousands', ' later', 'Maybe', ' so', ' can'],\n",
       " [' 60', ' why', 'This', ' the', ' in'],\n",
       " ['ucking', '.', ' person', ' of', '</s>'],\n",
       " [',', ' shows', 'v', 'c', \"'\"],\n",
       " [' the', '?', ' about', ' is', '</s>'],\n",
       " [' why', ' it', ',', ' is', ' does'],\n",
       " [' some', 'uated', ' act', ' them', ' by'],\n",
       " ['it', '.', ' upload', ' got', '</s>'],\n",
       " [' should', ' then', ' and', ' those', ' sabotage'],\n",
       " ['ucking', ' to', ' had', 'is', ' go'],\n",
       " [' jer', ' around', ' been', ' have', '.'],\n",
       " [' a', ' can', ' somebody', ' guess', ' miss'],\n",
       " ['ging', ' ve', ' i', ' the', ' seen'],\n",
       " [' have', ' many', ' so', ' involved', ' fucked'],\n",
       " [' for', ' that', ',', ' ya', ' stock'],\n",
       " [' isn', ' blowing', ' on', ' about', ' shit'],\n",
       " [' you', ' to', ' going', ' i', ' sit'],\n",
       " [' you', ' what', ' do', ' i', ' you'],\n",
       " [' could', ' would', ' people', ' how', ' shit'],\n",
       " [' shit', ' k', ' north', ' what', 'orea'],\n",
       " [' a', ' who', ',', ' fucking', \"'ll\"],\n",
       " [' s', ' till', ' say', ' they', ' their'],\n",
       " ['IG', ' that', ' suggesting', ' are', ' should'],\n",
       " [' i', ' you', ' did', ' what', ' move'],\n",
       " ['oke', ' you', ',', ' on', ' intoler'],\n",
       " [' big', ' wish', ' shut', ' would', ' about'],\n",
       " [' cover', ' you', ' town', ' tex', ' in'],\n",
       " [' female', '</s>', ' creature', '.', 'I'],\n",
       " [' of', '?', ' this', ' out', ' are'],\n",
       " [' got', ' single', ' that', ' issue', ' s'],\n",
       " [' for', ' pal', ' t', 'estine', ' done'],\n",
       " [' thinks', ' that', ',', ' fuck', '?'],\n",
       " [' pm', ',', ' love', ' i', ' d'],\n",
       " [' shit', ' of', ' christ', ' cro', 'Jesus'],\n",
       " [' where', ' give', ' a', ' t', ' are'],\n",
       " ['</s>', ' fucking', 'Planet', ' is', '.'],\n",
       " [',', ' keeps', ' but', 'His', ' functioning'],\n",
       " ['.', '<pad>', '</s>', 'They', ' up'],\n",
       " [' ignorant', 'ified', ' and', 'This', ' most'],\n",
       " [' because', ' need', ' what', 'G', ' sad'],\n",
       " ['inking', ' up', ' never', 'We', ' st'],\n",
       " [' a', ' sore', ' hateful', 'This', ' of'],\n",
       " [' the', ' he', ' constitution', 'F', ' over'],\n",
       " [' for', '.', ' fuck', 'Org', ' book'],\n",
       " [' our', '.', ' shitty', 'They', ' rejecting'],\n",
       " [',', ' s', ' but', 'The', ' bitch'],\n",
       " [' getting', '?', ' to', 'How', ' they'],\n",
       " [' retarded', ' become', ' as', 'They', ' as'],\n",
       " [' the', ' to', ' spending', 'L', ' but'],\n",
       " [' an', ',', ' oil', 'As', ' and'],\n",
       " [' dumb', ' it', ' enough', 'Hopefully', ' not'],\n",
       " [' shit', ' hear', ' themselves', 'Fox', ' to'],\n",
       " [' stupid', '?', ' plan', 'What', ' really'],\n",
       " [' mean', '.', ' cor', 'What', ' you'],\n",
       " ['.', '<pad>', '</s>', 'He', ' stupid'],\n",
       " [' of', '.', ' wearing', 'What', ' point'],\n",
       " [' do', ' own', ' it', 'Sh', ' even'],\n",
       " [' fuck', '<pad>', '!', 'Why', ' a'],\n",
       " [' fucking', '<pad>', ' believable', 'D', ' un'],\n",
       " [' this', ' story', ' sad', ' they', ' really'],\n",
       " ['.', ' just', ' joking', ',', ' to'],\n",
       " ['Yeah', ' t', ' they', '<pad>', ' give'],\n",
       " ['The', ' object', ' peoples', ' waste', ' decon'],\n",
       " [' tourists', ' silly', ' they', ' knew', ' were'],\n",
       " [' supposed', ' we', 'Sh', 'it', ' to'],\n",
       " [' racist', '.', 'ats', ' an', '</s>'],\n",
       " [' of', 'D', ',', ' change', ' my'],\n",
       " [' tit', ' laser', ' the', ' accidentally', ' read'],\n",
       " ['</s>', ' blow', '.', ' war', ' up'],\n",
       " [' zeal', 'ral', ' respecting', ' or', ' self'],\n",
       " [' to', ' you', ' jail', ' fucked', ' thought'],\n",
       " ['<pad>', '.', '<pad>', 'ucks', ' two'],\n",
       " [' straight', ' shit', ' up', ' as', ' be'],\n",
       " [' on', ' call', ' crap', ' emergency', ' idiots'],\n",
       " [',', ' an', ' but', ' being', ' dunno'],\n",
       " ['</s>', ' garbage', '<pad>', ' total', ' the'],\n",
       " [' i', ',', ' don', ' myself', ' aust'],\n",
       " [' *', ' *', ' ago', ' shit', ' have'],\n",
       " [' a', ' that', ' bolt', ' fuck', ' do'],\n",
       " [' the', ',', ' people', ' government', ' rich'],\n",
       " [' producer', ' consider', ' as', ' even', ' fuck'],\n",
       " [' so', ' grap', ' fucked', ' that', ' the'],\n",
       " [' and', ' sh', ' gigg', ' for', ' t'],\n",
       " [',', ' hypocrisy', ' g', ' useless', ' in'],\n",
       " [' you', ' you', 're', ' now', ' you'],\n",
       " [' just', ',', ' whim', ' crying', ' of'],\n",
       " ['<pad>', '?', '<pad>', ' shit', \"'t\"],\n",
       " ['</s>', ' you', '<pad>', ' with', ' fuck'],\n",
       " [' jail', ' rot', '.', ' should', ' of'],\n",
       " [' step', ' your', ' right', ' get', ' ya'],\n",
       " ['.', ' you', '</s>', ' anything', ' not'],\n",
       " [' fuck', ' allowed', ' up', ' were', ' to'],\n",
       " [',', ' see', ' she', ' i', ' of'],\n",
       " ['</s>', ' off', ' n', ' fuck', ' to'],\n",
       " [' people', 'ons', ' like', ' need', ' tr'],\n",
       " [' gave', 'G', ' shit', 't', 'ussia'],\n",
       " [' for', ' generations', ' happening', ' shit', '</s>'],\n",
       " [' with', ' fuck', '</s>', ' wrong', ' guys'],\n",
       " [' on', ' diversity', ' is', ' fuck', ' the'],\n",
       " [' he', ' ass', 'Did', ' bust', ' to'],\n",
       " [' the', ' it', ' tell', ' couldn', ' what'],\n",
       " [' fuck', ' no', ' say', ',', ' they'],\n",
       " [' brains', ' sell', ' total', ' is', ' guy'],\n",
       " [' r', ' invade', ' and', ' could', ','],\n",
       " [' it', ' season', ' finished', 'Holy', ' is'],\n",
       " [' it', ' problems', ' of', 'Number', ' comes'],\n",
       " [' from', ' this', ' buy', 'Only', ' the'],\n",
       " ['<pad>', '.', ' you', 'What', '<pad>'],\n",
       " [' out', ' fuck', 'ockey', 'Honestly', ' from'],\n",
       " [' you', ' how', \"'t\", '&', ' are'],\n",
       " [' good', ' on', ' sh', 'We', ' name'],\n",
       " [' fucking', ' by', ' bought', 'They', ' k'],\n",
       " [' sh', ' be', ' olds', 'D', 'its'],\n",
       " ['</s>', ' too', ' fucked', 'O', '<pad>'],\n",
       " [' brain', ' same', ' are', 'The', 'less'],\n",
       " [' and', ' multiple', ' fuck', 'Ap', ' called'],\n",
       " ['<pad>', '</s>', ' shit', 'And', '<pad>'],\n",
       " [' of', ' a', ' taxpayers', 'His', ' idiot'],\n",
       " ['.', ' it', \"'ll\", 'Fuck', '</s>'],\n",
       " ['</s>', ' again', ' is', 'Amy', '<pad>'],\n",
       " [' or', ' in', ' to', 'Use', ' i'],\n",
       " [' people', ' reddit', ' sick', 'I', ' with'],\n",
       " ['</s>', ' court', ' smiling', 'That', '<pad>'],\n",
       " [' he', ' happened', ' ever', 'None', ' were'],\n",
       " [' you', 'ass', ' a', 'If', ' will'],\n",
       " ['<pad>', '.', ' terrible', 'Probably', '<pad>'],\n",
       " [' to', ' dumb', ' a', 'Now', ' say'],\n",
       " [' is', ' military', ' can', \"'s\", 'The'],\n",
       " [' -', 'Fuck', ' off', '<pad>', '<pad>'],\n",
       " [' re', '.', 'Because', ' up', ' you'],\n",
       " [' he', ' get', ' should', 'Sure', ' years'],\n",
       " [' without', ' in', ' health', ' insurance', ' that'],\n",
       " ['a', ' give', ' t', 'Yet', ' fif'],\n",
       " [' the', ' p', 'The', ' can', ' is'],\n",
       " [' bitch', ' who', ' for', 'aned', ' mo'],\n",
       " [' those', ' people', ' of', ' who', ' for'],\n",
       " ['</s>', '.', ' out', ' of', 'iot'],\n",
       " [' my', ' social', ' so', ' there', ','],\n",
       " ['<pad>', '<pad>', '.', '<pad>', '</s>'],\n",
       " ['<pad>', '</s>', ' invade', '.', ' shit'],\n",
       " [' him', ' stab', ' im', ' to', ' hoping'],\n",
       " [' you', ' words', ' cares', ' stupid', ' what'],\n",
       " [' the', ',', ' thousand', ' times', ' goddamn'],\n",
       " [' to', ' way', ' bitch', ' the', ' all'],\n",
       " [' clown', ' a', ' you', ' making', ' are'],\n",
       " ['</s>', '.', ' punk', ' us', 'ing'],\n",
       " [' can', ' what', ' give', ' fuck', ' a'],\n",
       " ['<pad>', '<pad>', ' fingers', '</s>', '.'],\n",
       " ['<pad>', '</s>', ' id', '.', 'iotic'],\n",
       " [' such', ' is', 'hod', 'ovsky', 'ork'],\n",
       " [' then', ' fuck', ' know', ' called', ' someone'],\n",
       " [' shit', ' a', ' i', ' give', ' dont'],\n",
       " ['its', ' sh', 'u', ' for', ' tactic'],\n",
       " ['<pad>', '</s>', ' cunt', '.', ' en'],\n",
       " ['<pad>', '</s>', ' fuck', '.', ' frog'],\n",
       " ['?', ' time', ' happen', ' the', ' all'],\n",
       " [' death', ' national', ' we', ' a', ' need'],\n",
       " [' r', ',', ' in', 'as', ' tex'],\n",
       " [' be', ' would', ' fucking', ' \"', ' now'],\n",
       " [' all', ' realize', ' and', ' to', ' starting'],\n",
       " ['.', ' me', ' shit', ' of', ' out'],\n",
       " ['Care', ' the', ' go', ' just', ' described'],\n",
       " [',', ' shit', ' cop', '?', 'Holy'],\n",
       " [' sending', 'And', ' c', 'unts', ' home'],\n",
       " [' hauled', ' my', ' ass', ' i', ' they'],\n",
       " [' are', ' right', ' the', ' and', ' democr'],\n",
       " [' you', ' until', ' know', ' what', ' the'],\n",
       " ['ard', 'ieu', ' dep', ' to', ' will'],\n",
       " [' of', ' shit', ' bag', ' worthless', ' the'],\n",
       " [' s', ' view', ' what', ' trump', ' who'],\n",
       " [' environmentalists', ' the', ' think', 'This', ' people'],\n",
       " [' decisions', ' making', ' is', ' the', ' fucking'],\n",
       " [' weren', ' t', ' if', ' there', ' love'],\n",
       " [' store', ' or', ' of', ' name', ' give'],\n",
       " [' of', ' the', ' fucking', ' first', ' clear'],\n",
       " [' believe', ' people', ' can', ' i', ' fucking'],\n",
       " [' make', ' whatever', ' going', ' sure', ' missing'],\n",
       " [' call', ' people', ' teenager', ' your', ' it'],\n",
       " ['s', '.', ' ass', ' those', ' i'],\n",
       " ['<pad>', '<pad>', '.', ' sentence', ' the'],\n",
       " [' they', ' let', ' book', ' my', ' fucking'],\n",
       " [' to', ' me', ' mean', ' saying', ' why'],\n",
       " ['<pad>', '<pad>', '</s>', '!.', ' was'],\n",
       " ['<pad>', '<pad>', '</s>', '!.', ' t'],\n",
       " [' bunch', ' of', ' gather', ' you', ' what'],\n",
       " ['.', '</s>', ' level', ' orange', ' some'],\n",
       " [' storm', '.', ' the', ' survive', ' smart'],\n",
       " ['z', '!', ' bit', ' time', ' last'],\n",
       " [' has', ' access', ' because', ' is', ' respect'],\n",
       " [' making', ' up', ' names', ' him', ' him'],\n",
       " [' retarded', '.', ' is', ' party', ' stand'],\n",
       " [' should', ' be', ' med', ' pain', ' ridiculous'],\n",
       " [',', ' and', ' few', ' every', ' there'],\n",
       " [' show', ' the', 'es', ' vigilant', ' like'],\n",
       " [' with', ' anything', ' to', ' nothing', ' as'],\n",
       " [' guy', ' for', ' $', 'Some', 'bs'],\n",
       " ['G', ' has', ' eyes', ' g', 'o'],\n",
       " [' thought', ' do', ' just', ' to', ' world'],\n",
       " [' arrogant', ' treats', ' plus', ' up', 'He'],\n",
       " [' confused', ' shit', ' bull', 'Not', ' firm'],\n",
       " [' be', 'Perhaps', ' t', ' if', 'an'],\n",
       " [' than', ' just', ' i', ' is', 'That'],\n",
       " [' beck', ' gl', 'L', ' fuck', ' a'],\n",
       " [',', ' like', 'It', ' much', 'assic'],\n",
       " ['From', ' was', ' personal', ' experience', ' away'],\n",
       " [' and', ' later', 'D', 'IT', ' they'],\n",
       " [' fool', ' one', ' washed', ' for', 'No'],\n",
       " [' as', ' ve', ' corrupt', ',', 'Could'],\n",
       " [' which', ' most', 'ies', ' fire', 'Some'],\n",
       " [' unless', ' your', ' articles', ' off', 'Keep'],\n",
       " ['</s>', ' shit', '???', ' this', 'Holy'],\n",
       " [' all', 'ight', ' up', ' married', 'Stra'],\n",
       " ['<pad>', ' is', '<pad>', ' fucking', 'Everybody'],\n",
       " [' images', ' the', ' free', ' you', 'What'],\n",
       " [' and', ' as', ' funding', ' as', 'Almost'],\n",
       " ['.', ' the', ' edit', ' sorry', 'By'],\n",
       " ['</s>', \"'t\", ' by', ' shit', 'Don'],\n",
       " ['<pad>', ' really', '<pad>', ' not', 'I'],\n",
       " [' doing', ' fucking', ' was', ' them', 'She'],\n",
       " [' recognize', ' people', ' to', ' everyone', 'Do']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_words = []\n",
    "\n",
    "for ids, top_indices in zip(tokenized_inputs[\"input_ids\"], top_indices):\n",
    "    top_k_words.append([tokenizer_toxicity.decode(ids[idx]) for idx in top_indices])\n",
    "\n",
    "top_k_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 8, 9], device='cuda:0')\n",
      "tensor([1, 2, 3, 8, 7], device='cuda:0')\n",
      "tensor([ 1,  2,  3,  8, 10], device='cuda:0')\n",
      "tensor([1, 2, 3, 4, 8], device='cuda:0')\n",
      "tensor([2, 1, 3, 4, 5], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def get_top_k_words_per_sentence(input_ids, attentions, tokenizer, k=3):\n",
    "    top_k_words = []\n",
    "\n",
    "    # Iterate over each sentence and its corresponding attention scores\n",
    "    for ids, attention in zip(input_ids, attentions):\n",
    "        # Skip special tokens\n",
    "        token_attention_pairs = [(tok, att) for tok, att in zip(ids, attention) if tok not in [tokenizer.pad_token_id, tokenizer.cls_token_id, tokenizer.sep_token_id]]\n",
    "\n",
    "        # Sort by attention score in descending order and select top k\n",
    "        top_tokens = sorted(token_attention_pairs, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "        # Decode tokens to words\n",
    "        top_words = [tokenizer.decode([tok]) for tok, _ in top_tokens]\n",
    "        top_k_words.append(top_words)\n",
    "\n",
    "    return top_k_words\n",
    "\n",
    "# Example usage\n",
    "# Assuming you have your input_ids, attentions from the model, and tokenizer\n",
    "top_k_words = get_top_k_words_per_sentence(input_ids, attentions, tokenizer, k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each line of tokenized_inputs[\"input_ids\"], get the top 3 words with the highest attention weights\n",
    "top_words = []\n",
    "for i in range(len(tokenized_inputs[\"input_ids\"])):\n",
    "    top_words.append(tokenizer_toxicity.convert_ids_to_tokens(top_indices[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
