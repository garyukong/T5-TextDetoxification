{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 14:37:18.146148: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-10-31 14:37:18.146201: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-10-31 14:37:18.146231: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "import pprint\n",
    "from numba import cuda\n",
    "import optuna\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgarykong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WAND_NOTEBOOK_NAME\"] = \"w266_final_project_models\"\n",
    "os.environ[\"WANDB_DIR\"] = \"../models/wandb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"w266_final_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Parameters for classification\n",
    "BATCH_SIZE_EVAL = 32\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and get lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 10733\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 1193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source length for BART: 38\n",
      "Maximum target length for BART: 36\n",
      "Maximum source length for T5: 47\n",
      "Maximum target length for T5: 43\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/oAAAHWCAYAAADHF/LFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABe2klEQVR4nO3de1wWdf7//ycHAU+AkHCBKZK5CWZqakqZlaJoZHloy5aU0rQDuKn7UXNTMyxNK3U10mpNbdPt8N20tEIRLbdENErXUx5Kw1UvMBVISxCY3x/9mPXyUB4uGBge99ttbsvM+33NvN6zu7548Z55Xx6GYRgCAAAAAAC24Gl1AAAAAAAAwH0o9AEAAAAAsBEKfQAAAAAAbIRCHwAAAAAAG6HQBwAAAADARij0AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBQNKkSZPk4eGhH3/80epQAACAG3l4eCg5OdnqMIBKRaEPVJCtW7fq3nvvVUREhPz8/NSoUSN1795dc+bMsTq0CrN//355eHjopZdesjqUC5oyZYqWLVtmdRgAAJvw8PC4qO2zzz6zOlQX69ev16RJk5Sfn39R/R966CHVq1evYoO6Apc6HsDuvK0OALCj9evX64477lCTJk00dOhQORwOHThwQBs2bNDf/vY3DR8+3OoQa6wpU6bo3nvvVZ8+fawOBQBgA//4xz9c9t966y2lp6efczwqKqoyw/pd69ev17PPPquHHnpIgYGBVodzxew2HuBKUegDFeD5559XQECANm3adE6yycvLq/R4Tp48qbp161b6dQEAsLsHH3zQZX/Dhg1KT08/5/jlMAxDp06dUu3ata/4XABqFh7dByrAd999p5YtW573L8ohISEu+yUlJZo8ebKaNWsmX19fNW3aVH/9619VVFTk0s/Dw0OTJk0653xNmzbVQw89ZO4vXLhQHh4e+vzzz/XEE08oJCREV199tdn+6aef6rbbblP9+vXl7++vDh06aMmSJS7nzMrKUs+ePRUQEKA6derotttu05dffnnpN+ICioqK9Mwzz+jaa6+Vr6+vGjdurDFjxpx3zMnJyVq2bJmuv/56+fr6qmXLlkpLSzvnnJ999pnat28vPz8/NWvWTK+99pr53v2Z5zt58qQWLVpkPkp55r2TpPz8fHM2ICAgQA8//LB+/vlnlz7p6enq3LmzAgMDVa9ePV133XX661//6rb7AwCwlwULFqhr164KCQmRr6+voqOjNXfu3HP6NW3aVHfddZdWrlyp9u3bq3bt2nrttdckST/88IPuvvtu1a1bVyEhIRo5cqRWrlx53tcCfi+PT5o0SaNHj5YkRUZGmjlx//79VzzWi/kdojw/792793dz7i+//KI///nPuuqqq1S/fn3dfffdOnjwoMvvRRc7nt/7feKnn37SiBEj1LRpU/n6+iokJETdu3fX119/fcX3BahszOgDFSAiIkKZmZnatm2brr/++t/s+8gjj2jRokW699579Ze//EVZWVmaOnWqdu7cqaVLl152DE888YQaNmyoiRMn6uTJk5J+/SPA4MGD1bJlS40bN06BgYH65ptvlJaWpj/96U+SpDVr1qhXr15q166dnnnmGXl6epq/oPz73//WTTfddNkxSVJZWZnuvvtuffHFFxo2bJiioqK0detWzZw5U7t37z7n/fkvvvhCH3zwgZ544gnVr19fs2fPVv/+/ZWTk6Pg4GBJ0jfffKOePXsqLCxMzz77rEpLS5WSkqKGDRu6nOsf//iHHnnkEd10000aNmyYJKlZs2Yufe677z5FRkZq6tSp+vrrr/X3v/9dISEhmjZtmiRp+/btuuuuu3TDDTcoJSVFvr6+2rt3r1v/EAIAsJe5c+eqZcuWuvvuu+Xt7a3ly5friSeeUFlZmZKSklz67tq1Sw888IAeffRRDR06VNddd51Onjyprl276vDhw3ryySflcDi0ZMkSrV279pxrXUwe79evn3bv3q1//vOfmjlzpq666ipJOidvXqpL/R3i93Ku9OvaAO+9954GDhyoTp066fPPP1d8fLzLeS5mPBfz+8Rjjz2m//f//p+Sk5MVHR2to0eP6osvvtDOnTt14403XtG9ASqdAcDtVq1aZXh5eRleXl5GTEyMMWbMGGPlypVGcXGxS7/NmzcbkoxHHnnE5fj//d//GZKMNWvWmMckGc8888w514qIiDASExPN/QULFhiSjM6dOxslJSXm8fz8fKN+/fpGx44djV9++cXlHGVlZeZ/Nm/e3IiLizOPGYZh/Pzzz0ZkZKTRvXv33xz3vn37DEnGiy++eME+//jHPwxPT0/j3//+t8vxefPmGZKML7/80mXMPj4+xt69e81jW7ZsMSQZc+bMMY/17t3bqFOnjnHw4EHz2J49ewxvb2/j7H/m6tat63K/yj3zzDOGJGPw4MEux/v27WsEBweb+zNnzjQkGUeOHLngGAEANVdSUtI5uefnn38+p19cXJxxzTXXuByLiIgwJBlpaWkux19++WVDkrFs2TLz2C+//GK0aNHCkGSsXbvWMIxLy+MvvviiIcnYt2/fRY0rMTHRqFu37gXbL+XaF5tzs7OzDUnGiBEjXPo99NBD5/xe9FvjudjfJwICAoykpKQL3wSgGuHRfaACdO/eXZmZmbr77ru1ZcsWTZ8+XXFxcWrUqJE++ugjs98nn3wiSRo1apTL5//yl79Ikj7++OPLjmHo0KHy8vIy99PT0/XTTz/pqaeekp+fn0vf8sfbN2/erD179uhPf/qTjh49qh9//FE//vijTp48qW7dumndunUqKyu77Jgk6f3331dUVJRatGhhnv/HH39U165dJemc2YnY2FiXWfcbbrhB/v7++v777yVJpaWlWr16tfr06aPw8HCz37XXXqtevXpdcnyPPfaYy/6tt96qo0ePqrCwUJLM1zE+/PDDK74XAICa4cx37AsKCvTjjz/qtttu0/fff6+CggKXvpGRkYqLi3M5lpaWpkaNGunuu+82j/n5+Wno0KEu/Sojj1/I5Vz793Ju+aP1TzzxhEu/y1nU+Pd+n5B+zfFZWVk6dOjQJZ8fqGp4dB+oIB06dNAHH3yg4uJibdmyRUuXLtXMmTN17733avPmzYqOjtYPP/wgT09PXXvttS6fdTgcCgwM1A8//HDZ14+MjHTZ/+677yTpN18l2LNnjyQpMTHxgn0KCgrUoEGDy45rz5492rlz5wUfDzx7scImTZqc06dBgwY6fvy42f+XX3455x5KOu+x33P29crHevz4cfn7++v+++/X3//+dz3yyCN66qmn1K1bN/Xr10/33nuvPD352ykA4FxffvmlnnnmGWVmZp7zDnpBQYECAgLM/bPzt/Tr+/nNmjVzWXdGOjfPVUYev5DLufbv5dzy35POvifuyO/l1yv/fUKSpk+frsTERDVu3Fjt2rXTnXfeqUGDBumaa6655OsBVqPQByqYj4+POnTooA4dOugPf/iDHn74Yb3//vt65plnzD5nJ+5LUVpaet7jl7NCb/lf2l988UW1adPmvH2u9Dt0y8rK1KpVK82YMeO87Y0bN3bZP/OphDMZhnFFcVzI712vdu3aWrdundauXauPP/5YaWlpevfdd9W1a1etWrXqgp8HANRM3333nbp166YWLVpoxowZaty4sXx8fPTJJ59o5syZ58xyX8kK+5WRx9157crM8Rdzrfvuu0+33nqrli5dqlWrVunFF1/UtGnT9MEHH1zWU4KAlSj0gUrUvn17SdLhw4cl/bpoX1lZmfbs2ePy/bq5ubnKz89XRESEeaxBgwbKz893OV9xcbF5rt9T/rjatm3bLviX8PI+/v7+io2NvbhBXaJmzZppy5Yt6tat2xX9gaNcSEiI/Pz8tHfv3nPaznfMHdf09PRUt27d1K1bN82YMUNTpkzR008/rbVr11bYfQMAVE/Lly9XUVGRPvroI5dZ5fMtpHchERER2rFjhwzDcMljZ+e5S8nj7siHl3vti1X+e9K+ffvUvHlz83hF5XdJCgsL0xNPPKEnnnhCeXl5uvHGG/X8889T6KPa4TlToAKsXbv2vH+NLn8n/7rrrpMk3XnnnZKkWbNmufQrn+0+c1XZZs2aad26dS79Xn/99QvO6J+tR48eql+/vqZOnapTp065tJXH2q5dOzVr1kwvvfSSTpw4cc45jhw5clHX+i333XefDh48qDfeeOOctl9++cX8hoCL5eXlpdjYWC1btszlnbq9e/fq008/Pad/3bp1z/mDyaU4duzYOcfKZy7O/npAAADKZ5LP/L2goKBACxYsuOhzxMXF6eDBgy7r/Jw6deqcXHopebxu3bqSdEU58XKvfbHK1yp49dVXXY7PmTPnnL5XOp7S0tJz1ksICQlReHg4+R3VEjP6QAUYPny4fv75Z/Xt21ctWrRQcXGx1q9fr3fffVdNmzbVww8/LElq3bq1EhMT9frrrys/P1+33XabNm7cqEWLFqlPnz664447zHM+8sgjeuyxx9S/f391795dW7Zs0cqVK82vkPk9/v7+mjlzph555BF16NBBf/rTn9SgQQNt2bJFP//8sxYtWiRPT0/9/e9/V69evdSyZUs9/PDDatSokQ4ePKi1a9fK399fy5cv/91rZWRknPPHBEnq06ePBg4cqPfee0+PPfaY1q5dq1tuuUWlpaX69ttv9d5775nfHXwpJk2apFWrVumWW27R448/rtLSUr3yyiu6/vrrtXnzZpe+7dq10+rVqzVjxgyFh4crMjJSHTt2vOhrpaSkaN26dYqPj1dERITy8vL06quv6uqrr1bnzp0vKW4AgP316NFDPj4+6t27tx599FGdOHFCb7zxhkJCQi76qbxHH31Ur7zyih544AE9+eSTCgsL0+LFi83Fdctnsy8lj7dr106S9PTTT2vAgAGqVauWevfubRbM53P69Gk999xz5xwPCgrSE0884ZbfIc7Url079e/fX7NmzdLRo0fNr9fbvXu3y7gvdzxn+umnn3T11Vfr3nvvVevWrVWvXj2tXr1amzZt0ssvv3xJcQNVgnUL/gP29emnnxqDBw82WrRoYdSrV8/w8fExrr32WmP48OFGbm6uS9/Tp08bzz77rBEZGWnUqlXLaNy4sTFu3Djj1KlTLv1KS0uNsWPHGldddZVRp04dIy4uzti7d+8Fv15v06ZN543to48+Mm6++Wajdu3ahr+/v3HTTTcZ//znP136fPPNN0a/fv2M4OBgw9fX14iIiDDuu+8+IyMj4zfHXf71ehfa/vGPfxiGYRjFxcXGtGnTjJYtWxq+vr5GgwYNjHbt2hnPPvusUVBQYJ5P0nm/5ubsMRuGYWRkZBht27Y1fHx8jGbNmhl///vfjb/85S+Gn5+fS79vv/3W6NKli1G7dm1Dknme8q/6Oftr88rvZ/nX9WRkZBj33HOPER4ebvj4+Bjh4eHGAw88YOzevfs37w0AoGY439frffTRR8YNN9xg+Pn5GU2bNjWmTZtmvPnmm+d8HVxERIQRHx9/3vN+//33Rnx8vFG7dm2jYcOGxl/+8hfjX//6lyHJ2LBhg0vfi83jkydPNho1amR4enr+7lftJSYmXjC/N2vW7JKufbE51zAM4+TJk0ZSUpIRFBRk1KtXz+jTp4+xa9cuQ5LxwgsvXNR4Lub3iaKiImP06NFG69atjfr16xt169Y1Wrdubbz66qsXvCdAVeZhGBW0ohUAWKxPnz7avn27uRIwAAB2MmvWLI0cOVL//e9/1ahRI6vDqTSbN29W27Zt9fbbbyshIcHqcIAqiXf0AdjCL7/84rK/Z88effLJJ7r99tutCQgAADc6O8+dOnVKr732mpo3b27rIv/scUu//oHD09NTXbp0sSAioHrgHX0AtnDNNdfooYce0jXXXKMffvhBc+fOlY+Pj8aMGWN1aAAAXLF+/fqpSZMmatOmjQoKCvT222/r22+/1eLFi60OrUJNnz5d2dnZuuOOO+Tt7a1PP/1Un376qYYNG3bOV/IC+B8e3QdgCw8//LDWrl0rp9MpX19fxcTEaMqUKbrxxhutDg0AgCs2a9Ys/f3vf9f+/ftVWlqq6OhojRkzRvfff7/VoVWo9PR0Pfvss9qxY4dOnDihJk2aaODAgXr66afl7c2cJXAhFPoAAAAAANgI7+gDAIDLtm7dOvXu3Vvh4eHy8PDQsmXLXNoNw9DEiRMVFham2rVrKzY29pwFMo8dO6aEhAT5+/srMDBQQ4YMOed7uP/zn//o1ltvlZ+fnxo3bqzp06dX9NAAAKi2KPQBAMBlO3nypFq3bq3U1NTztk+fPl2zZ8/WvHnzlJWVpbp16youLk6nTp0y+yQkJGj79u1KT0/XihUrtG7dOg0bNsxsLywsVI8ePRQREaHs7Gy9+OKLmjRpkl5//fUKHx8AANURj+5LKisr06FDh1S/fn15eHhYHQ4AoIYzDEM//fSTwsPD5elZff4m7+HhoaVLl6pPnz6Sfh1HeHi4/vKXv+j//u//JEkFBQUKDQ3VwoULNWDAAO3cuVPR0dHatGmT2rdvL0lKS0vTnXfeqf/+978KDw/X3Llz9fTTT8vpdMrHx0eS9NRTT2nZsmX69ttvLyo2cj0AoKqpyHzPChaSDh06xKqdAIAq58CBA7r66qutDuOy7du3T06nU7GxseaxgIAAdezYUZmZmRowYIAyMzMVGBhoFvmSFBsbK09PT2VlZalv377KzMxUly5dzCJfkuLi4jRt2jQdP35cDRo0OOfaRUVFKioqMvcPHjyo6OjoChopAACXryLyPYW+pPr160v69Qb7+/tbHA0AoKYrLCxU48aNzfxUXTmdTklSaGioy/HQ0FCzzel0KiQkxKXd29tbQUFBLn0iIyPPOUd52/kK/alTp+rZZ5895zi5HgBQVVRkvqfQl8xH+Pz9/Un+AIAqg0fML9+4ceM0atQoc7/8lylyPQCgqqmIfF99XvwDAADVisPhkCTl5ua6HM/NzTXbHA6H8vLyXNpLSkp07Ngxlz7nO8eZ1zibr6+vWdRT3AMAahoKfQAAUCEiIyPlcDiUkZFhHissLFRWVpZiYmIkSTExMcrPz1d2drbZZ82aNSorK1PHjh3NPuvWrdPp06fNPunp6bruuuvO+9g+AAA1HYU+AAC4bCdOnNDmzZu1efNmSb8uwLd582bl5OTIw8NDI0aM0HPPPaePPvpIW7du1aBBgxQeHm6uzB8VFaWePXtq6NCh2rhxo7788kslJydrwIABCg8PlyT96U9/ko+Pj4YMGaLt27fr3Xff1d/+9jeXR/MBAMD/8I4+AAC4bF999ZXuuOMOc7+8+E5MTNTChQs1ZswYnTx5UsOGDVN+fr46d+6stLQ0+fn5mZ9ZvHixkpOT1a1bN3l6eqp///6aPXu22R4QEKBVq1YpKSlJ7dq101VXXaWJEydq2LBhlTdQAACqEQ/DMAyrg7BaYWGhAgICVFBQwDt8AADLkZfcj3sKAKhqKjI38eg+AAAAAAA2QqEPAAAAAICNUOgDAAAAAGAjFPoAAAAAANgIhT4AAAAAADZCoQ8AAAAAgI1Q6AMAAAAAYCMU+gAAAAAA2AiFPgAAAAAANkKhDwAAAACAjXhbHQAqVs/efXX4yFFzP6xhsNKWL7UwIgAAUO7+QUN06MhxSVLOvu/UJLKZJCm8YQO9+9Z8K0MDAFRjFPo2d/jIUUUNnmbu73xzrIXRAACAMx06clyN+o6WJG2f8oj588GlL1oZFgCgmqPQt4kzZ+6ZtQcAAACAmotC3ybOnLln1h4AAAAAai4W4wMAAAAAwEYo9AEAAAAAsBEKfQAAAAAAbIRCHwAAAAAAG6HQBwAAAADARij0AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAAAAAwEYo9AEAAAAAsBFLC/3S0lJNmDBBkZGRql27tpo1a6bJkyfLMAyzj2EYmjhxosLCwlS7dm3FxsZqz549Luc5duyYEhIS5O/vr8DAQA0ZMkQnTpyo7OEAAAC43f2DhujWXv10a69+un/QEKvDAQBUA5YW+tOmTdPcuXP1yiuvaOfOnZo2bZqmT5+uOXPmmH2mT5+u2bNna968ecrKylLdunUVFxenU6dOmX0SEhK0fft2paena8WKFVq3bp2GDRtmxZAAAADc6tCR42rUd7Qa9R2tQ0eOWx0OAKAa8Lby4uvXr9c999yj+Ph4SVLTpk31z3/+Uxs3bpT062z+rFmzNH78eN1zzz2SpLfeekuhoaFatmyZBgwYoJ07dyotLU2bNm1S+/btJUlz5szRnXfeqZdeeknh4eHWDA4AAAAAAAtYOqN/8803KyMjQ7t375YkbdmyRV988YV69eolSdq3b5+cTqdiY2PNzwQEBKhjx47KzMyUJGVmZiowMNAs8iUpNjZWnp6eysrKOu91i4qKVFhY6LLVFPv3fa/Wnbqodacu6tm7r9XhAAAAAADczNIZ/aeeekqFhYVq0aKFvLy8VFpaqueff14JCQmSJKfTKUkKDQ11+VxoaKjZ5nQ6FRIS4tLu7e2toKAgs8/Zpk6dqmeffdbdw6kWSgwPRQ2eJkna+eZYi6MBAAAAALibpTP67733nhYvXqwlS5bo66+/1qJFi/TSSy9p0aJFFXrdcePGqaCgwNwOHDhQodcDAAAAAKCyWDqjP3r0aD311FMaMGCAJKlVq1b64YcfNHXqVCUmJsrhcEiScnNzFRYWZn4uNzdXbdq0kSQ5HA7l5eW5nLekpETHjh0zP382X19f+fr6VsCIAAAAAACwlqUz+j///LM8PV1D8PLyUllZmSQpMjJSDodDGRkZZnthYaGysrIUExMjSYqJiVF+fr6ys7PNPmvWrFFZWZk6duxYCaMAAAAAAKDqsHRGv3fv3nr++efVpEkTtWzZUt98841mzJihwYMHS5I8PDw0YsQIPffcc2revLkiIyM1YcIEhYeHq0+fPpKkqKgo9ezZU0OHDtW8efN0+vRpJScna8CAAay4DwAAAACocSwt9OfMmaMJEyboiSeeUF5ensLDw/Xoo49q4sSJZp8xY8bo5MmTGjZsmPLz89W5c2elpaXJz8/P7LN48WIlJyerW7du8vT0VP/+/TV79mwrhgQAAAAAgKUsLfTr16+vWbNmadasWRfs4+HhoZSUFKWkpFywT1BQkJYsWVIBEQIAAAAAUL1YWujDWvv3fa/WnbpIksIaBitt+VKLIwIAAAAAXCkK/RqsxPBQ1OBpkqSdb461OBoAAAAAgDtYuuo+AAAAAABwLwp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARVt0HAACoYPcPGqJDR45LksIbNtC7b823OCIAgJ1R6EOStH/f92rdqYu5H9YwWGnLl1oYEQAA9nHoyHE16jtaknRw6YsWRwMAsDsKfUiSSgwPRQ2eZu7vfHOshdEAAAAAAC4X7+gDAAAAAGAjFPoAAAAAANgIhT4AAAAAADZCoQ8AAAAAgI1Q6AMAAAAAYCOsuo/f1bN3Xx0+clQSX7sHAAAAAFUdhT5+1+EjR82v3uNr9wAAAACgauPRfQAAAAAAbIRCHwAAAAAAG6HQBwAAAADARij0AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARCn0AAAAAAGzE2+oAUH317N1Xh48cNffDGgYrbflSCyMCAMDedn+7U7f26idJCm/YQO++Nd/iiAAAVREz+rhsh48cVdTgaeZ2ZtEPAIAklZaWasKECYqMjFTt2rXVrFkzTZ48WYZhmH0Mw9DEiRMVFham2rVrKzY2Vnv27HE5z7Fjx5SQkCB/f38FBgZqyJAhOnHiRGUPx3KnDU816jtajfqO1qEjx60OBwBQRVHoAwCACjNt2jTNnTtXr7zyinbu3Klp06Zp+vTpmjNnjtln+vTpmj17tubNm6esrCzVrVtXcXFxOnXqlNknISFB27dvV3p6ulasWKF169Zp2LBhVgwJAIAqj0f3AQBAhVm/fr3uuecexcfHS5KaNm2qf/7zn9q4caOkX2fzZ82apfHjx+uee+6RJL311lsKDQ3VsmXLNGDAAO3cuVNpaWnatGmT2rdvL0maM2eO7rzzTr300ksKDw+3ZnAAAFRRzOgDAIAKc/PNNysjI0O7d++WJG3ZskVffPGFevXqJUnat2+fnE6nYmNjzc8EBASoY8eOyszMlCRlZmYqMDDQLPIlKTY2Vp6ensrKyjrvdYuKilRYWOiyAQBQUzCjj0uyf9/3at2py68/5+QoyuJ4AABV21NPPaXCwkK1aNFCXl5eKi0t1fPPP6+EhARJktPplCSFhoa6fC40NNRsczqdCgkJcWn39vZWUFCQ2edsU6dO1bPPPuvu4VR59w8aYr67z2J9AFBzWTqj37RpU3l4eJyzJSUlSZJOnTqlpKQkBQcHq169eurfv79yc3NdzpGTk6P4+HjVqVNHISEhGj16tEpKSqwYTo1QYniYi++VlJRaHQ4AoIp77733tHjxYi1ZskRff/21Fi1apJdeekmLFi2q0OuOGzdOBQUF5nbgwIEKvV5VcejIcRbrAwBYO6O/adMmlZb+r1jctm2bunfvrj/+8Y+SpJEjR+rjjz/W+++/r4CAACUnJ6tfv3768ssvJf26km98fLwcDofWr1+vw4cPa9CgQapVq5amTJliyZgAAMD/jB49Wk899ZQGDBggSWrVqpV++OEHTZ06VYmJiXI4HJKk3NxchYWFmZ/Lzc1VmzZtJEkOh0N5eXku5y0pKdGxY8fMz5/N19dXvr6+FTAiAACqPktn9Bs2bCiHw2FuK1asULNmzXTbbbepoKBA8+fP14wZM9S1a1e1a9dOCxYs0Pr167VhwwZJ0qpVq7Rjxw69/fbbatOmjXr16qXJkycrNTVVxcXFVg4NAABI+vnnn+Xp6frrhpeXl8rKyiRJkZGRcjgcysjIMNsLCwuVlZWlmJgYSVJMTIzy8/OVnZ1t9lmzZo3KysrUsWPHShgFAADVS5VZjK+4uFhvv/22Bg8eLA8PD2VnZ+v06dMui/O0aNFCTZo0cVmcp1WrVi7v9cXFxamwsFDbt2+/4LVYoAcAgMrRu3dvPf/88/r444+1f/9+LV26VDNmzFDfvn0lSR4eHhoxYoSee+45ffTRR9q6dasGDRqk8PBw9enTR5IUFRWlnj17aujQodq4caO+/PJLJScna8CAAay4DwDAeVSZxfiWLVum/Px8PfTQQ5J+XXjHx8dHgYGBLv3OXpznfIv3lLddSE1doAcAgMo2Z84cTZgwQU888YTy8vIUHh6uRx99VBMnTjT7jBkzRidPntSwYcOUn5+vzp07Ky0tTX5+fmafxYsXKzk5Wd26dZOnp6f69++v2bNnWzEkAACqvCpT6M+fP1+9evWqlL/Mjxs3TqNGjTL3CwsL1bhx4wq/LgAANU39+vU1a9YszZo164J9PDw8lJKSopSUlAv2CQoK0pIlSyogQgAA7KdKFPo//PCDVq9erQ8++MA85nA4VFxcrPz8fJdZ/dzcXHPhHYfDoY0bN7qcq3xV/gstziOxQA8AAAAAwL6qxDv6CxYsUEhIiOLj481j7dq1U61atVwW59m1a5dycnJcFufZunWry0q86enp8vf3V3R0dOUNwCI9e/dV605d1LpTF+3PybE6HAAAAABAFWD5jH5ZWZkWLFigxMREeXv/L5yAgAANGTJEo0aNUlBQkPz9/TV8+HDFxMSoU6dOkqQePXooOjpaAwcO1PTp0+V0OjV+/HglJSXViBn7w0eOKmrwNEnS3vH3WRwNAAAAAKAqsLzQX716tXJycjR48OBz2mbOnGkuuFNUVKS4uDi9+uqrZruXl5dWrFihxx9/XDExMapbt64SExN/8x0/AAAAAADszPJCv0ePHjIM47xtfn5+Sk1NVWpq6gU/HxERoU8++aSiwgMAAAAAoFqpEu/oAwAAAAAA96DQBwAAAADARij0AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAAAAAwEYo9AEAAAAAsBEKfQAAAAAAbMTb6gAAAABwZe4fNESHjhzXnr171cjqYAAAlmNGHwAAoJo7dOS4GvUdreLTpVaHAgCoAij0AQAAAACwEQp9AAAAAABshHf0USF69u6rw0eOSpLCGgYrbflSiyMCAAAAgJqBQh8V4vCRo4oaPE2StPPNsRZHAwCANVgkDwBgBR7dBwAAqCAskgcAsAKFPgAAAAAANkKhDwAAAACAjVDoAwAAAABgIxT6AAAAAADYCIU+AAAAAAA2QqEPAAAAAICNUOgDAAAAAGAjFPoAAAAAANgIhT4AAAAAADZCoQ8AAAAAgI1Q6AMAAAAAYCPeVgcA+9u/73u17tRFkhTWMFhpy5daHBEAAAAA2BeFPipcieGhqMHTJEk73xxrcTQAAAAAYG+WP7p/8OBBPfjggwoODlbt2rXVqlUrffXVV2a7YRiaOHGiwsLCVLt2bcXGxmrPnj0u5zh27JgSEhLk7++vwMBADRkyRCdOnKjsoQAAAAAAYDlLC/3jx4/rlltuUa1atfTpp59qx44devnll9WgQQOzz/Tp0zV79mzNmzdPWVlZqlu3ruLi4nTq1CmzT0JCgrZv36709HStWLFC69at07Bhw6wYEgAAAAAAlrL00f1p06apcePGWrBggXksMjLS/NkwDM2aNUvjx4/XPffcI0l66623FBoaqmXLlmnAgAHauXOn0tLStGnTJrVv316SNGfOHN1555166aWXFB4eXrmDAgAAAADAQpbO6H/00Udq3769/vjHPyokJERt27bVG2+8Ybbv27dPTqdTsbGx5rGAgAB17NhRmZmZkqTMzEwFBgaaRb4kxcbGytPTU1lZWee9blFRkQoLC102AAAAAADswNJC//vvv9fcuXPVvHlzrVy5Uo8//rj+/Oc/a9GiRZIkp9MpSQoNDXX5XGhoqNnmdDoVEhLi0u7t7a2goCCzz9mmTp2qgIAAc2vcuLG7hwYAAAAAgCUsLfTLysp04403asqUKWrbtq2GDRumoUOHat68eRV63XHjxqmgoMDcDhw4UKHXAwAAAACgslj6jn5YWJiio6NdjkVFRelf//qXJMnhcEiScnNzFRYWZvbJzc1VmzZtzD55eXku5ygpKdGxY8fMz5/N19dXvr6+7hoGAABAlXb/oCE6dOS4JCm8YQO9+9Z8iyMCAFQkS2f0b7nlFu3atcvl2O7duxURESHp14X5HA6HMjIyzPbCwkJlZWUpJiZGkhQTE6P8/HxlZ2ebfdasWaOysjJ17NixEkYBAABQtR06clyN+o5Wo76jzYIfAGBfls7ojxw5UjfffLOmTJmi++67Txs3btTrr7+u119/XZLk4eGhESNG6LnnnlPz5s0VGRmpCRMmKDw8XH369JH06xMAPXv2NB/5P336tJKTkzVgwABW3AcAAAAA1DiWFvodOnTQ0qVLNW7cOKWkpCgyMlKzZs1SQkKC2WfMmDE6efKkhg0bpvz8fHXu3FlpaWny8/Mz+yxevFjJycnq1q2bPD091b9/f82ePduKIQEAAAAAYClLC31Juuuuu3TXXXddsN3Dw0MpKSlKSUm5YJ+goCAtWbKkIsIDAAAAAKBasfQdfQAAAAAA4F6Wz+gDAACg8uz+dqdu7dVPEivwA4BdUegDAADUIKcNTzXqO1qSdHDpixZHAwCoCDy6DwAAAACAjVDoAwAAAABgIzy6X4307N1Xh48cNff35+QoysJ4AAAAAABVD4V+NXL4yFFFDZ5m7u8df5+F0QAAAAAAqiIKfVSq/fu+V+tOXcz9sIbBSlu+1MKIAAAAAMBeKPRRqUoMD5enEna+OdbCaAAAAADAfliMDwAAAAAAG6HQBwAAAADARij0AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAABXq4MGDevDBBxUcHKzatWurVatW+uqrr8x2wzA0ceJEhYWFqXbt2oqNjdWePXtcznHs2DElJCTI399fgYGBGjJkiE6cOFHZQwEAoFqg0AcAABXm+PHjuuWWW1SrVi19+umn2rFjh15++WU1aNDA7DN9+nTNnj1b8+bNU1ZWlurWrau4uDidOnXK7JOQkKDt27crPT1dK1as0Lp16zRs2DArhgQAQJXnbXUAAADAvqZNm6bGjRtrwYIF5rHIyEjzZ8MwNGvWLI0fP1733HOPJOmtt95SaGioli1bpgEDBmjnzp1KS0vTpk2b1L59e0nSnDlzdOedd+qll15SeHh45Q4KAIAqjhl9AABQYT766CO1b99ef/zjHxUSEqK2bdvqjTfeMNv37dsnp9Op2NhY81hAQIA6duyozMxMSVJmZqYCAwPNIl+SYmNj5enpqaysrPNet6ioSIWFhS4bAAA1BYU+AACoMN9//73mzp2r5s2ba+XKlXr88cf15z//WYsWLZIkOZ1OSVJoaKjL50JDQ802p9OpkJAQl3Zvb28FBQWZfc42depUBQQEmFvjxo3dPTRb2P3tTt3aq59u7dVP9w8aYnU4AAA34dF9AABQYcrKytS+fXtNmTJFktS2bVtt27ZN8+bNU2JiYoVdd9y4cRo1apS5X1hYSLF/HqcNTzXqO1qSdHDpixZHAwBwF2b0AQBAhQkLC1N0dLTLsaioKOXk5EiSHA6HJCk3N9elT25urtnmcDiUl5fn0l5SUqJjx46Zfc7m6+srf39/lw0AgJqCQh8AAFSYW265Rbt27XI5tnv3bkVEREj6dWE+h8OhjIwMs72wsFBZWVmKiYmRJMXExCg/P1/Z2dlmnzVr1qisrEwdO3ashFEAAFC98Og+AACoMCNHjtTNN9+sKVOm6L777tPGjRv1+uuv6/XXX5ckeXh4aMSIEXruuefUvHlzRUZGasKECQoPD1efPn0k/foEQM+ePTV06FDNmzdPp0+fVnJysgYMGMCK+wAAnAeFPgAAqDAdOnTQ0qVLNW7cOKWkpCgyMlKzZs1SQkKC2WfMmDE6efKkhg0bpvz8fHXu3FlpaWny8/Mz+yxevFjJycnq1q2bPD091b9/f82ePduKIQEAUOVR6AMAgAp111136a677rpgu4eHh1JSUpSSknLBPkFBQVqyZElFhAcAgO3wjj4AAAAAADZCoQ8AAAAAgI1Q6AMAAAAAYCOWFvqTJk2Sh4eHy9aiRQuz/dSpU0pKSlJwcLDq1aun/v37n/M9uzk5OYqPj1edOnUUEhKi0aNHq6SkpLKHAgAAAABAlWD5YnwtW7bU6tWrzX1v7/+FNHLkSH388cd6//33FRAQoOTkZPXr109ffvmlJKm0tFTx8fFyOBxav369Dh8+rEGDBqlWrVqaMmVKpY8FAAAAAACrWV7oe3t7y+FwnHO8oKBA8+fP15IlS9S1a1dJ0oIFCxQVFaUNGzaoU6dOWrVqlXbs2KHVq1crNDRUbdq00eTJkzV27FhNmjRJPj4+lT0cXKL9+75X605dJElhDYOVtnypxREBAAAAQPVm+Tv6e/bsUXh4uK655holJCQoJydHkpSdna3Tp08rNjbW7NuiRQs1adJEmZmZkqTMzEy1atVKoaGhZp+4uDgVFhZq+/btF7xmUVGRCgsLXTZYo8TwUNTgaYoaPE2Hjxy1OhwAAAAAqPYsLfQ7duyohQsXKi0tTXPnztW+fft066236qeffpLT6ZSPj48CAwNdPhMaGiqn0ylJcjqdLkV+eXt524VMnTpVAQEB5ta4cWP3DgwAAAAAAItY+uh+r169zJ9vuOEGdezYUREREXrvvfdUu3btCrvuuHHjNGrUKHO/sLCQYh8AAAAAYAuWv6N/psDAQP3hD3/Q3r171b17dxUXFys/P99lVj83N9d8p9/hcGjjxo0u5yhflf987/2X8/X1la+vr/sHAAAAaqT7Bw3RoSPHJUnhDRvo3bfmWxwRAKAms/wd/TOdOHFC3333ncLCwtSuXTvVqlVLGRkZZvuuXbuUk5OjmJgYSVJMTIy2bt2qvLw8s096err8/f0VHR1d6fEDAICa6dCR42rUd7Qa9R1tFvwAAFjF0hn9//u//1Pv3r0VERGhQ4cO6ZlnnpGXl5ceeOABBQQEaMiQIRo1apSCgoLk7++v4cOHKyYmRp06dZIk9ejRQ9HR0Ro4cKCmT58up9Op8ePHKykpiRl7AACAy8QTCgBQvV3WjP4111yjo0fPXSE9Pz9f11xzzUWf57///a8eeOABXXfddbrvvvsUHBysDRs2qGHDhpKkmTNn6q677lL//v3VpUsXORwOffDBB+bnvby8tGLFCnl5eSkmJkYPPvigBg0apJSUlMsZFgAANYK78jjsiycUAKB6u6wZ/f3796u0tPSc40VFRTp48OBFn+edd975zXY/Pz+lpqYqNTX1gn0iIiL0ySefXPQ1AQCo6dyVxwEAQNV0SYX+Rx99ZP68cuVKBQQEmPulpaXKyMhQ06ZN3RYcAABwH/I4AAA1wyUV+n369JEkeXh4KDEx0aWtVq1aatq0qV5++WW3BYeaq2fvvjp85H+PlYY1DFba8qUWRgQA1R95HACAmuGSCv2ysjJJUmRkpDZt2qSrrrqqQoICDh85qqjB08z9nW+OtTAaALAH8jgAADXDZb2jv2/fPnfHAQAAKgl5HAAAe7vsr9fLyMhQRkaG8vLyzBmCcm+++eYVBwYAACoOeRwAAPu6rEL/2WefVUpKitq3b6+wsDB5eHi4Oy4AAFBByOMAANjbZRX68+bN08KFCzVw4EB3xwMAACoYeRwAAHvzvJwPFRcX6+abb3Z3LAAAoBKQxwEAsLfLKvQfeeQRLVmyxN2xAACASkAeBwDA3i7r0f1Tp07p9ddf1+rVq3XDDTeoVq1aLu0zZsxwS3AAAMD9yOMAANjbZRX6//nPf9SmTRtJ0rZt21zaWNAHAICqjTwOAIC9XVahv3btWnfHAQAAKgl5HAAAe7usd/QBAAAAAEDVdFkz+nfcccdvPtq3Zs2ayw4IAABULPI4AAD2dlmFfvl7feVOnz6tzZs3a9u2bUpMTHRHXAAAoIKQxwEAsLfLKvRnzpx53uOTJk3SiRMnriggAABQscjjAADYm1vf0X/wwQf15ptvuvOUAACgkpDHAQCwB7cW+pmZmfLz83PnKQEAQCUhjwMAYA+X9eh+v379XPYNw9Dhw4f11VdfacKECW4JDAAAVAzyOAAA9nZZhX5AQIDLvqenp6677jqlpKSoR48ebgkMAABUDPI4AAD2dlmF/oIFC9wdBwAAqCTkcQAA7O2yCv1y2dnZ2rlzpySpZcuWatu2rVuCAgAAFY88DgCAPV1WoZ+Xl6cBAwbos88+U2BgoCQpPz9fd9xxh9555x01bNjQnTECAAA3Io8DAGBvl7Xq/vDhw/XTTz9p+/btOnbsmI4dO6Zt27apsLBQf/7zn90dIwAAcCPyOAAA9nZZM/ppaWlavXq1oqKizGPR0dFKTU1lER8AAKo48jgAAPZ2WTP6ZWVlqlWr1jnHa9WqpbKysisOCgAAVBzyOAAA9nZZhX7Xrl315JNP6tChQ+axgwcPauTIkerWrZvbggMAAO5HHsel2P3tTt3aq59u7dVP9w8aYnU4AICLcFmF/iuvvKLCwkI1bdpUzZo1U7NmzRQZGanCwkLNmTPH3TECAAA3Io/jUpw2PNWo72g16jtah44ctzocAMBFuKx39Bs3bqyvv/5aq1ev1rfffitJioqKUmxsrFuDAwAA7kceBwDA3i5pRn/NmjWKjo5WYWGhPDw81L17dw0fPlzDhw9Xhw4d1LJlS/373/+uqFgBAMAVII8DAFAzXFKhP2vWLA0dOlT+/v7ntAUEBOjRRx/VjBkz3BYcAABwH/I4AAA1wyUV+lu2bFHPnj0v2N6jRw9lZ2dfcVAAAMD9yOMAANQMl1To5+bmnvfreMp5e3vryJEjlxXICy+8IA8PD40YMcI8durUKSUlJSk4OFj16tVT//79lZub6/K5nJwcxcfHq06dOgoJCdHo0aNVUlJyWTEAAGBnFZnHAQBA1XFJhX6jRo20bdu2C7b/5z//UVhY2CUHsWnTJr322mu64YYbXI6PHDlSy5cv1/vvv6/PP/9chw4dUr9+/cz20tJSxcfHq7i4WOvXr9eiRYu0cOFCTZw48ZJjAADA7ioqjwMAgKrlkgr9O++8UxMmTNCpU6fOafvll1/0zDPP6K677rqkAE6cOKGEhAS98cYbatCggXm8oKBA8+fP14wZM9S1a1e1a9dOCxYs0Pr167VhwwZJ0qpVq7Rjxw69/fbbatOmjXr16qXJkycrNTVVxcXFlxQHAAB2VxF5HAAAVD2XVOiPHz9ex44d0x/+8AdNnz5dH374oT788ENNmzZN1113nY4dO6ann376kgJISkpSfHz8OV/pk52drdOnT7scb9GihZo0aaLMzExJUmZmplq1aqXQ0FCzT1xcnAoLC7V9+/YLXrOoqEiFhYUuGwAAdlcReRwAAFQ93pfSOTQ0VOvXr9fjjz+ucePGyTAMSZKHh4fi4uKUmprqUnT/nnfeeUdff/21Nm3adE6b0+mUj4+PAgMDz4nB6XSafc6+Xvl+eZ/zmTp1qp599tmLjhMAADtwdx4HAABV0yUV+pIUERGhTz75RMePH9fevXtlGIaaN2/u8tj9xThw4ICefPJJpaeny8/P71LDuCLjxo3TqFGjzP3CwkI1bty4UmMAAMAK7srjAACg6rrkQr9cgwYN1KFDh8u+cHZ2tvLy8nTjjTeax0pLS7Vu3Tq98sorWrlypYqLi5Wfn+8yq5+bmyuHwyFJcjgc2rhxo8t5y1flL+9zPr6+vvL19b3s2AEAqO6uNI8DAICq65Le0Xenbt26aevWrdq8ebO5tW/fXgkJCebPtWrVUkZGhvmZXbt2KScnRzExMZKkmJgYbd26VXl5eWaf9PR0+fv7Kzo6utLHBAAAAACA1S57Rv9K1a9fX9dff73Lsbp16yo4ONg8PmTIEI0aNUpBQUHy9/fX8OHDFRMTo06dOkmSevTooejoaA0cOFDTp0+X0+nU+PHjlZSUxIw9AAAAAKBGsqzQvxgzZ86Up6en+vfvr6KiIsXFxenVV1812728vLRixQo9/vjjiomJUd26dZWYmKiUlBQLowYAAAAAwDpVqtD/7LPPXPb9/PyUmpqq1NTUC36mfFEhAAAAAABg4Tv6AAAAAADA/Sj0AQAAAACwkSr16D5wIfv3fa/WnbpIksIaBitt+VKLIwIAAACAqolCH9VCieGhqMHTJEk73xxrcTQAAAAAUHXx6D4AAAAAADZCoQ8AAAAAgI1Q6AMAAAAAYCMU+gAAAAAA2AiFPgAAqDQvvPCCPDw8NGLECPPYqVOnlJSUpODgYNWrV0/9+/dXbm6uy+dycnIUHx+vOnXqKCQkRKNHj1ZJSUklRw8AQPVAoQ8AACrFpk2b9Nprr+mGG25wOT5y5EgtX75c77//vj7//HMdOnRI/fr1M9tLS0sVHx+v4uJirV+/XosWLdLChQs1ceLEyh4CAADVAoU+AACocCdOnFBCQoLeeOMNNWjQwDxeUFCg+fPna8aMGeratavatWunBQsWaP369dqwYYMkadWqVdqxY4fefvtttWnTRr169dLkyZOVmpqq4uJiq4YEAECVRaGPamf/vu/VulMXte7URT1797U6HADARUhKSlJ8fLxiY2NdjmdnZ+v06dMux1u0aKEmTZooMzNTkpSZmalWrVopNDTU7BMXF6fCwkJt3779vNcrKipSYWGhywb3uX/QEN3aq59u7dVP9w8aYnU4AICzeFsdAHCpSgwPRQ2eJkna+eZYi6MBAPyed955R19//bU2bdp0TpvT6ZSPj48CAwNdjoeGhsrpdJp9zizyy9vL285n6tSpevbZZ90QPc7n0JHjatR3tCTp4NIXLY4GAHA2ZvQBAECFOXDggJ588kktXrxYfn5+lXbdcePGqaCgwNwOHDhQadcGAMBqFPoAAKDCZGdnKy8vTzfeeKO8vb3l7e2tzz//XLNnz5a3t7dCQ0NVXFys/Px8l8/l5ubK4XBIkhwOxzmr8Jfvl/c5m6+vr/z9/V02AABqCgp9AABQYbp166atW7dq8+bN5ta+fXslJCSYP9eqVUsZGRnmZ3bt2qWcnBzFxMRIkmJiYrR161bl5eWZfdLT0+Xv76/o6OhKHxMAAFUd7+gDAIAKU79+fV1//fUux+rWravg4GDz+JAhQzRq1CgFBQXJ399fw4cPV0xMjDp16iRJ6tGjh6KjozVw4EBNnz5dTqdT48ePV1JSknx9fSt9TAAAVHUU+gAAwFIzZ86Up6en+vfvr6KiIsXFxenVV1812728vLRixQo9/vjjiomJUd26dZWYmKiUlBQLowYAoOqi0AcAAJXqs88+c9n38/NTamqqUlNTL/iZiIgIffLJJxUcGQAA9sA7+gAAAAAA2AiFPgAAAAAANsKj+1VQz959dfjIUUlSWMNgpS1fanFEAAAAAIDqgkK/Cjp85KiiBk+TJO18c6zF0QAAAAAAqhMe3QcAAAAAwEYo9AEAAAAAsBEKfQAAAAAAbIRCHwAAAAAAG6HQBwAAAADARij0AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGLC30586dqxtuuEH+/v7y9/dXTEyMPv30U7P91KlTSkpKUnBwsOrVq6f+/fsrNzfX5Rw5OTmKj49XnTp1FBISotGjR6ukpKSyhwIAAAAAQJVgaaF/9dVX64UXXlB2dra++uorde3aVffcc4+2b98uSRo5cqSWL1+u999/X59//rkOHTqkfv36mZ8vLS1VfHy8iouLtX79ei1atEgLFy7UxIkTrRoSAABAjbL72526tVc/3dqrn+4fNMTqcAAAkrytvHjv3r1d9p9//nnNnTtXGzZs0NVXX6358+dryZIl6tq1qyRpwYIFioqK0oYNG9SpUyetWrVKO3bs0OrVqxUaGqo2bdpo8uTJGjt2rCZNmiQfHx8rhgUAAFBjnDY81ajvaEnSwaUvWhwNAECqQu/ol5aW6p133tHJkycVExOj7OxsnT59WrGxsWafFi1aqEmTJsrMzJQkZWZmqlWrVgoNDTX7xMXFqbCw0Hwq4HyKiopUWFjosgEAAAAAYAeWF/pbt25VvXr15Ovrq8cee0xLly5VdHS0nE6nfHx8FBgY6NI/NDRUTqdTkuR0Ol2K/PL28rYLmTp1qgICAsytcePG7h0UAAAAAAAWsbzQv+6667R582ZlZWXp8ccfV2Jionbs2FGh1xw3bpwKCgrM7cCBAxV6PQAAAAAAKoul7+hLko+Pj6699lpJUrt27bRp0yb97W9/0/3336/i4mLl5+e7zOrn5ubK4XBIkhwOhzZu3OhyvvJV+cv7nI+vr698fX3dPBIAAAAAAKxn+Yz+2crKylRUVKR27dqpVq1aysjIMNt27dqlnJwcxcTESJJiYmK0detW5eXlmX3S09Pl7++v6OjoSo8dAAAAAACrWTqjP27cOPXq1UtNmjTRTz/9pCVLluizzz7TypUrFRAQoCFDhmjUqFEKCgqSv7+/hg8frpiYGHXq1EmS1KNHD0VHR2vgwIGaPn26nE6nxo8fr6SkJGbsAQAAAAA1kqWFfl5engYNGqTDhw8rICBAN9xwg1auXKnu3btLkmbOnClPT0/1799fRUVFiouL06uvvmp+3svLSytWrNDjjz+umJgY1a1bV4mJiUpJSbFqSAAAAAAAWMrSQn/+/Pm/2e7n56fU1FSlpqZesE9ERIQ++eQTd4eGaqhn7746fOSouR/WMFhpy5daGBEAAAAAVD7LF+MD3OXwkaOKGjzN3N/55lgLowEAAAAAa1S5xfgAAAAAAMDlo9AHAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABth1X0AAIDLcP+gITp05Lgkac/evWpkcTwAAJRjRh8AAOAyHDpyXI36jlajvqNVfLrU6nAAADBR6AMAAAAAYCM8ug8AAAC3O/PVhvCGDfTuW/MtjggAag4KfQAAALhd+asNknRw6YsWRwMANQuP7gMAAAAAYCMU+gAAAAAA2AiFPgAAAAAANkKhDwAAAACAjVDoAwAAAABgIxT6AAAAAADYCIU+AAAAAAA2QqEPAAAAAICNUOgDAAAAAGAjFPoAAAAAANgIhT4AAAAAADZCoQ8AAAAAgI1Q6AMAAAAAYCMU+gAAAAAA2AiFPgAAAAAANkKhDwAAAACAjVDoAwAAAABgIxT6AAAAAADYCIU+AAAAAAA2QqEPAAAAAICNUOgDAAAAAGAj3lYHAFSU/fu+V+tOXSRJYQ2DlbZ8qcURAQAAAEDFs3RGf+rUqerQoYPq16+vkJAQ9enTR7t27XLpc+rUKSUlJSk4OFj16tVT//79lZub69InJydH8fHxqlOnjkJCQjR69GiVlJRU5lBQBZUYHooaPE1Rg6fp8JGjVocDAAAAAJXC0kL/888/V1JSkjZs2KD09HSdPn1aPXr00MmTJ80+I0eO1PLly/X+++/r888/16FDh9SvXz+zvbS0VPHx8SouLtb69eu1aNEiLVy4UBMnTrRiSAAAAAAAWMrSR/fT0tJc9hcuXKiQkBBlZ2erS5cuKigo0Pz587VkyRJ17dpVkrRgwQJFRUVpw4YN6tSpk1atWqUdO3Zo9erVCg0NVZs2bTR58mSNHTtWkyZNko+PzznXLSoqUlFRkblfWFhYsQMFAACowXZ/u1O39vp1oia8YQO9+9Z8iyMCAHurUovxFRQUSJKCgoIkSdnZ2Tp9+rRiY2PNPi1atFCTJk2UmZkpScrMzFSrVq0UGhpq9omLi1NhYaG2b99+3utMnTpVAQEB5ta4ceOKGhIAAECNd9rwVKO+o9Wo72gdOnLc6nAAwPaqTKFfVlamESNG6JZbbtH1118vSXI6nfLx8VFgYKBL39DQUDmdTrPPmUV+eXt52/mMGzdOBQUF5nbgwAE3jwYAAAAAAGtUmVX3k5KStG3bNn3xxRcVfi1fX1/5+vpW+HUAAAAAAKhsVWJGPzk5WStWrNDatWt19dVXm8cdDoeKi4uVn5/v0j83N1cOh8Psc/Yq/OX75X0AAAAAAKgpLC30DcNQcnKyli5dqjVr1igyMtKlvV27dqpVq5YyMjLMY7t27VJOTo5iYmIkSTExMdq6davy8vLMPunp6fL391d0dHTlDAQAAAAAgCrC0kf3k5KStGTJEn344YeqX7+++U59QECAateurYCAAA0ZMkSjRo1SUFCQ/P39NXz4cMXExKhTp06SpB49eig6OloDBw7U9OnT5XQ6NX78eCUlJfF4PgAAAACgxrF0Rn/u3LkqKCjQ7bffrrCwMHN79913zT4zZ87UXXfdpf79+6tLly5yOBz64IMPzHYvLy+tWLFCXl5eiomJ0YMPPqhBgwYpJSXFiiEBAIAzTJ06VR06dFD9+vUVEhKiPn36aNeuXS59Tp06paSkJAUHB6tevXrq37//Oa/l5eTkKD4+XnXq1FFISIhGjx6tkpKSyhwKAADVhqUz+oZh/G4fPz8/paamKjU19YJ9IiIi9Mknn7gzNAAA4Aaff/65kpKS1KFDB5WUlOivf/2revTooR07dqhu3bqSpJEjR+rjjz/W+++/r4CAACUnJ6tfv3768ssvJUmlpaWKj4+Xw+HQ+vXrdfjwYQ0aNEi1atXSlClTrBweAABVUpVZdR8AANhPWlqay/7ChQsVEhKi7OxsdenSRQUFBZo/f76WLFmirl27SpIWLFigqKgobdiwQZ06ddKqVau0Y8cOrV69WqGhoWrTpo0mT56ssWPHatKkSfLx8bFiaAAAVFlVYtV9AABQMxQUFEiSgoKCJEnZ2dk6ffq0YmNjzT4tWrRQkyZNlJmZKUnKzMxUq1atFBoaavaJi4tTYWGhtm/fft7rFBUVqbCw0GUDAKCmoNAHAACVoqysTCNGjNAtt9yi66+/XpLkdDrl4+OjwMBAl76hoaHmIr1Op9OlyC9vL287n6lTpyogIMDcGjdu7ObRAABQdVHoAwCASpGUlKRt27bpnXfeqfBrjRs3TgUFBeZ24MCBCr8mAABVBe/oAwCACpecnKwVK1Zo3bp1uvrqq83jDodDxcXFys/Pd5nVz83NlcPhMPts3LjR5Xzlq/KX9zmbr68vX7MLAKixmNEHAAAVxjAMJScna+nSpVqzZo0iIyNd2tu1a6datWopIyPDPLZr1y7l5OQoJiZGkhQTE6OtW7cqLy/P7JOeni5/f39FR0dXzkAAAKhGmNEHAAAVJikpSUuWLNGHH36o+vXrm+/UBwQEqHbt2goICNCQIUM0atQoBQUFyd/fX8OHD1dMTIw6deokSerRo4eio6M1cOBATZ8+XU6nU+PHj1dSUhKz9gAAnAeFPgAAqDBz586VJN1+++0uxxcsWKCHHnpIkjRz5kx5enqqf//+KioqUlxcnF599VWzr5eXl1asWKHHH39cMTExqlu3rhITE5WSklJZwwAAoFqh0AcAABXGMIzf7ePn56fU1FSlpqZesE9ERIQ++eQTd4YGAIBt8Y4+AAAAAAA2QqEPAAAAAICNUOgDAAAAAGAjFPoAAAAAANgIhT4AAAAAADZCoQ8AAAAAgI3w9XpVRM/efXX4yFFJ0v6cHEVZHA8AAEBFu3/QEB06clySFN6wgd59a77FEQGAPVDoVxGHjxxV1OBpkqS94++zOBoAAICKd+jIcTXqO1qSdHDpixZHAwD2waP7AAAAAADYCDP6qBH27/terTt1kSSFNQxW2vKlFkcEAAAAABWDQh81QonhYb4asfPNsRZHAwAAAAAVh0f3AQAAAACwEQp9AAAAAABshEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARVt0HAACA5XZ/u1O39uonSQpv2EDvvjXf4ogAoPqi0AcAAIDlThueatR3tCTp4NIXLY4GAKo3Ht0HAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAAAAAwEYsLfTXrVun3r17Kzw8XB4eHlq2bJlLu2EYmjhxosLCwlS7dm3FxsZqz549Ln2OHTumhIQE+fv7KzAwUEOGDNGJEycqcRQAAAAAAFQdlhb6J0+eVOvWrZWamnre9unTp2v27NmaN2+esrKyVLduXcXFxenUqVNmn4SEBG3fvl3p6elasWKF1q1bp2HDhlXWEAAAAAAAqFK8rbx4r1691KtXr/O2GYahWbNmafz48brnnnskSW+99ZZCQ0O1bNkyDRgwQDt37lRaWpo2bdqk9u3bS5LmzJmjO++8Uy+99JLCw8MrbSwAAAAAAFQFVfYd/X379snpdCo2NtY8FhAQoI4dOyozM1OSlJmZqcDAQLPIl6TY2Fh5enoqKyvrgucuKipSYWGhywYAAAAAgB1U2ULf6XRKkkJDQ12Oh4aGmm1Op1MhISEu7d7e3goKCjL7nM/UqVMVEBBgbo0bN3Zz9AAAAAAAWMPSR/etMm7cOI0aNcrcLywspNivoXr27qvDR46a+2ENg5W2fKmFEQEAAADAlamyhb7D4ZAk5ebmKiwszDyem5urNm3amH3y8vJcPldSUqJjx46Znz8fX19f+fr6uj9oVDuHjxxV1OBp5v7ON8daGA0AAAAAXLkq++h+ZGSkHA6HMjIyzGOFhYXKyspSTEyMJCkmJkb5+fnKzs42+6xZs0ZlZWXq2LFjpccMAAAAAIDVLJ3RP3HihPbu3Wvu79u3T5s3b1ZQUJCaNGmiESNG6LnnnlPz5s0VGRmpCRMmKDw8XH369JEkRUVFqWfPnho6dKjmzZun06dPKzk5WQMGDGDFfQAA4Bb3DxqiQ0eOS5LCGzbQu2/NtzgiAAB+m6WF/ldffaU77rjD3C9/bz4xMVELFy7UmDFjdPLkSQ0bNkz5+fnq3Lmz0tLS5OfnZ35m8eLFSk5OVrdu3eTp6an+/ftr9uzZlT4WAABgT4eOHFejvqMlSQeXvmhxNAAA/D5LC/3bb79dhmFcsN3Dw0MpKSlKSUm5YJ+goCAtWbKkIsIDAACAhXiaAgAuT5VdjA8AAAA1G09TAMDlqbKL8QEAAAAAgEtHoQ8AAAAAgI1Q6AMAAAAAYCO8ow+cYf++79W6UxdJUljDYKUtX2pxRAAAAABwaSj0gTOUGB6KGjxNkrTzzbEWRwMAAAAAl45H9wEAAAAAsBEKfQAAAAAAbIRCHwAAAAAAG6HQBwAAAADARliMDwAAAFXe7m936tZe/SRJ4Q0b6N235lscEQBUXRT6wAXwVXsAAFQdpw1PNeo7WpJ0cOmLFkcDAFUbhT5wAXzVHgAAAIDqiEIfuAjM7gMAAACoLij0LdKzd18dPnLU3N+fk6MoC+PBb2N2HwAAAEB1QaFvkcNHjpqFoyTtHX+fhdEAAAAAAOyCr9cDAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAAAAAwEYo9AEAAAAAsBG+Xg+4Qj1799XhI0clSWENg5W2fKnFEQEAUHPcP2iIDh05LkkKb9hA77413+KIAMB6FPrAFTp85KiiBk+TJO18c6zF0QAAULMcOnJcjfqOliQdXPqixdEAQNVAoQ8AAABb2P3tTt3aq58kZvcB1GwU+gAAAGc583HwPXv3qpHF8eDinDY8md0HAFHoA261f9/3at2piyTe1weA6uzMx8G3T3nE4mhwOZjdB1CTUegDblRiePC+PgAAVQCz+wBqMr5eDwAAAAAAG2FGH7AAX8kHAAAAoKJQ6AMV5Mz39SXXgp6v5AOAqocF+GqGM/975t19AHZFoQ9UkDPf15co6AGgqmMBvprhzP+eeXcfgF3Z5h391NRUNW3aVH5+furYsaM2btxodUgAAMCNyPVwt/KV+e8fNMTqUADArWwxo//uu+9q1KhRmjdvnjp27KhZs2YpLi5Ou3btUkhIiNXhAb+Jr+QDgN9HrkdFKF+Z/8yZ/TMf7c/Z952aRDaTxGP+AKoXW8zoz5gxQ0OHDtXDDz+s6OhozZs3T3Xq1NGbb75pdWjA7yp/xD9q8DRlbtyk1p26mFvP3n2tDg8AqgRyPSpL+aP9jfqO1k+nSsyfy4t/AKgOqv2MfnFxsbKzszVu3DjzmKenp2JjY5WZmXnezxQVFamoqMjcLygokCQVFhZWaKz97vuTnD8ekyTl/Pe/uvaXk2abUVam0////pk/l5aUmHGVlpSYx939mTN//q1zuPszVoz1Su/PmZ939/05XWro2gcmmp9Jf26Qrm9/syTJcVWQPnhviSTX/y05Dx+UI+x/S0aduX/mZ852oXNc7OcBVJzyfysMw7A4kqqhsnJ9ScnpM/5tLj3vzyUlp81zXKj/+fq44xwVcZ3Lvaa7466I61xK/0uN+6FhSTp8NF+S9N/9+3R100hJUlhwoBa+nqqzlfc/s/3Mc1zoc+c7x8VeE0DVV6H53qjmDh48aEgy1q9f73J89OjRxk033XTezzzzzDOGJDY2NjY2tiq9fffdd5WRSqs8cj0bGxsbm523isj31X5G/3KMGzdOo0aNMvfz8/MVERGhnJwcBQQEWBiZPRQWFqpx48Y6cOCA/P39rQ7HFrin7sX9dD/uqXsVFBSoSZMmCgoKsjqUaotcX7H4/7z7cU/dj3vqXtxP96vIfF/tC/2rrrpKXl5eys3NdTmem5srh8Nx3s/4+vrK19f3nOMBAQH8j9aN/P39uZ9uxj11L+6n+3FP3cvT0xZL6Vwxcn3Vxf/n3Y976n7cU/fifrpfReT7av8bhI+Pj9q1a6eMjAzzWFlZmTIyMhQTE2NhZAAAwB3I9QAAXJpqP6MvSaNGjVJiYqLat2+vm266SbNmzdLJkyf18MMPWx0aAABwA3I9AAAXzxaF/v33368jR45o4sSJcjqdatOmjdLS0hQaGnpRn/f19dUzzzxz3kf8cOm4n+7HPXUv7qf7cU/di/t5LnJ91cL9dD/uqftxT92L++l+FXlPPQyD7+4BAAAAAMAuqv07+gAAAAAA4H8o9AEAAAAAsBEKfQAAAAAAbIRCHwAAAAAAG6nxhX5qaqqaNm0qPz8/dezYURs3brQ6pGph6tSp6tChg+rXr6+QkBD16dNHu3btculz6tQpJSUlKTg4WPXq1VP//v2Vm5trUcTVzwsvvCAPDw+NGDHCPMY9vTQHDx7Ugw8+qODgYNWuXVutWrXSV199ZbYbhqGJEycqLCxMtWvXVmxsrPbs2WNhxFVbaWmpJkyYoMjISNWuXVvNmjXT5MmTdeaartzT37Zu3Tr17t1b4eHh8vDw0LJly1zaL+b+HTt2TAkJCfL391dgYKCGDBmiEydOVOIoqh9y/eUj31cscr17kO/dh1x/5apMrjdqsHfeecfw8fEx3nzzTWP79u3G0KFDjcDAQCM3N9fq0Kq8uLg4Y8GCBca2bduMzZs3G3feeafRpEkT48SJE2afxx57zGjcuLGRkZFhfPXVV0anTp2Mm2++2cKoq4+NGzcaTZs2NW644QbjySefNI9zTy/esWPHjIiICOOhhx4ysrKyjO+//95YuXKlsXfvXrPPCy+8YAQEBBjLli0ztmzZYtx9991GZGSk8csvv1gYedX1/PPPG8HBwcaKFSuMffv2Ge+//75Rr149429/+5vZh3v62z755BPj6aefNj744ANDkrF06VKX9ou5fz179jRat25tbNiwwfj3v/9tXHvttcYDDzxQySOpPsj1V4Z8X3HI9e5Bvncvcv2Vqyq5vkYX+jfddJORlJRk7peWlhrh4eHG1KlTLYyqesrLyzMkGZ9//rlhGIaRn59v1KpVy3j//ffNPjt37jQkGZmZmVaFWS389NNPRvPmzY309HTjtttuM5M/9/TSjB071ujcufMF28vKygyHw2G8+OKL5rH8/HzD19fX+Oc//1kZIVY78fHxxuDBg12O9evXz0hISDAMg3t6qc5O/hdz/3bs2GFIMjZt2mT2+fTTTw0PDw/j4MGDlRZ7dUKudy/yvXuQ692HfO9e5Hr3sjLX19hH94uLi5Wdna3Y2FjzmKenp2JjY5WZmWlhZNVTQUGBJCkoKEiSlJ2drdOnT7vc3xYtWqhJkybc39+RlJSk+Ph4l3sncU8v1UcffaT27dvrj3/8o0JCQtS2bVu98cYbZvu+ffvkdDpd7mdAQIA6duzI/byAm2++WRkZGdq9e7ckacuWLfriiy/Uq1cvSdzTK3Ux9y8zM1OBgYFq37692Sc2Nlaenp7Kysqq9JirOnK9+5Hv3YNc7z7ke/ci11esysz13u4Lu3r58ccfVVpaqtDQUJfjoaGh+vbbby2KqnoqKyvTiBEjdMstt+j666+XJDmdTvn4+CgwMNClb2hoqJxOpwVRVg/vvPOOvv76a23atOmcNu7ppfn+++81d+5cjRo1Sn/961+1adMm/fnPf5aPj48SExPNe3a+fwO4n+f31FNPqbCwUC1atJCXl5dKS0v1/PPPKyEhQZK4p1foYu6f0+lUSEiIS7u3t7eCgoK4x+dBrncv8r17kOvdi3zvXuT6ilWZub7GFvpwn6SkJG3btk1ffPGF1aFUawcOHNCTTz6p9PR0+fn5WR1OtVdWVqb27dtrypQpkqS2bdtq27ZtmjdvnhITEy2Ornp67733tHjxYi1ZskQtW7bU5s2bNWLECIWHh3NPgRqAfH/lyPXuR753L3K9fdTYR/evuuoqeXl5nbOKaW5urhwOh0VRVT/JyclasWKF1q5dq6uvvto87nA4VFxcrPz8fJf+3N8Ly87OVl5enm688UZ5e3vL29tbn3/+uWbPni1vb2+FhoZyTy9BWFiYoqOjXY5FRUUpJydHksx7xr8BF2/06NF66qmnNGDAALVq1UoDBw7UyJEjNXXqVEnc0yt1MffP4XAoLy/Ppb2kpETHjh3jHp8Hud59yPfuQa53P/K9e5HrK1Zl5voaW+j7+PioXbt2ysjIMI+VlZUpIyNDMTExFkZWPRiGoeTkZC1dulRr1qxRZGSkS3u7du1Uq1Ytl/u7a9cu5eTkcH8voFu3btq6das2b95sbu3bt1dCQoL5M/f04t1yyy3nfAXU7t27FRERIUmKjIyUw+FwuZ+FhYXKysrifl7Azz//LE9P17Th5eWlsrIySdzTK3Ux9y8mJkb5+fnKzs42+6xZs0ZlZWXq2LFjpcdc1ZHrrxz53r3I9e5Hvncvcn3FqtRcf6UrCVZn77zzjuHr62ssXLjQ2LFjhzFs2DAjMDDQcDqdVodW5T3++ONGQECA8dlnnxmHDx82t59//tns89hjjxlNmjQx1qxZY3z11VdGTEyMERMTY2HU1c+ZK/EaBvf0UmzcuNHw9vY2nn/+eWPPnj3G4sWLjTp16hhvv/222eeFF14wAgMDjQ8//ND4z3/+Y9xzzz18PcxvSExMNBo1amR+5c4HH3xgXHXVVcaYMWPMPtzT3/bTTz8Z33zzjfHNN98YkowZM2YY33zzjfHDDz8YhnFx969nz55G27ZtjaysLOOLL74wmjdvztfr/QZy/ZUh31c8cv2VId+7F7n+ylWVXF+jC33DMIw5c+YYTZo0MXx8fIybbrrJ2LBhg9UhVQuSzrstWLDA7PPLL78YTzzxhNGgQQOjTp06Rt++fY3Dhw9bF3Q1dHby555emuXLlxvXX3+94evra7Ro0cJ4/fXXXdrLysqMCRMmGKGhoYavr6/RrVs3Y9euXRZFW/UVFhYaTz75pNGkSRPDz8/PuOaaa4ynn37aKCoqMvtwT3/b2rVrz/tvZ2JiomEYF3f/jh49ajzwwANGvXr1DH9/f+Phhx82fvrpJwtGU32Q6y8f+b7ikeuvHPnefcj1V66q5HoPwzCMS37mAAAAAAAAVEk19h19AAAAAADsiEIfAAAAAAAbodAHAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAAAAAwEYo9AEAAAAAsBEKfQBVxkMPPaQ+ffpYHQYAAKhA5Hug4lHoAzWQ1Ql2//798vDw0ObNmy2LAQAAuyPfAzUXhT4AAAAAADZCoQ/AxbZt29SrVy/Vq1dPoaGhGjhwoH788Uez/fbbb9ef//xnjRkzRkFBQXI4HJo0aZLLOb799lt17txZfn5+io6O1urVq+Xh4aFly5ZJkiIjIyVJbdu2lYeHh26//XaXz7/00ksKCwtTcHCwkpKSdPr06YocMgAANQ75HrA3Cn0Apvz8fHXt2lVt27bVV199pbS0NOXm5uq+++5z6bdo0SLVrVtXWVlZmj59ulJSUpSeni5JKi0tVZ8+fVSnTh1lZWXp9ddf19NPP+3y+Y0bN0qSVq9ercOHD+uDDz4w29auXavvvvtOa9eu1aJFi7Rw4UItXLiwYgcOAEANQr4H7M/b6gAAVB2vvPKK2rZtqylTppjH3nzzTTVu3Fi7d+/WH/7wB0nSDTfcoGeeeUaS1Lx5c73yyivKyMhQ9+7dlZ6eru+++06fffaZHA6HJOn5559X9+7dzXM2bNhQkhQcHGz2KdegQQO98sor8vLyUosWLRQfH6+MjAwNHTq0QscOAEBNQb4H7I9CH4Bpy5YtWrt2rerVq3dO23fffeeS+M8UFhamvLw8SdKuXbvUuHFjl4R+0003XXQMLVu2lJeXl8u5t27deknjAAAAF0a+B+yPQh+A6cSJE+rdu7emTZt2TltYWJj5c61atVzaPDw8VFZW5pYYKvLcAACAfA/UBBT6AEw33nij/vWvf6lp06by9r68fx6uu+46HThwQLm5uQoNDZUkbdq0yaWPj4+PpF/f7wMAAJWLfA/YH4vxATVUQUGBNm/e7LINGzZMx44d0wMPPKBNmzbpu+++08qVK/Xwww9fdJLu3r27mjVrpsTERP3nP//Rl19+qfHjx0v69a/1khQSEqLatWubi/8UFBRU2DgBAKjJyPdAzUShD9RQn332mdq2beuyTZ48WV9++aVKS0vVo0cPtWrVSiNGjFBgYKA8PS/unwsvLy8tW7ZMJ06cUIcOHfTII4+Yq/D6+flJkry9vTV79my99tprCg8P1z333FNh4wQAoCYj3wM1k4dhGIbVQQCwty+//FKdO3fW3r171axZM6vDAQAAFYB8D1QdFPoA3G7p0qWqV6+emjdvrr179+rJJ59UgwYN9MUXX1gdGgAAcBPyPVB1sRgfALf76aefNHbsWOXk5Oiqq65SbGysXn75ZavDAgAAbkS+B6ouZvQBAAAAALARFuMDAAAAAMBGKPQBAAAAALARCn0AAAAAAGyEQh8AAAAAABuh0AcAAAAAwEYo9AEAAAAAsBEKfQAAAAAAbIRCHwAAAAAAG/n/ABgc4DMJviepAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_lengths(raw_datasets, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the lengths of the source and target sequences in the dataset.\n",
    "\n",
    "    Args:\n",
    "        raw_datasets (DatasetDict): dictionary containing the raw dataset\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for encoding the sequences\n",
    "    \n",
    "    Returns:\n",
    "        source_lengths (list): list of lengths of source sequences\n",
    "        target_lengths (list): list of lengths of target sequences\n",
    "        max_source_length (int): maximum length of source sequence\n",
    "        max_target_length (int): maximum length of target sequence\n",
    "    \"\"\"\n",
    "    train_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"train\"][\"source\"]]\n",
    "    val_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"validation\"][\"source\"]]\n",
    "    test_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"test\"][\"source\"]]\n",
    "    source_lengths = train_source_lengths + val_source_lengths + test_source_lengths\n",
    "    max_source_length = max(source_lengths)\n",
    "\n",
    "    train_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"train\"][\"target\"]]\n",
    "    val_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"validation\"][\"target\"]]\n",
    "    test_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"test\"][\"target\"]]\n",
    "    target_lengths = train_target_lengths + val_target_lengths + test_target_lengths\n",
    "    max_target_length = max(target_lengths)\n",
    "    \n",
    "    return source_lengths, target_lengths, max_source_length, max_target_length\n",
    "\n",
    "def plot_lengths(source_lengths, target_lengths):\n",
    "    \"\"\"\n",
    "    Plot the distribution of lengths of source and target sequences.\n",
    "\n",
    "    Args:\n",
    "        source_lengths (list): list of lengths of source sequences\n",
    "        target_lengths (list): list of lengths of target sequences\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.histplot(source_lengths, ax=axes[0])\n",
    "    sns.histplot(target_lengths, ax=axes[1])\n",
    "    axes[0].set_title(\"Source Lengths\")\n",
    "    axes[1].set_title(\"Target Lengths\")\n",
    "    axes[0].set_xlabel(\"Length\")\n",
    "    axes[1].set_xlabel(\"Length\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "    axes[0].set_xlim(0, 100)\n",
    "    axes[1].set_xlim(0, 100)\n",
    "\n",
    "# Get distribution of lengths of source across train, val, and test\n",
    "bart_source_lengths, bart_target_lengths, bart_max_source_length, bart_max_target_length = get_lengths(raw_datasets, AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\"))\n",
    "\n",
    "# Plot side by side distributions of source and target lengths\n",
    "plot_lengths(bart_source_lengths, bart_target_lengths)\n",
    "\n",
    "# Check what lengths would be after tokenization with t5\n",
    "t5_source_lengths, t5_target_lengths, t5_max_source_length, t5_max_target_length = get_lengths(raw_datasets, AutoTokenizer.from_pretrained(\"t5-base\"))\n",
    "\n",
    "# Plot side by side distributions of source and target lengths\n",
    "plot_lengths(t5_source_lengths, t5_target_lengths)\n",
    "\n",
    "# Print max lengths\n",
    "print(\"Maximum source length for BART:\", bart_max_source_length)\n",
    "print(\"Maximum target length for BART:\", bart_max_target_length)\n",
    "print(\"Maximum source length for T5:\", t5_max_source_length)\n",
    "print(\"Maximum target length for T5:\", t5_max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "# Toxicity classifier\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"SkolkovoInstitute/roberta_toxicity_classifier\"\n",
    ")\n",
    "\n",
    "# Acceptability classifier\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    # tox_ref = calc_tox_acceptability(refs, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    # sta_ref = (tox_ref == sta_correct_label).sum() / len(tox_ref)\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "    # sta_pct = sta_pred / sta_ref\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    # acc_ref = calc_tox_acceptability(refs, tokenizer_acceptability, model_acceptability)\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "    # acc_pct = acc_pred / acc_ref\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA_preds\": sta_pred,\n",
    "        # \"STA_pct\": sta_pct,\n",
    "        \"Acceptability_preds\": acc_pred,\n",
    "        # \"Acceptability_pct\": acc_pct,\n",
    "        \"BERT_score_f1\": bert_score_f1,\n",
    "        \"Overall\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (DELETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/2_Models.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m detoxified_text_list\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m y_pred_delete \u001b[39m=\u001b[39m baseline_detoxifier(raw_datasets[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m'\u001b[39m\u001b[39msource\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m delete_eval_validation \u001b[39m=\u001b[39m evaluate_metrics(raw_datasets[\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mtarget\u001b[39;49m\u001b[39m'\u001b[39;49m], y_pred_delete)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m delete_eval_validation\n",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/2_Models.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=162'>163</a>\u001b[0m bleu \u001b[39m=\u001b[39m calc_sacrebleu(refs, preds)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=164'>165</a>\u001b[0m \u001b[39m# Calculate toxicity classification\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=165'>166</a>\u001b[0m \u001b[39m# tox_ref = calc_tox_acceptability(refs, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=166'>167</a>\u001b[0m tox_pred \u001b[39m=\u001b[39m calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, output_mean\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=168'>169</a>\u001b[0m \u001b[39m# Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=169'>170</a>\u001b[0m \u001b[39mif\u001b[39;00m to_neutral:\n",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/2_Models.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(data, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)[\u001b[39m\"\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     \u001b[39mif\u001b[39;00m output_score:\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X14sdnNjb2RlLXJlbW90ZQ%3D%3D?line=117'>118</a>\u001b[0m         result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39msoftmax(logits, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)[:, \u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:1198\u001b[0m, in \u001b[0;36mRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1193\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1195\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1198\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroberta(\n\u001b[1;32m   1199\u001b[0m     input_ids,\n\u001b[1;32m   1200\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1201\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1202\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1203\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1204\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1205\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1206\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1207\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1208\u001b[0m )\n\u001b[1;32m   1209\u001b[0m sequence_output \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1210\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(sequence_output)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/roberta/modeling_roberta.py:791\u001b[0m, in \u001b[0;36mRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    790\u001b[0m \u001b[39melif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 791\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m    792\u001b[0m     input_shape \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39msize()\n\u001b[1;32m    793\u001b[0m \u001b[39melif\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3934\u001b[0m, in \u001b[0;36mPreTrainedModel.warn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   3929\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3930\u001b[0m \u001b[39mShows a one-time warning if the input_ids appear to contain padding and no attention mask was given.\u001b[39;00m\n\u001b[1;32m   3931\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   3933\u001b[0m \u001b[39m# Skip the check during tracing.\u001b[39;00m\n\u001b[0;32m-> 3934\u001b[0m \u001b[39mif\u001b[39;00m is_torch_fx_proxy(input_ids) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_tracing() \u001b[39mor\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m   3935\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m   3937\u001b[0m \u001b[39mif\u001b[39;00m (attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mor\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:480\u001b[0m, in \u001b[0;36mis_torchdynamo_compiling\u001b[0;34m()\u001b[0m\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 480\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_dynamo\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mdynamo\u001b[39;00m  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[1;32m    482\u001b[0m     \u001b[39mreturn\u001b[39;00m dynamo\u001b[39m.\u001b[39mis_compiling()\n\u001b[1;32m    483\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/__init__.py:112\u001b[0m\n\u001b[1;32m    107\u001b[0m     allowed_functions\u001b[39m.\u001b[39m_disallowed_function_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(fn))\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m fn\n\u001b[1;32m    111\u001b[0m \u001b[39m@disallow_in_graph\u001b[39;49m\n\u001b[0;32m--> 112\u001b[0m \u001b[39mdef\u001b[39;49;00m \u001b[39mgraph_break\u001b[39;49m():\n\u001b[1;32m    113\u001b[0m \u001b[39m    \u001b[39;49m\u001b[39m\"\"\"Force a graph break\"\"\"\u001b[39;49;00m\n\u001b[1;32m    114\u001b[0m     \u001b[39mpass\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/__init__.py:106\u001b[0m, in \u001b[0;36mdisallow_in_graph\u001b[0;34m(fn)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[39mreturn\u001b[39;00m [disallow_in_graph(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m fn]\n\u001b[1;32m    105\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mcallable\u001b[39m(fn), \u001b[39m\"\u001b[39m\u001b[39mdisallow_in_graph expects a callable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 106\u001b[0m allowed_functions\u001b[39m.\u001b[39;49m_allowed_function_ids\u001b[39m.\u001b[39;49mremove(\u001b[39mid\u001b[39;49m(fn))\n\u001b[1;32m    107\u001b[0m allowed_functions\u001b[39m.\u001b[39m_disallowed_function_ids\u001b[39m.\u001b[39madd(\u001b[39mid\u001b[39m(fn))\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m fn\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:73\u001b[0m, in \u001b[0;36mmake_function_id_set.<locals>.FunctionIdSet.remove\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mremove\u001b[39m(\u001b[39mself\u001b[39m, idx: \u001b[39mint\u001b[39m):\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mif\u001b[39;00m idx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m():\n\u001b[1;32m     74\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_ids\u001b[39m.\u001b[39mremove(idx)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:55\u001b[0m, in \u001b[0;36mmake_function_id_set.<locals>.FunctionIdSet.__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 55\u001b[0m         value \u001b[39m=\u001b[39m lazy_initializer()\n\u001b[1;32m     56\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[1;32m     57\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_ids \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(value\u001b[39m.\u001b[39mkeys())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:185\u001b[0m, in \u001b[0;36m_allowed_function_ids\u001b[0;34m()\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[39melif\u001b[39;00m inspect\u001b[39m.\u001b[39mgetmodule(obj) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_safe_constant(obj):\n\u001b[1;32m    183\u001b[0m                 torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 185\u001b[0m _find_torch_objects(torch)\n\u001b[1;32m    186\u001b[0m _find_torch_objects(math)\n\u001b[1;32m    188\u001b[0m \u001b[39m# torch.Tensor.{fn}\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:179\u001b[0m, in \u001b[0;36m_allowed_function_ids.<locals>._find_torch_objects\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtorch.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m _is_allowed_module_prefix(\n\u001b[1;32m    176\u001b[0m         obj\n\u001b[1;32m    177\u001b[0m     ):\n\u001b[1;32m    178\u001b[0m         torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 179\u001b[0m         _find_torch_objects(obj)\n\u001b[1;32m    180\u001b[0m \u001b[39melif\u001b[39;00m _is_allowed_module_prefix(obj):\n\u001b[1;32m    181\u001b[0m     torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:179\u001b[0m, in \u001b[0;36m_allowed_function_ids.<locals>._find_torch_objects\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[39mif\u001b[39;00m obj\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mtorch.\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m _is_allowed_module_prefix(\n\u001b[1;32m    176\u001b[0m         obj\n\u001b[1;32m    177\u001b[0m     ):\n\u001b[1;32m    178\u001b[0m         torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 179\u001b[0m         _find_torch_objects(obj)\n\u001b[1;32m    180\u001b[0m \u001b[39melif\u001b[39;00m _is_allowed_module_prefix(obj):\n\u001b[1;32m    181\u001b[0m     torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:180\u001b[0m, in \u001b[0;36m_allowed_function_ids.<locals>._find_torch_objects\u001b[0;34m(module)\u001b[0m\n\u001b[1;32m    178\u001b[0m         torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m         _find_torch_objects(obj)\n\u001b[0;32m--> 180\u001b[0m \u001b[39melif\u001b[39;00m _is_allowed_module_prefix(obj):\n\u001b[1;32m    181\u001b[0m     torch_object_ids[\u001b[39mid\u001b[39m(obj)] \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmodule\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    182\u001b[0m \u001b[39melif\u001b[39;00m inspect\u001b[39m.\u001b[39mgetmodule(obj) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_safe_constant(obj):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_dynamo/allowed_functions.py:154\u001b[0m, in \u001b[0;36m_allowed_function_ids.<locals>._is_allowed_module_prefix\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    143\u001b[0m disallowed_modules \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.optim.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    145\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.nn.modules.rnn.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtorch.distributed.fsdp.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    152\u001b[0m )\n\u001b[1;32m    153\u001b[0m allowed_modules_dot \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m([x \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m allowed_modules])\n\u001b[0;32m--> 154\u001b[0m module \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39;49mgetmodule(obj)\n\u001b[1;32m    155\u001b[0m \u001b[39mif\u001b[39;00m module \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/inspect.py:854\u001b[0m, in \u001b[0;36mgetmodule\u001b[0;34m(object, _filename)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[39mif\u001b[39;00m ismodule(\u001b[39mobject\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\n\u001b[0;32m--> 854\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39;49m(\u001b[39mobject\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39m__module__\u001b[39;49m\u001b[39m'\u001b[39;49m):\n\u001b[1;32m    855\u001b[0m     \u001b[39mreturn\u001b[39;00m sys\u001b[39m.\u001b[39mmodules\u001b[39m.\u001b[39mget(\u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m)\n\u001b[1;32m    856\u001b[0m \u001b[39m# Try the filename to modulename cache\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    detoxified_text_list = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        detoxified_text_list.append(text)\n",
    "\n",
    "    return detoxified_text_list\n",
    "\n",
    "y_pred_delete = baseline_detoxifier(raw_datasets[\"validation\"]['source'])\n",
    "delete_eval_validation = evaluate_metrics(raw_datasets[\"validation\"]['target'], y_pred_delete)\n",
    "delete_eval_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (BART Base Detox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_PRED_BART_PICKLE_FILE = \"../data/processed/y_pred_bart.pkl\"\n",
    "\n",
    "if os.path.isfile(Y_PRED_BART_PICKLE_FILE):\n",
    "    # Load predictions from pickle file\n",
    "    with open(Y_PRED_BART_PICKLE_FILE, \"rb\") as f:\n",
    "        y_pred_bart = pickle.load(f)\n",
    "else:\n",
    "    # Create predictions using BART\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "\n",
    "    # Create predictions using BART and show progress using tqdm\n",
    "    y_pred_bart = pipe_bart(raw_datasets[\"validation\"]['source'], max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # Convert to list of strings\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    y_pred_bart[:5]\n",
    "\n",
    "    # Save predictions as a pickle file\n",
    "    with open(Y_PRED_BART_PICKLE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_pred_bart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.7015951162845684,\n",
       " 'STA_preds': 0.9178541492036881,\n",
       " 'Acceptability_preds': 0.71802455,\n",
       " 'BERT_score_f1': 0.9451393306205379,\n",
       " 'Overall': 0.8400934594361843}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate BART predictions\n",
    "bart_eval_validation = evaluate_metrics(raw_datasets[\"validation\"]['target'], y_pred_bart)\n",
    "bart_eval_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to Fine-tune T5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_t5_base = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer_t5_base = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    }
   ],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "prefixed_datasets = add_prefix(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=32,\n",
    "                per_device_eval_batch_size=128,\n",
    "                learning_rate=1e-4,\n",
    "                num_train_epochs=30,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=4,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        model_checkpoint (str): Name of the pre-trained model to use.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Validation/test dataset.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of generated sequences.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        compute_metrics (function): Function to compute evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    GEN_CONFIG_PATH = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(GEN_CONFIG_PATH)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        # save_total_limit=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=GEN_CONFIG_PATH,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "\n",
    "    # Create a partial function with the tokenizer argument included\n",
    "    compute_metrics_with_tokenizer = partial(compute_metrics, tokenizer=tokenizer)\n",
    "    \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_with_tokenizer,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 (Unidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a795db7d01074532b2edc87d668b94da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491cf3462d114614b1ec77505b2baba0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbde97b1ba854a3a90b06770757a38b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    }
   ],
   "source": [
    "prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "tokenized_datasets_t5_small = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a1fbda7d4f489da18dd50b38397952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c329bc0030b94a9985c49c21d250fe68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed57ecf3a3c54a79ae679f09db743c14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231031_141736-al18titv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/al18titv' target=\"_blank\">t5-small-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/al18titv' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/al18titv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2688' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2688/10080 10:40 < 29:23, 4.19 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>1.002992</td>\n",
       "      <td>0.577241</td>\n",
       "      <td>0.743504</td>\n",
       "      <td>0.678014</td>\n",
       "      <td>0.921097</td>\n",
       "      <td>0.732672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.062000</td>\n",
       "      <td>0.954667</td>\n",
       "      <td>0.592385</td>\n",
       "      <td>0.821459</td>\n",
       "      <td>0.688148</td>\n",
       "      <td>0.924167</td>\n",
       "      <td>0.769524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.995800</td>\n",
       "      <td>0.939699</td>\n",
       "      <td>0.598228</td>\n",
       "      <td>0.842414</td>\n",
       "      <td>0.690134</td>\n",
       "      <td>0.925105</td>\n",
       "      <td>0.779659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.948300</td>\n",
       "      <td>0.923069</td>\n",
       "      <td>0.597996</td>\n",
       "      <td>0.854987</td>\n",
       "      <td>0.694469</td>\n",
       "      <td>0.924770</td>\n",
       "      <td>0.785442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.913600</td>\n",
       "      <td>0.915044</td>\n",
       "      <td>0.601244</td>\n",
       "      <td>0.865046</td>\n",
       "      <td>0.701344</td>\n",
       "      <td>0.925471</td>\n",
       "      <td>0.791630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.882700</td>\n",
       "      <td>0.918080</td>\n",
       "      <td>0.606649</td>\n",
       "      <td>0.891031</td>\n",
       "      <td>0.699932</td>\n",
       "      <td>0.925584</td>\n",
       "      <td>0.802845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.855500</td>\n",
       "      <td>0.922288</td>\n",
       "      <td>0.600217</td>\n",
       "      <td>0.873428</td>\n",
       "      <td>0.698467</td>\n",
       "      <td>0.926352</td>\n",
       "      <td>0.794379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.829700</td>\n",
       "      <td>0.919788</td>\n",
       "      <td>0.601931</td>\n",
       "      <td>0.894384</td>\n",
       "      <td>0.696203</td>\n",
       "      <td>0.925716</td>\n",
       "      <td>0.802524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td></td></tr><tr><td>eval/BERT_score_f1</td><td></td></tr><tr><td>eval/BLEU</td><td></td></tr><tr><td>eval/Overall</td><td></td></tr><tr><td>eval/STA_preds</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.6962</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92572</td></tr><tr><td>eval/BLEU</td><td>0.60193</td></tr><tr><td>eval/Overall</td><td>0.80252</td></tr><tr><td>eval/STA_preds</td><td>0.89438</td></tr><tr><td>eval/loss</td><td>0.91979</td></tr><tr><td>eval/runtime</td><td>46.1381</td></tr><tr><td>eval/samples_per_second</td><td>25.857</td></tr><tr><td>eval/steps_per_second</td><td>0.217</td></tr><tr><td>train/epoch</td><td>8.0</td></tr><tr><td>train/global_step</td><td>2688</td></tr><tr><td>train/learning_rate</td><td>7e-05</td></tr><tr><td>train/loss</td><td>0.8297</td></tr><tr><td>train/total_flos</td><td>819807057149952.0</td></tr><tr><td>train/train_loss</td><td>0.96719</td></tr><tr><td>train/train_runtime</td><td>640.8923</td></tr><tr><td>train/train_samples_per_second</td><td>502.409</td></tr><tr><td>train/train_steps_per_second</td><td>15.728</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/al18titv' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/al18titv</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231031_141736-al18titv/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-detoxify\")\n",
    "trainer_t5_small.train() # General rule is 10% number of epochs\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate difference between checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, overall scores are maximized even as validation loss increases (see t5-small-detoxify). We can see that the main aspect that improves is style transfer accuracy (STA), but also acceptability. Next, we check samples of text generated at the checkpoint in which validation loss is minimized vs. the checkpoint where the overall score is maximized and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint paths\n",
    "CHECKPOINT_T5_SMALL_MINLOSS = \"../models/t5-small-detoxify/checkpoint-1680\" # Epoch 5, min loss\n",
    "CHECKPOINT_T5_SMALL_BEST = \"../models/t5-small-detoxify/checkpoint-2016\" # Epoch 15, best overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# The trainer object for t5-small-detoxify is already the best model, so we can just load it\n",
    "trainer_t5_small_best = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=CHECKPOINT_T5_SMALL_BEST,\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")\n",
    "\n",
    "# Load the trainer object for t5-small-detoxify with the minimum loss\n",
    "trainer_t5_small_minloss = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=CHECKPOINT_T5_SMALL_MINLOSS,\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions from trainer objects\n",
    "def get_preds_df (trainer_1=trainer_t5_small_best,\n",
    "                  trainer_2=trainer_t5_small_minloss,\n",
    "                  tokenizer=tokenizer_t5_small,\n",
    "                  tokenized_data=tokenized_datasets_t5_small[\"validation\"],\n",
    "                  raw_data=raw_datasets[\"validation\"]):\n",
    "    \"\"\"\n",
    "    Gets the predictions from a trainer object and returns a dataframe containing the source, target, and prediction.\n",
    "\n",
    "    Args:\n",
    "        trainer_object (Seq2SeqTrainer): trainer object containing the model\n",
    "        tokenized_data (Dataset): dataset to be evaluated\n",
    "        raw_data (Dataset): dataset containing the raw data\n",
    "\n",
    "    Returns:\n",
    "        preds (Seq2SeqPrediction): Seq2SeqPrediction object containing the predictions\n",
    "        decoded_preds (list): list of decoded predictions\n",
    "        df (DataFrame): dataframe containing the source, target, and prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    preds_1 = trainer_1.predict(tokenized_data)\n",
    "    decoded_preds_1 = tokenizer.batch_decode(preds_1.predictions, skip_special_tokens=True)\n",
    "    preds_2 = trainer_2.predict(tokenized_data)\n",
    "    decoded_preds_2 = tokenizer.batch_decode(preds_2.predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Create dataframe containing source, target, and prediction\n",
    "    df = pd.DataFrame()\n",
    "    df[\"source\"] = raw_data[\"source\"]\n",
    "    df[\"target\"] = raw_data[\"target\"]\n",
    "    df[\"pred_1\"] = decoded_preds_1\n",
    "    df[\"pred_2\"] = decoded_preds_2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Get predictions for t5-small-detoxify\n",
    "t5_small_df = get_preds_df(trainer_1=trainer_t5_small_best,\n",
    "                                trainer_2=trainer_t5_small_minloss,\n",
    "                                tokenizer=tokenizer_t5_small,\n",
    "                                tokenized_data=tokenized_datasets_t5_small[\"validation\"],\n",
    "                                raw_data=raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 62\n",
      "Source: Spank them and send them back to california.\n",
      "Target: Scold them and send them back to california\n",
      "Prediction (Best model): Send them back to california\n",
      "Prediction (Min val error): Spank them and send them back to California.\n",
      "Diff in toxicity: -0.994\n",
      "Diff in acceptability: -0.006\n",
      "Diff in BERT: -0.002\n",
      "\n",
      "Index: 84\n",
      "Source: Its the adults who are screaming and carrying on like idiots.\n",
      "Target: Its the adults who are screaming and carrying on\n",
      "Prediction (Best model): It's the adults who are screaming and carrying on.\n",
      "Prediction (Min val error): Its the adults who are screaming and carrying on like idiots.\n",
      "Diff in toxicity: -0.999\n",
      "Diff in acceptability: 0.699\n",
      "Diff in BERT: -0.013\n",
      "\n",
      "Index: 132\n",
      "Source: All canadians are paying for what the left wing voted in, many canadians were not so stupid.\n",
      "Target: All canadians are paying for what the left wing voted in, many canadians had common sense.\n",
      "Prediction (Best model): All canadians are paying for what the left wing voted in, many canadians were not so smart.\n",
      "Prediction (Min val error): All canadians are paying for what the left wing voted in, many canadians were not so stupid.\n",
      "Diff in toxicity: -0.928\n",
      "Diff in acceptability: -0.195\n",
      "Diff in BERT: -0.001\n",
      "\n",
      "Index: 142\n",
      "Source: To be fair obama is just doing what any corrupt republican piece of shit would do.\n",
      "Target: To be fair obama is just doing what any corrupt republican would do\n",
      "Prediction (Best model): To be fair Obama is just doing what any corrupt republican would do.\n",
      "Prediction (Min val error): To be fair obama is just doing what any corrupt republican would do.\n",
      "Diff in toxicity: -0.544\n",
      "Diff in acceptability: 0.1\n",
      "Diff in BERT: 0.0\n",
      "\n",
      "Index: 144\n",
      "Source: Never take any responsibility and then post silly comments blaming liberals.\n",
      "Target: Never take any responsibility and then post bad comments blaming liberals\n",
      "Prediction (Best model): Never take any responsibility and then post comments blaming liberals.\n",
      "Prediction (Min val error): Never take any responsibility and then post silly comments blaming liberals.\n",
      "Diff in toxicity: -0.951\n",
      "Diff in acceptability: 0.001\n",
      "Diff in BERT: -0.002\n",
      "\n",
      "Index: 237\n",
      "Source: Yes yung n i feel stupid ova here tryin keep up wiff yuh know i juss made dis shit\n",
      "Target: Yes yung and i feel naive ova here tryin keep up yuh know i just made this\n",
      "Prediction (Best model): Yes yung n i feel bad ova here tryin keep up wiff yuh know i juss made dis\n",
      "Prediction (Min val error): Yes yung n i feel bad ova here tryin keep up wiff yuh know i juss made dis shit\n",
      "Diff in toxicity: -0.994\n",
      "Diff in acceptability: 0.035\n",
      "Diff in BERT: -0.004\n",
      "\n",
      "Index: 251\n",
      "Source: Are you suggesting that denver is as corrupt as brazil and greece?\n",
      "Target: Are you implying that Denver, like Brazil and Greece, is corrupt?\n",
      "Prediction (Best model): Are you suggesting that denver is corrupt as brazil and greece?\n",
      "Prediction (Min val error): Are you suggesting that denver is as corrupt as Brazil and greece?\n",
      "Diff in toxicity: -0.574\n",
      "Diff in acceptability: -0.395\n",
      "Diff in BERT: 0.004\n",
      "\n",
      "Index: 253\n",
      "Source: Would u want a nigga to act like a bitch lol?\n",
      "Target: Would you want a person to act like that lol?\n",
      "Prediction (Best model): Would you want a man to act like a bad person lol?\n",
      "Prediction (Min val error): Would you want a nigga to act like a bad person lol?\n",
      "Diff in toxicity: -0.991\n",
      "Diff in acceptability: 0.014\n",
      "Diff in BERT: 0.02\n",
      "\n",
      "Index: 285\n",
      "Source: President obama was a fool, or a liar.\n",
      "Target: President Obama was not suitable\n",
      "Prediction (Best model): President Obama was not good.\n",
      "Prediction (Min val error): President Obama was a liar\n",
      "Diff in toxicity: -0.991\n",
      "Diff in acceptability: -0.006\n",
      "Diff in BERT: 0.032\n",
      "\n",
      "Index: 291\n",
      "Source: Since you are taught from a book that was written by a moron you become one.\n",
      "Target: You are what you learn from.\n",
      "Prediction (Best model): Since you are taught from a book that was written by a writer you become one.\n",
      "Prediction (Min val error): Since you are taught from a book that was written by a moron you become one.\n",
      "Diff in toxicity: -0.999\n",
      "Diff in acceptability: -0.413\n",
      "Diff in BERT: 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Calculate relevant metric columns for t5_small_df\n",
    "# t5_small_df[\"pred_1_BLEURT\"] = calc_bleurt(t5_small_df[\"target\"], t5_small_df[\"pred_1\"], output_mean=False)\n",
    "# t5_small_df[\"pred_2_BLEURT\"] = calc_bleurt(t5_small_df[\"target\"], t5_small_df[\"pred_2\"], output_mean=False)\n",
    "t5_small_df[\"pred_1_toxic_class\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "t5_small_df[\"pred_2_toxic_class\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "t5_small_df[\"pred_1_toxic_score\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_2_toxic_score\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=True, output_mean=False)\n",
    "t5_small_df[\"source_acceptability\"] = calc_tox_acceptability(t5_small_df[\"source\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"target_acceptability\"] = calc_tox_acceptability(t5_small_df[\"target\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_1_acceptability\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_2_acceptability\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_1_BERT_score\"] = calc_bert_score(t5_small_df[\"target\"], t5_small_df[\"pred_1\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "t5_small_df[\"pred_2_BERT_score\"] = calc_bert_score(t5_small_df[\"target\"], t5_small_df[\"pred_2\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "# Calculate differences in BLEURT, toxicity, acceptability, and BERT score\n",
    "t5_small_df[\"diff_toxic_score\"] = t5_small_df[\"pred_1_toxic_score\"] - t5_small_df[\"pred_2_toxic_score\"]\n",
    "# t5_small_df[\"diff_BLEURT\"] = t5_small_df[\"pred_1_BLEURT\"] - t5_small_df[\"pred_2_BLEURT\"]\n",
    "t5_small_df[\"diff_acceptability\"] = t5_small_df[\"pred_1_acceptability\"] - t5_small_df[\"pred_2_acceptability\"]\n",
    "t5_small_df[\"diff_BERT_score\"] = t5_small_df[\"pred_1_BERT_score\"] - t5_small_df[\"pred_2_BERT_score\"]\n",
    "\n",
    "# Filter to rows where pred_1_toxicity is less than pred_2_toxicity\n",
    "t5_small_df_filtered = t5_small_df[t5_small_df[\"pred_1_toxic_class\"] < t5_small_df[\"pred_2_toxic_class\"]]\n",
    "\n",
    "# Print as individual lines\n",
    "for index, row in t5_small_df_filtered.head(10).iterrows():\n",
    "    print(\"Index:\", index)\n",
    "    print(\"Source:\", row[\"source\"])\n",
    "    print(\"Target:\", row[\"target\"])\n",
    "    print(\"Prediction (Best model):\", row[\"pred_1\"])\n",
    "    print(\"Prediction (Min val error):\", row[\"pred_2\"])\n",
    "    print(\"Diff in toxicity:\", round(row[\"diff_toxic_score\"], 3))\n",
    "    # print(\"Diff in BLEURT:\", round(row[\"diff_BLEURT\"], 3))\n",
    "    print(\"Diff in acceptability:\", round(row[\"diff_acceptability\"], 3))\n",
    "    print(\"Diff in BERT:\", round(row[\"diff_BERT_score\"], 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this sample, it looks like the predictions from the best model are indeed better at detoxifying text (exmple 108, 211) and is sufficiently different from the target text that there is no indication of overfitting per se. So going forward we prioritize selecting best models based on the overall score as opposed to minimizing validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1191ba68562e407dab33d14a565c390f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c1bf702dfe4e15a57b0b47b734aa4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db2427e5b84430c884781b06fca7733",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231031_144310-t3fjhu9t</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/t3fjhu9t' target=\"_blank\">t5-base-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/t3fjhu9t' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/t3fjhu9t</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2352' max='10080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2352/10080 16:38 < 54:44, 2.35 it/s, Epoch 7/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.812222</td>\n",
       "      <td>0.608180</td>\n",
       "      <td>0.890193</td>\n",
       "      <td>0.723307</td>\n",
       "      <td>0.925496</td>\n",
       "      <td>0.807474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.798929</td>\n",
       "      <td>0.601930</td>\n",
       "      <td>0.883487</td>\n",
       "      <td>0.709482</td>\n",
       "      <td>0.926611</td>\n",
       "      <td>0.800999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.798679</td>\n",
       "      <td>0.608734</td>\n",
       "      <td>0.906119</td>\n",
       "      <td>0.720927</td>\n",
       "      <td>0.926683</td>\n",
       "      <td>0.813716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.807012</td>\n",
       "      <td>0.605299</td>\n",
       "      <td>0.899413</td>\n",
       "      <td>0.716527</td>\n",
       "      <td>0.927316</td>\n",
       "      <td>0.809594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.830957</td>\n",
       "      <td>0.609381</td>\n",
       "      <td>0.911148</td>\n",
       "      <td>0.716312</td>\n",
       "      <td>0.927588</td>\n",
       "      <td>0.815116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.846663</td>\n",
       "      <td>0.604496</td>\n",
       "      <td>0.909472</td>\n",
       "      <td>0.720942</td>\n",
       "      <td>0.926022</td>\n",
       "      <td>0.814081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.883831</td>\n",
       "      <td>0.602264</td>\n",
       "      <td>0.907795</td>\n",
       "      <td>0.720766</td>\n",
       "      <td>0.926995</td>\n",
       "      <td>0.813123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f75799223e64fa18cc577d2a4e2aeab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.460180"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td></td></tr><tr><td>eval/BERT_score_f1</td><td></td></tr><tr><td>eval/BLEU</td><td></td></tr><tr><td>eval/Overall</td><td></td></tr><tr><td>eval/STA_preds</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.72077</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92699</td></tr><tr><td>eval/BLEU</td><td>0.60226</td></tr><tr><td>eval/Overall</td><td>0.81312</td></tr><tr><td>eval/STA_preds</td><td>0.9078</td></tr><tr><td>eval/loss</td><td>0.88383</td></tr><tr><td>eval/runtime</td><td>54.183</td></tr><tr><td>eval/samples_per_second</td><td>22.018</td></tr><tr><td>eval/steps_per_second</td><td>0.185</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>2352</td></tr><tr><td>train/learning_rate</td><td>8e-05</td></tr><tr><td>train/loss</td><td>nan</td></tr><tr><td>train/total_flos</td><td>3225777208980480.0</td></tr><tr><td>train/train_loss</td><td>nan</td></tr><tr><td>train/train_runtime</td><td>998.9733</td></tr><tr><td>train/train_samples_per_second</td><td>322.321</td></tr><tr><td>train/train_steps_per_second</td><td>10.09</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-base-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/t3fjhu9t' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/t3fjhu9t</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231031_144310-t3fjhu9t/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_t5_base = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_base},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_base = setup_trainer(\n",
    "    output_dir_name=\"t5-base-detoxify\",\n",
    "    model_checkpoint=\"t5-base\",\n",
    "    train_dataset=tokenized_datasets_t5_base[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_base[\"validation\"],\n",
    ")\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-base-detoxify\")\n",
    "trainer_t5_base.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like t5-small and t5-base performs similarly, but t5-base reaches convergence much earlier. There's also indication of overfitting occurring much earlier (e.g., by epoch 5 validation loss jumps up significantly) without overall performance improving. We will use t5-small going forward given computational constraints. Similar performance may indicate that the task at hand is not too complex and the extra complexity of t5-base is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Bi-directional, No custom loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_dataset(datasets):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    return extended_datasets\n",
    "\n",
    "def get_indices(dataset):\n",
    "    \"\"\"\n",
    "    Saves the indices of data that is to_neutral and to_toxic.\n",
    "    \"\"\"\n",
    "    to_neutral_idx = []\n",
    "    to_toxic_idx = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][\"source\"].startswith(\"to_neutral\"):\n",
    "            to_neutral_idx.append(i)\n",
    "        else:\n",
    "            to_toxic_idx.append(i)\n",
    "\n",
    "    return to_neutral_idx, to_toxic_idx\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset created using create_bidirectional_datasets e.g., raw_datasets_bd[\"validation\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = get_indices(bd_dataset)[0]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        to_neutral=True\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial without shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fef4d83793441eb871e9601ef19f3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da10ea62684d4886a9fbda8f79670e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75ebc2b2b7064bcea1570a2f8ef85ee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231031_150013-363z1wxk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/363z1wxk' target=\"_blank\">t5-small-bd-noshuffle-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/363z1wxk' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/363z1wxk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5368' max='20130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5368/20130 20:05 < 55:17, 4.45 it/s, Epoch 8/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.581200</td>\n",
       "      <td>1.297624</td>\n",
       "      <td>0.582431</td>\n",
       "      <td>0.710813</td>\n",
       "      <td>0.678005</td>\n",
       "      <td>0.919003</td>\n",
       "      <td>0.720213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.343000</td>\n",
       "      <td>1.220252</td>\n",
       "      <td>0.595135</td>\n",
       "      <td>0.795474</td>\n",
       "      <td>0.682763</td>\n",
       "      <td>0.922740</td>\n",
       "      <td>0.758317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.257800</td>\n",
       "      <td>1.186172</td>\n",
       "      <td>0.601159</td>\n",
       "      <td>0.863370</td>\n",
       "      <td>0.692295</td>\n",
       "      <td>0.924668</td>\n",
       "      <td>0.788972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.203900</td>\n",
       "      <td>1.174466</td>\n",
       "      <td>0.602681</td>\n",
       "      <td>0.865884</td>\n",
       "      <td>0.687537</td>\n",
       "      <td>0.924235</td>\n",
       "      <td>0.789244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.166600</td>\n",
       "      <td>1.154176</td>\n",
       "      <td>0.601517</td>\n",
       "      <td>0.883487</td>\n",
       "      <td>0.702484</td>\n",
       "      <td>0.925840</td>\n",
       "      <td>0.799363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.131100</td>\n",
       "      <td>1.153290</td>\n",
       "      <td>0.607493</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.701396</td>\n",
       "      <td>0.925796</td>\n",
       "      <td>0.807038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.104700</td>\n",
       "      <td>1.140285</td>\n",
       "      <td>0.607121</td>\n",
       "      <td>0.891869</td>\n",
       "      <td>0.704214</td>\n",
       "      <td>0.926410</td>\n",
       "      <td>0.804297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.079200</td>\n",
       "      <td>1.136118</td>\n",
       "      <td>0.606703</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>0.697141</td>\n",
       "      <td>0.927136</td>\n",
       "      <td>0.803614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca1d741358c4296bcdd98b260d89db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.124705"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td></td></tr><tr><td>eval/BERT_score_f1</td><td></td></tr><tr><td>eval/BLEU</td><td></td></tr><tr><td>eval/Overall</td><td></td></tr><tr><td>eval/STA_preds</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.69714</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92714</td></tr><tr><td>eval/BLEU</td><td>0.6067</td></tr><tr><td>eval/Overall</td><td>0.80361</td></tr><tr><td>eval/STA_preds</td><td>0.89355</td></tr><tr><td>eval/loss</td><td>1.13612</td></tr><tr><td>eval/runtime</td><td>91.2959</td></tr><tr><td>eval/samples_per_second</td><td>26.135</td></tr><tr><td>eval/steps_per_second</td><td>0.208</td></tr><tr><td>train/epoch</td><td>8.0</td></tr><tr><td>train/global_step</td><td>5368</td></tr><tr><td>train/learning_rate</td><td>7e-05</td></tr><tr><td>train/loss</td><td>1.0792</td></tr><tr><td>train/total_flos</td><td>1560059568979968.0</td></tr><tr><td>train/train_loss</td><td>1.23342</td></tr><tr><td>train/train_runtime</td><td>1205.9476</td></tr><tr><td>train/train_samples_per_second</td><td>534.003</td></tr><tr><td>train/train_steps_per_second</td><td>16.692</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-bd-noshuffle-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/363z1wxk' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/363z1wxk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231031_150013-363z1wxk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd = create_bidirectional_dataset(raw_datasets)\n",
    "\n",
    "tokenized_datasets_bd_t5_small = raw_datasets_bd.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_bd = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-bd-noshuffle\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_bd_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_bd_t5_small[\"validation\"],\n",
    "    compute_metrics=partial(compute_metrics_bd, bd_dataset=raw_datasets_bd[\"validation\"], shuffled_data=False)\n",
    "    )\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-bd-noshuffle-detoxify\")\n",
    "trainer_t5_small_bd.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial with shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7522a4a520314d499f8f2a78d2bac53a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e71fb22f2c94a2c9e0b49d5363044ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf5d14488af45d291105879a741bfca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231031_161940-gyjcydqs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/gyjcydqs' target=\"_blank\">t5-small-bd-shuffle-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/gyjcydqs' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/gyjcydqs</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6039' max='20130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 6039/20130 22:04 < 51:32, 4.56 it/s, Epoch 9/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.582300</td>\n",
       "      <td>1.291271</td>\n",
       "      <td>0.582313</td>\n",
       "      <td>0.698240</td>\n",
       "      <td>0.689065</td>\n",
       "      <td>0.918604</td>\n",
       "      <td>0.717292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.340700</td>\n",
       "      <td>1.219955</td>\n",
       "      <td>0.596120</td>\n",
       "      <td>0.811400</td>\n",
       "      <td>0.687961</td>\n",
       "      <td>0.922587</td>\n",
       "      <td>0.765893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.256100</td>\n",
       "      <td>1.183220</td>\n",
       "      <td>0.604068</td>\n",
       "      <td>0.852473</td>\n",
       "      <td>0.694322</td>\n",
       "      <td>0.924519</td>\n",
       "      <td>0.785571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.204600</td>\n",
       "      <td>1.167809</td>\n",
       "      <td>0.602693</td>\n",
       "      <td>0.865884</td>\n",
       "      <td>0.698694</td>\n",
       "      <td>0.924281</td>\n",
       "      <td>0.791487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.161100</td>\n",
       "      <td>1.155576</td>\n",
       "      <td>0.602983</td>\n",
       "      <td>0.865046</td>\n",
       "      <td>0.691605</td>\n",
       "      <td>0.925625</td>\n",
       "      <td>0.790061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.128800</td>\n",
       "      <td>1.152056</td>\n",
       "      <td>0.603047</td>\n",
       "      <td>0.885163</td>\n",
       "      <td>0.707908</td>\n",
       "      <td>0.925451</td>\n",
       "      <td>0.801347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.099900</td>\n",
       "      <td>1.140844</td>\n",
       "      <td>0.606081</td>\n",
       "      <td>0.906957</td>\n",
       "      <td>0.707975</td>\n",
       "      <td>0.925890</td>\n",
       "      <td>0.810772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.075200</td>\n",
       "      <td>1.137721</td>\n",
       "      <td>0.605489</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.701951</td>\n",
       "      <td>0.926568</td>\n",
       "      <td>0.807237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.053600</td>\n",
       "      <td>1.131585</td>\n",
       "      <td>0.607370</td>\n",
       "      <td>0.904443</td>\n",
       "      <td>0.704556</td>\n",
       "      <td>0.926540</td>\n",
       "      <td>0.809470</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eee3cba215745f18ed2420ab8845757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.033 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td></td></tr><tr><td>eval/BERT_score_f1</td><td></td></tr><tr><td>eval/BLEU</td><td></td></tr><tr><td>eval/Overall</td><td></td></tr><tr><td>eval/STA_preds</td><td></td></tr><tr><td>eval/loss</td><td></td></tr><tr><td>eval/runtime</td><td></td></tr><tr><td>eval/samples_per_second</td><td></td></tr><tr><td>eval/steps_per_second</td><td></td></tr><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr><tr><td>train/learning_rate</td><td></td></tr><tr><td>train/loss</td><td></td></tr><tr><td>train/total_flos</td><td></td></tr><tr><td>train/train_loss</td><td></td></tr><tr><td>train/train_runtime</td><td></td></tr><tr><td>train/train_samples_per_second</td><td></td></tr><tr><td>train/train_steps_per_second</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.70456</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92654</td></tr><tr><td>eval/BLEU</td><td>0.60737</td></tr><tr><td>eval/Overall</td><td>0.80947</td></tr><tr><td>eval/STA_preds</td><td>0.90444</td></tr><tr><td>eval/loss</td><td>1.13158</td></tr><tr><td>eval/runtime</td><td>89.1448</td></tr><tr><td>eval/samples_per_second</td><td>26.765</td></tr><tr><td>eval/steps_per_second</td><td>0.213</td></tr><tr><td>train/epoch</td><td>9.0</td></tr><tr><td>train/global_step</td><td>6039</td></tr><tr><td>train/learning_rate</td><td>7e-05</td></tr><tr><td>train/loss</td><td>1.0536</td></tr><tr><td>train/total_flos</td><td>1753284838490112.0</td></tr><tr><td>train/train_loss</td><td>1.21135</td></tr><tr><td>train/train_runtime</td><td>1325.0712</td></tr><tr><td>train/train_samples_per_second</td><td>485.997</td></tr><tr><td>train/train_steps_per_second</td><td>15.192</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-bd-shuffle-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/gyjcydqs' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/gyjcydqs</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231031_161940-gyjcydqs/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd_shuffle = raw_datasets_bd.copy()\n",
    "raw_datasets_bd_shuffle[\"train\"] = raw_datasets_bd_shuffle[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "raw_datasets_bd_shuffle = DatasetDict(raw_datasets_bd_shuffle)\n",
    "\n",
    "tokenized_datasets_bd_shuffle_t5_small = raw_datasets_bd_shuffle.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_bd_shuffle = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-bd-shuffle\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_bd_shuffle_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_bd_shuffle_t5_small[\"validation\"],\n",
    "    compute_metrics=partial(compute_metrics_bd, bd_dataset=raw_datasets_bd[\"validation\"], shuffled_data=True)\n",
    "    )\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-bd-shuffle-detoxify\")\n",
    "trainer_t5_small_bd_shuffle.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check that there isn't a bug in sta_pct (seems odd that it's so high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Bi-directional, with Custom Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for original loss function:\n",
    "\n",
    "https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer.html#Trainer.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_loss(self, model, inputs):\n",
    "#         \"\"\"\n",
    "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "#         Subclass and override for custom behavior.\n",
    "#         \"\"\"\n",
    "#         outputs = model(**inputs)\n",
    "#         # Save past state if it exists\n",
    "#         # TODO: this needs to be fixed and made cleaner later.\n",
    "#         if self.args.past_index >= 0:\n",
    "#             self._past = outputs[self.args.past_index]\n",
    "\n",
    "#         if self.label_smoother is not None and \"labels\" in inputs:\n",
    "#             return self.label_smoother(outputs, inputs[\"labels\"])\n",
    "#         else:\n",
    "#             # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "#             return outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7442, device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded for this optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/2_Models.ipynb Cell 70\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning-1-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning-1-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m trainer_t5_small_cl\u001b[39m=\u001b[39m setup_trainer()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning-1-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m trainer_t5_small_cl\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1963\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m   1961\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1962\u001b[0m         \u001b[39m# tpu-comment: accelerate wrapped optimizers call xm.optimizer_step\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1964\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m   1965\u001b[0m     scale_before \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/optimizer.py:132\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, closure)\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:368\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m OptState\u001b[39m.\u001b[39mREADY:\n\u001b[1;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m--> 368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    372\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n",
      "\u001b[0;31mAssertionError\u001b[0m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "# class Seq2SeqTrainerCustomLoss(Seq2SeqTrainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         \"\"\"\n",
    "#         Compute custom loss for the model.\n",
    "\n",
    "#         Args:\n",
    "#             model (torch.nn.Module): The model training or evaluating.\n",
    "#             inputs (dict): The inputs and targets of the model.\n",
    "#             return_outputs (bool): Whether to return model outputs.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.FloatTensor: The loss value.\n",
    "#         \"\"\"\n",
    "#         # Call prediction_step\n",
    "#         loss, encoded_y_pred, encoded_y_test = self.prediction_step(model, inputs, prediction_loss_only=False)\n",
    "    \n",
    "#         # Decode the generated tokens\n",
    "#         if isinstance(encoded_y_pred, tuple):\n",
    "#             encoded_y_pred = encoded_y_pred[0]\n",
    "#         decoded_y_pred = self.tokenizer.batch_decode(encoded_y_pred, skip_special_tokens=True)\n",
    "\n",
    "#         # Decode the labels\n",
    "#         ## Replace -100s in the labels as we can't decode them\n",
    "#         encoded_y_test = torch.where(encoded_y_test != -100, encoded_y_test, torch.tensor(self.tokenizer.pad_token_id))\n",
    "#         decoded_y_test = self.tokenizer.batch_decode(encoded_y_test, skip_special_tokens=True)\n",
    "\n",
    "#         # Some simple post-processing\n",
    "#         decoded_y_pred = [pred.strip() for pred in decoded_y_pred]\n",
    "#         decoded_y_test = [label.strip() for label in decoded_y_test]\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         # TO DO: MAKE THIS WORK WITH PYTORCH LIGHTNING\n",
    "#         composite_score = evaluate_metrics(\n",
    "#             decoded_y_test,\n",
    "#             decoded_y_pred,\n",
    "#             tokenizer_toxicity=tokenizer_toxicity,\n",
    "#             model_toxicity=model_toxicity,\n",
    "#             tokenizer_acceptability=tokenizer_acceptability,\n",
    "#             model_acceptability=model_acceptability,\n",
    "#             include_bleurt=False\n",
    "#         )['Overall']\n",
    "\n",
    "#         # # Composite score will be on a scale of 0 - 1, so we can invert it to get the loss\n",
    "#         custom_loss = torch.tensor(1 - composite_score, dtype=torch.float, requires_grad=True, device=self.args.device)\n",
    "\n",
    "#         print(custom_loss)\n",
    "#         return custom_loss\n",
    "    \n",
    "# def setup_trainer(model_name_t5=\"t5-small\",\n",
    "#                 per_device_train_batch_size=64,\n",
    "#                 per_device_eval_batch_size=64,\n",
    "#                 learning_rate=3e-4,\n",
    "#                 weight_decay=0.01,\n",
    "#                 num_train_epochs=10,\n",
    "#                 max_length=50,\n",
    "#                 num_beams=4):\n",
    "    \n",
    "#     # Define generation config\n",
    "#     generation_config = GenerationConfig(\n",
    "#         max_length=max_length,\n",
    "#         num_beams=num_beams,\n",
    "#         eos_token_id=tokenizer_t5.eos_token_id,\n",
    "#         bos_token_id=tokenizer_t5.bos_token_id,\n",
    "#         pad_token_id=tokenizer_t5.pad_token_id,\n",
    "#         decoder_start_token_id=tokenizer_t5.pad_token_id)\n",
    "\n",
    "#     # Save the generation config\n",
    "#     GEN_CONFIG_PATH = \"../models/t5-small-detoxify-cl/generation_config\"\n",
    "#     generation_config.save_pretrained(GEN_CONFIG_PATH)\n",
    "\n",
    "#     # Define the training arguments\n",
    "#     args = Seq2SeqTrainingArguments(\n",
    "#         output_dir=f'../models/{model_name_t5}_detoxify_cl',\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         logging_strategy=\"epoch\",\n",
    "#         save_total_limit=1,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#         learning_rate=learning_rate, \n",
    "#         weight_decay=weight_decay,\n",
    "#         predict_with_generate=True,\n",
    "#         generation_config=GEN_CONFIG_PATH,\n",
    "#         fp16=True,\n",
    "#         report_to=\"wandb\",\n",
    "#         logging_steps=100,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"Overall\",\n",
    "#         greater_is_better=True,\n",
    "#     )\n",
    "\n",
    "#     # Reinstantiate the model\n",
    "#     model_t5 = T5ForConditionalGeneration.from_pretrained(model_name_t5)\n",
    "\n",
    "#     # Instantiate the trainer\n",
    "#     trainer = Seq2SeqTrainerCustomLoss(\n",
    "#         model = model_t5,\n",
    "#         args = args,\n",
    "#         train_dataset=tokenized_datasets[\"train\"],\n",
    "#         eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#         data_collator=data_collator,\n",
    "#         tokenizer=tokenizer_t5,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "\n",
    "#     return trainer\n",
    "\n",
    "# trainer_t5_small_cl= setup_trainer()\n",
    "# trainer_t5_small_cl.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune T5 Model (With Negative Word List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-Jvh1WXlz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
