{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "import pprint\n",
    "from numba import cuda\n",
    "import optuna\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgarykong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WAND_NOTEBOOK_NAME\"] = \"w266_final_project_models\"\n",
    "os.environ[\"WANDB_DIR\"] = \"../models/wandb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"w266_final_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Parameters for classification\n",
    "BATCH_SIZE_EVAL = 32\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and get lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 10733\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 1193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5_fast.py:158: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1498: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  if pd.api.types.is_categorical_dtype(vector):\n",
      "/opt/conda/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning: use_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n",
      "  with pd.option_context('mode.use_inf_as_na', True):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum source length for BART: 38\n",
      "Maximum target length for BART: 36\n",
      "Maximum source length for T5: 47\n",
      "Maximum target length for T5: 43\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def get_lengths(raw_datasets, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the lengths of the source and target sequences in the dataset.\n",
    "\n",
    "    Args:\n",
    "        raw_datasets (DatasetDict): dictionary containing the raw dataset\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for encoding the sequences\n",
    "    \n",
    "    Returns:\n",
    "        source_lengths (list): list of lengths of source sequences\n",
    "        target_lengths (list): list of lengths of target sequences\n",
    "        max_source_length (int): maximum length of source sequence\n",
    "        max_target_length (int): maximum length of target sequence\n",
    "    \"\"\"\n",
    "    train_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"train\"][\"source\"]]\n",
    "    val_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"validation\"][\"source\"]]\n",
    "    test_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"test\"][\"source\"]]\n",
    "    source_lengths = train_source_lengths + val_source_lengths + test_source_lengths\n",
    "    max_source_length = max(source_lengths)\n",
    "\n",
    "    train_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"train\"][\"target\"]]\n",
    "    val_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"validation\"][\"target\"]]\n",
    "    test_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"test\"][\"target\"]]\n",
    "    target_lengths = train_target_lengths + val_target_lengths + test_target_lengths\n",
    "    max_target_length = max(target_lengths)\n",
    "    \n",
    "    return source_lengths, target_lengths, max_source_length, max_target_length\n",
    "\n",
    "def plot_lengths(source_lengths, target_lengths):\n",
    "    \"\"\"\n",
    "    Plot the distribution of lengths of source and target sequences.\n",
    "\n",
    "    Args:\n",
    "        source_lengths (list): list of lengths of source sequences\n",
    "        target_lengths (list): list of lengths of target sequences\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    sns.histplot(source_lengths, ax=axes[0])\n",
    "    sns.histplot(target_lengths, ax=axes[1])\n",
    "    axes[0].set_title(\"Source Lengths\")\n",
    "    axes[1].set_title(\"Target Lengths\")\n",
    "    axes[0].set_xlabel(\"Length\")\n",
    "    axes[1].set_xlabel(\"Length\")\n",
    "    axes[0].set_ylabel(\"Count\")\n",
    "    axes[1].set_ylabel(\"Count\")\n",
    "    axes[0].set_xlim(0, 200)\n",
    "\n",
    "# Get distribution of lengths of source across train, val, and test\n",
    "bart_source_lengths, bart_target_lengths, bart_max_source_length, bart_max_target_length = get_lengths(raw_datasets, AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\"))\n",
    "\n",
    "# Plot side by side distributions of source and target lengths\n",
    "plot_lengths(bart_source_lengths, bart_target_lengths)\n",
    "\n",
    "# Check what lengths would be after tokenization with t5\n",
    "t5_source_lengths, t5_target_lengths, t5_max_source_length, t5_max_target_length = get_lengths(raw_datasets, AutoTokenizer.from_pretrained(\"t5-base\"))\n",
    "\n",
    "# Plot side by side distributions of source and target lengths\n",
    "plot_lengths(t5_source_lengths, t5_target_lengths)\n",
    "\n",
    "# Print max lengths\n",
    "print(\"Maximum source length for BART:\", bart_max_source_length)\n",
    "print(\"Maximum target length for BART:\", bart_max_target_length)\n",
    "print(\"Maximum source length for T5:\", t5_max_source_length)\n",
    "print(\"Maximum target length for T5:\", t5_max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "# Toxicity classifier\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\n",
    "    \"SkolkovoInstitute/roberta_toxicity_classifier\"\n",
    ")\n",
    "\n",
    "# Acceptability classifier\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    # tox_ref = calc_tox_acceptability(refs, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    # sta_ref = (tox_ref == sta_correct_label).sum() / len(tox_ref)\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "    # sta_pct = sta_pred / sta_ref\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    # acc_ref = calc_tox_acceptability(refs, tokenizer_acceptability, model_acceptability)\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "    # acc_pct = acc_pred / acc_ref\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA_preds\": sta_pred,\n",
    "        # \"STA_pct\": sta_pct,\n",
    "        \"Acceptability_preds\": acc_pred,\n",
    "        # \"Acceptability_pct\": acc_pct,\n",
    "        \"BERT_score_f1\": bert_score_f1,\n",
    "        \"Overall\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (DELETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.5291006187073797,\n",
       " 'STA_preds': 0.6596814752724225,\n",
       " 'Acceptability_preds': 0.47865131,\n",
       " 'BERT_score_f1': 0.9118211820459325,\n",
       " 'Overall': 0.6477872132543977}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    detoxified_text_list = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        detoxified_text_list.append(text)\n",
    "\n",
    "    return detoxified_text_list\n",
    "\n",
    "y_pred_delete = baseline_detoxifier(raw_datasets[\"validation\"]['source'])\n",
    "delete_eval_validation = evaluate_metrics(raw_datasets[\"validation\"]['target'], y_pred_delete)\n",
    "delete_eval_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (BART Base Detox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_PRED_BART_PICKLE_FILE = \"../data/processed/y_pred_bart.pkl\"\n",
    "\n",
    "if os.path.isfile(Y_PRED_BART_PICKLE_FILE):\n",
    "    # Load predictions from pickle file\n",
    "    with open(Y_PRED_BART_PICKLE_FILE, \"rb\") as f:\n",
    "        y_pred_bart = pickle.load(f)\n",
    "else:\n",
    "    # Create predictions using BART\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "\n",
    "    # Create predictions using BART and show progress using tqdm\n",
    "    y_pred_bart = pipe_bart(raw_datasets[\"validation\"]['source'], max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # Convert to list of strings\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    y_pred_bart[:5]\n",
    "\n",
    "    # Save predictions as a pickle file\n",
    "    with open(Y_PRED_BART_PICKLE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_pred_bart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.7015951162845684,\n",
       " 'STA_preds': 0.9178541492036881,\n",
       " 'Acceptability_preds': 0.71802455,\n",
       " 'BERT_score_f1': 0.9451393313699676,\n",
       " 'Overall': 0.8400934595860703}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate BART predictions\n",
    "bart_eval_validation = evaluate_metrics(raw_datasets[\"validation\"]['target'], y_pred_bart)\n",
    "bart_eval_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to Fine-tune T5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_t5_base = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "tokenizer_t5_base = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    return datasetdict_copy\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=32,\n",
    "                per_device_eval_batch_size=128,\n",
    "                learning_rate=1e-4,\n",
    "                num_train_epochs=30,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=4,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        model_checkpoint (str): Name of the pre-trained model to use.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Validation/test dataset.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of generated sequences.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        compute_metrics (function): Function to compute evaluation metrics.\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    GEN_CONFIG_PATH = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(GEN_CONFIG_PATH)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        # save_total_limit=2,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=GEN_CONFIG_PATH,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "\n",
    "    # Create a partial function with the tokenizer argument included\n",
    "    compute_metrics_with_tokenizer = partial(compute_metrics, tokenizer=tokenizer)\n",
    "    \n",
    "    # Instantiate the trainer\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_with_tokenizer,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 (Unidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/2_Models.ipynb Cell 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m prefixed_datasets \u001b[39m=\u001b[39m add_prefix(raw_datasets)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m tokenized_datasets_t5_small \u001b[39m=\u001b[39m prefixed_datasets\u001b[39m.\u001b[39;49mmap(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     preprocess_function,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m     fn_kwargs\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mtokenizer\u001b[39m\u001b[39m'\u001b[39m: tokenizer_t5_small},\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     batched\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     remove_columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msource\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtarget\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m trainer_t5_small \u001b[39m=\u001b[39m setup_trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     output_dir_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mt5-small-detoxify\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     model_checkpoint\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mt5-small\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     train_dataset\u001b[39m=\u001b[39mtokenized_datasets_t5_small[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     eval_dataset\u001b[39m=\u001b[39mtokenized_datasets_t5_small[\u001b[39m\"\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning6-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m wandb\u001b[39m.\u001b[39minit(project\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mw266_final_project\u001b[39m\u001b[39m\"\u001b[39m, name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mt5-small-detoxify\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "tokenized_datasets_t5_small = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-detoxify\")\n",
    "trainer_t5_small.train() # General rule is 10% number of epochs\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate difference between checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, overall scores are maximized even as validation loss increases (see t5-small-detoxify). We can see that the main aspect that improves is style transfer accuracy (STA), but also acceptability. Next, we check samples of text generated at the checkpoint in which validation loss is minimized vs. the checkpoint where the overall score is maximized and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint paths\n",
    "CHECKPOINT_T5_SMALL_MINLOSS = \"../models/t5-small-detoxify/checkpoint-2016\" # Epoch 5, min loss\n",
    "CHECKPOINT_T5_SMALL_BEST = trainer_t5_small.state.best_model_checkpoint # Epoch 15, best overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# The trainer object for t5-small-detoxify is already the best model, so we can just load it\n",
    "trainer_t5_small_best = trainer_t5_small\n",
    "\n",
    "# Load the trainer object for t5-small-detoxify with the minimum loss\n",
    "trainer_t5_small_minloss = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=CHECKPOINT_T5_SMALL_MINLOSS,\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions from trainer objects\n",
    "def get_preds_df (trainer_1=trainer_t5_small_best,\n",
    "                  trainer_2=trainer_t5_small_minloss,\n",
    "                  tokenizer=tokenizer_t5_small,\n",
    "                  tokenized_data=tokenized_datasets_t5_small[\"validation\"],\n",
    "                  raw_data=raw_datasets[\"validation\"]):\n",
    "    \"\"\"\n",
    "    Gets the predictions from a trainer object and returns a dataframe containing the source, target, and prediction.\n",
    "\n",
    "    Args:\n",
    "        trainer_object (Seq2SeqTrainer): trainer object containing the model\n",
    "        tokenized_data (Dataset): dataset to be evaluated\n",
    "        raw_data (Dataset): dataset containing the raw data\n",
    "\n",
    "    Returns:\n",
    "        preds (Seq2SeqPrediction): Seq2SeqPrediction object containing the predictions\n",
    "        decoded_preds (list): list of decoded predictions\n",
    "        df (DataFrame): dataframe containing the source, target, and prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    preds_1 = trainer_1.predict(tokenized_data)\n",
    "    decoded_preds_1 = tokenizer.batch_decode(preds_1.predictions, skip_special_tokens=True)\n",
    "    preds_2 = trainer_2.predict(tokenized_data)\n",
    "    decoded_preds_2 = tokenizer.batch_decode(preds_2.predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Create dataframe containing source, target, and prediction\n",
    "    df = pd.DataFrame()\n",
    "    df[\"source\"] = raw_data[\"source\"]\n",
    "    df[\"target\"] = raw_data[\"target\"]\n",
    "    df[\"pred_1\"] = decoded_preds_1\n",
    "    df[\"pred_2\"] = decoded_preds_2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Get predictions for t5-small-detoxify\n",
    "t5_small_df = get_preds_df(trainer_1=trainer_t5_small_best,\n",
    "                                trainer_2=trainer_t5_small_minloss,\n",
    "                                tokenizer=tokenizer_t5_small,\n",
    "                                tokenized_data=tokenized_datasets_t5_small[\"validation\"],\n",
    "                                raw_data=raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 41\n",
      "Source: to_neutral: to_neutral: You wouldn't know a leftist if he bit you on the ass.\n",
      "Target: You wouldn't know a leftist if he walk up to you\n",
      "Prediction (Best model): You wouldn't know a leftist if he attacked you.\n",
      "Prediction (Min val error): You wouldn't know a leftist if he bit you.\n",
      "Diff in toxicity: -0.902\n",
      "Diff in BLEURT: 0.204\n",
      "Diff in acceptability: 0.006\n",
      "Diff in BERT: 0.003\n",
      "\n",
      "Index: 84\n",
      "Source: to_neutral: to_neutral: Its the adults who are screaming and carrying on like idiots.\n",
      "Target: Its the adults who are screaming and carrying on\n",
      "Prediction (Best model): It's the adults who are screaming and carrying on bad things.\n",
      "Prediction (Min val error): Its the adults who are screaming and carrying on like idiots.\n",
      "Diff in toxicity: -0.999\n",
      "Diff in BLEURT: -0.162\n",
      "Diff in acceptability: 0.705\n",
      "Diff in BERT: -0.037\n",
      "\n",
      "Index: 177\n",
      "Source: to_neutral: to_neutral: Lola is a fucking beast ok\n",
      "Target: Lola is a beast ok\n",
      "Prediction (Best model): Lola is ok\n",
      "Prediction (Min val error): Lola is a beast ok\n",
      "Diff in toxicity: -0.783\n",
      "Diff in BLEURT: -1.563\n",
      "Diff in acceptability: -0.056\n",
      "Diff in BERT: -0.118\n",
      "\n",
      "Index: 207\n",
      "Source: to_neutral: to_neutral: Cunt's too gutless to risk a double dissolution.\n",
      "Target: Guy's too gutless to risk a double dissolution.\n",
      "Prediction (Best model): It's too gutless to risk a double dissolution.\n",
      "Prediction (Min val error): Cunt's too gutless to risk a double dissolution\n",
      "Diff in toxicity: -0.99\n",
      "Diff in BLEURT: 0.5\n",
      "Diff in acceptability: 0.587\n",
      "Diff in BERT: 0.013\n",
      "\n",
      "Index: 285\n",
      "Source: to_neutral: to_neutral: President obama was a fool, or a liar.\n",
      "Target: President Obama was not suitable\n",
      "Prediction (Best model): President Obama was not a good person.\n",
      "Prediction (Min val error): President Obama was a liar.\n",
      "Diff in toxicity: -0.73\n",
      "Diff in BLEURT: 0.623\n",
      "Diff in acceptability: -0.0\n",
      "Diff in BERT: 0.03\n",
      "\n",
      "Index: 299\n",
      "Source: to_neutral: to_neutral: This pope is a fucking drongo.\n",
      "Target: This pope is unsuitable.\n",
      "Prediction (Best model): This pope is not good\n",
      "Prediction (Min val error): This pope is a drongo\n",
      "Diff in toxicity: -0.945\n",
      "Diff in BLEURT: 1.262\n",
      "Diff in acceptability: 0.028\n",
      "Diff in BERT: 0.043\n",
      "\n",
      "Index: 317\n",
      "Source: to_neutral: to_neutral: I was shitting myself when she started on me l\n",
      "Target: I was scared when she started on me I\n",
      "Prediction (Best model): I was messed up when she started on me\n",
      "Prediction (Min val error): I was shitting myself when she started on me\n",
      "Diff in toxicity: -0.969\n",
      "Diff in BLEURT: -0.029\n",
      "Diff in acceptability: -0.003\n",
      "Diff in BERT: 0.026\n",
      "\n",
      "Index: 400\n",
      "Source: to_neutral: to_neutral: Youre just being foolishly prideful.\n",
      "Target: You're just being naively prideful.\n",
      "Prediction (Best model): You're just being unwise.\n",
      "Prediction (Min val error): You are just being foolishly prideful.\n",
      "Diff in toxicity: -0.994\n",
      "Diff in BLEURT: -0.497\n",
      "Diff in acceptability: 0.003\n",
      "Diff in BERT: -0.025\n",
      "\n",
      "Index: 413\n",
      "Source: to_neutral: to_neutral: These ungrateful koreans needs to feel the pain under kim uns rule!\n",
      "Target: These Koreans need to feel the pain under Kim's rule\n",
      "Prediction (Best model): These koreans needs to feel the pain under kim uns rule!\n",
      "Prediction (Min val error): These ungrateful koreans needs to feel the pain under kim uns rule!\n",
      "Diff in toxicity: -0.96\n",
      "Diff in BLEURT: 0.499\n",
      "Diff in acceptability: 0.069\n",
      "Diff in BERT: 0.026\n",
      "\n",
      "Index: 448\n",
      "Source: to_neutral: to_neutral: Nasty liberal media is picking on poor donald by reporting what hes said.\n",
      "Target: Liberal media is picking on poor donald by reporting what hes said.\n",
      "Prediction (Best model): Liberal media is picking on donald by reporting what hes said.\n",
      "Prediction (Min val error): Nasty liberal media is picking on donald by reporting what hes said.\n",
      "Diff in toxicity: -0.952\n",
      "Diff in BLEURT: 0.113\n",
      "Diff in acceptability: 0.111\n",
      "Diff in BERT: 0.015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Calculate relevant metric columns for t5_small_df\n",
    "t5_small_df[\"pred_1_BLEURT\"] = calc_bleurt(t5_small_df[\"target\"], t5_small_df[\"pred_1\"], output_mean=False)\n",
    "t5_small_df[\"pred_2_BLEURT\"] = calc_bleurt(t5_small_df[\"target\"], t5_small_df[\"pred_2\"], output_mean=False)\n",
    "t5_small_df[\"pred_1_toxic_class\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "t5_small_df[\"pred_2_toxic_class\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "t5_small_df[\"pred_1_toxic_score\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_2_toxic_score\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=True, output_mean=False)\n",
    "t5_small_df[\"source_acceptability\"] = calc_tox_acceptability(t5_small_df[\"source\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"target_acceptability\"] = calc_tox_acceptability(t5_small_df[\"target\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_1_acceptability\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_2_acceptability\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_1_BERT_score\"] = calc_bert_score(t5_small_df[\"target\"], t5_small_df[\"pred_1\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "t5_small_df[\"pred_2_BERT_score\"] = calc_bert_score(t5_small_df[\"target\"], t5_small_df[\"pred_2\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "# Calculate differences in BLEURT, toxicity, acceptability, and BERT score\n",
    "t5_small_df[\"diff_toxic_score\"] = t5_small_df[\"pred_1_toxic_score\"] - t5_small_df[\"pred_2_toxic_score\"]\n",
    "t5_small_df[\"diff_BLEURT\"] = t5_small_df[\"pred_1_BLEURT\"] - t5_small_df[\"pred_2_BLEURT\"]\n",
    "t5_small_df[\"diff_acceptability\"] = t5_small_df[\"pred_1_acceptability\"] - t5_small_df[\"pred_2_acceptability\"]\n",
    "t5_small_df[\"diff_BERT_score\"] = t5_small_df[\"pred_1_BERT_score\"] - t5_small_df[\"pred_2_BERT_score\"]\n",
    "\n",
    "# Filter to rows where pred_1_toxicity is less than pred_2_toxicity\n",
    "t5_small_df_filtered = t5_small_df[t5_small_df[\"pred_1_toxic_class\"] < t5_small_df[\"pred_2_toxic_class\"]]\n",
    "\n",
    "# Print as individual lines\n",
    "for index, row in t5_small_df_filtered.head(10).iterrows():\n",
    "    print(\"Index:\", index)\n",
    "    print(\"Source:\", row[\"source\"])\n",
    "    print(\"Target:\", row[\"target\"])\n",
    "    print(\"Prediction (Best model):\", row[\"pred_1\"])\n",
    "    print(\"Prediction (Min val error):\", row[\"pred_2\"])\n",
    "    print(\"Diff in toxicity:\", round(row[\"diff_toxic_score\"], 3))\n",
    "    print(\"Diff in BLEURT:\", round(row[\"diff_BLEURT\"], 3))\n",
    "    print(\"Diff in acceptability:\", round(row[\"diff_acceptability\"], 3))\n",
    "    print(\"Diff in BERT:\", round(row[\"diff_BERT_score\"], 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this sample, it looks like the predictions from the best model are indeed better at detoxifying text (exmple 108, 211) and is sufficiently different from the target text that there is no indication of overfitting per se. So going forward we prioritize selecting best models based on the overall score as opposed to minimizing validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231026_124331-fc1gbly0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/fc1gbly0' target=\"_blank\">t5-base-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/fc1gbly0' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/fc1gbly0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1680' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1680/6720 14:23 < 43:13, 1.94 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Sta Pct</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Acceptability Pct</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.992200</td>\n",
       "      <td>0.813114</td>\n",
       "      <td>0.612634</td>\n",
       "      <td>0.890193</td>\n",
       "      <td>0.933216</td>\n",
       "      <td>0.717977</td>\n",
       "      <td>1.002652</td>\n",
       "      <td>0.925422</td>\n",
       "      <td>0.881428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.815200</td>\n",
       "      <td>0.800356</td>\n",
       "      <td>0.606100</td>\n",
       "      <td>0.880134</td>\n",
       "      <td>0.922671</td>\n",
       "      <td>0.706827</td>\n",
       "      <td>0.987081</td>\n",
       "      <td>0.926470</td>\n",
       "      <td>0.872999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.723700</td>\n",
       "      <td>0.802879</td>\n",
       "      <td>0.607952</td>\n",
       "      <td>0.901928</td>\n",
       "      <td>0.945518</td>\n",
       "      <td>0.712528</td>\n",
       "      <td>0.995042</td>\n",
       "      <td>0.926195</td>\n",
       "      <td>0.884045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.654800</td>\n",
       "      <td>0.809315</td>\n",
       "      <td>0.605336</td>\n",
       "      <td>0.889355</td>\n",
       "      <td>0.932337</td>\n",
       "      <td>0.713441</td>\n",
       "      <td>0.996316</td>\n",
       "      <td>0.927057</td>\n",
       "      <td>0.878677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.593900</td>\n",
       "      <td>0.840008</td>\n",
       "      <td>0.603130</td>\n",
       "      <td>0.897737</td>\n",
       "      <td>0.941125</td>\n",
       "      <td>0.713326</td>\n",
       "      <td>0.996156</td>\n",
       "      <td>0.926323</td>\n",
       "      <td>0.881571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d9b082db4744c558439ad08a01675f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.026 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.057996…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_pct</td><td>█▁▅▅▅</td></tr><tr><td>eval/Acceptability_preds</td><td>█▁▅▅▅</td></tr><tr><td>eval/BERT_score_f1</td><td>▁▅▄█▅</td></tr><tr><td>eval/BLEU</td><td>█▃▅▃▁</td></tr><tr><td>eval/Overall</td><td>▆▁█▅▆</td></tr><tr><td>eval/STA_pct</td><td>▄▁█▄▇</td></tr><tr><td>eval/STA_preds</td><td>▄▁█▄▇</td></tr><tr><td>eval/loss</td><td>▃▁▁▃█</td></tr><tr><td>eval/runtime</td><td>▁▆▄▄█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▅▅▁</td></tr><tr><td>eval/steps_per_second</td><td>█▃▅▅▁</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▄▃▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_pct</td><td>0.99616</td></tr><tr><td>eval/Acceptability_preds</td><td>0.71333</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92632</td></tr><tr><td>eval/BLEU</td><td>0.60313</td></tr><tr><td>eval/Overall</td><td>0.88157</td></tr><tr><td>eval/STA_pct</td><td>0.94112</td></tr><tr><td>eval/STA_preds</td><td>0.89774</td></tr><tr><td>eval/loss</td><td>0.84001</td></tr><tr><td>eval/runtime</td><td>67.7827</td></tr><tr><td>eval/samples_per_second</td><td>17.6</td></tr><tr><td>eval/steps_per_second</td><td>0.148</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>1680</td></tr><tr><td>train/learning_rate</td><td>8e-05</td></tr><tr><td>train/loss</td><td>0.5939</td></tr><tr><td>train/total_flos</td><td>2300808233088000.0</td></tr><tr><td>train/train_loss</td><td>0.75595</td></tr><tr><td>train/train_runtime</td><td>863.6876</td></tr><tr><td>train/train_samples_per_second</td><td>248.539</td></tr><tr><td>train/train_steps_per_second</td><td>7.781</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-base-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/fc1gbly0' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/fc1gbly0</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_124331-fc1gbly0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets_t5_base = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_base},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_base = setup_trainer(\n",
    "    output_dir_name=\"t5-base-detoxify\",\n",
    "    model_checkpoint=\"t5-base\",\n",
    "    train_dataset=tokenized_datasets_t5_base[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_base[\"validation\"],\n",
    ")\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-base-detoxify\")\n",
    "trainer_t5_base.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like t5-small and t5-base performs similarly, but t5-base reaches convergence much earlier. There's also indication of overfitting occurring much earlier (e.g., by epoch 5 validation loss jumps up significantly) without overall performance improving. We will use t5-small going forward given computational constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Bi-directional, No custom loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_dataset(datasets):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    return extended_datasets\n",
    "\n",
    "def get_indices(dataset):\n",
    "    \"\"\"\n",
    "    Saves the indices of data that is to_neutral and to_toxic.\n",
    "    \"\"\"\n",
    "    to_neutral_idx = []\n",
    "    to_toxic_idx = []\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][\"source\"].startswith(\"to_neutral\"):\n",
    "            to_neutral_idx.append(i)\n",
    "        else:\n",
    "            to_toxic_idx.append(i)\n",
    "\n",
    "    return to_neutral_idx, to_toxic_idx\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and labels\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset created using create_bidirectional_datasets e.g., raw_datasets_bd[\"validation\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = get_indices(bd_dataset)[0]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        to_neutral=True\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial without shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbaf87fa8e25489a9e48f5a0e00a703f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7cf05d549f44c96b78f6d0392748f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a7a4e1492c47b2ac90ca207bffcf84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:XRT configuration not detected. Defaulting to preview PJRT runtime. To silence this warning and continue using PJRT, explicitly set PJRT_DEVICE to a supported device or configure XRT. To disable default device selection, set PJRT_SELECT_DEFAULT_DEVICE=0\n",
      "WARNING:root:For more information about the status of PJRT, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:Defaulting to PJRT_DEVICE=CPU\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231031_140250-dyia1hr8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/dyia1hr8' target=\"_blank\">t5-small-bd-noshuffle-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/dyia1hr8' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/dyia1hr8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='20130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   33/20130 00:02 < 28:06, 11.91 it/s, Epoch 0.05/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd = create_bidirectional_dataset(raw_datasets)\n",
    "\n",
    "tokenized_datasets_bd_t5_small = raw_datasets_bd.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_bd = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-bd-noshuffle\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_bd_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_bd_t5_small[\"validation\"],\n",
    "    compute_metrics=partial(compute_metrics_bd, bd_dataset=raw_datasets_bd[\"validation\"], shuffled_data=False)\n",
    "    )\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-bd-noshuffle-detoxify\")\n",
    "trainer_t5_small_bd.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial with shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a088317d95cc480a8bbc2db10fd054aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04c7c937672f460bb80a920adeace817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6404bca88dd741989c0f5735d5be84a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231026_164214-hxjvhsla</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/hxjvhsla' target=\"_blank\">t5-small-bd-shuffle-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/hxjvhsla' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/hxjvhsla</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2013' max='20130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2013/20130 08:05 < 1:12:51, 4.14 it/s, Epoch 3/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Sta Pct</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Acceptability Pct</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.581100</td>\n",
       "      <td>1.283874</td>\n",
       "      <td>0.564078</td>\n",
       "      <td>0.746018</td>\n",
       "      <td>1.561404</td>\n",
       "      <td>0.693716</td>\n",
       "      <td>1.013768</td>\n",
       "      <td>0.913812</td>\n",
       "      <td>1.122893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.337200</td>\n",
       "      <td>1.217955</td>\n",
       "      <td>0.570398</td>\n",
       "      <td>0.661358</td>\n",
       "      <td>1.384211</td>\n",
       "      <td>0.683826</td>\n",
       "      <td>0.999314</td>\n",
       "      <td>0.916303</td>\n",
       "      <td>1.050887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.254400</td>\n",
       "      <td>1.181158</td>\n",
       "      <td>0.565640</td>\n",
       "      <td>0.606035</td>\n",
       "      <td>1.268421</td>\n",
       "      <td>0.693915</td>\n",
       "      <td>1.014059</td>\n",
       "      <td>0.916840</td>\n",
       "      <td>1.006676</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c68962ba5174eea8edd538b5ac6aeb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.033 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.046077…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_pct</td><td>█▁█</td></tr><tr><td>eval/Acceptability_preds</td><td>█▁█</td></tr><tr><td>eval/BERT_score_f1</td><td>▁▇█</td></tr><tr><td>eval/BLEU</td><td>▁█▃</td></tr><tr><td>eval/Overall</td><td>█▄▁</td></tr><tr><td>eval/STA_pct</td><td>█▄▁</td></tr><tr><td>eval/STA_preds</td><td>█▄▁</td></tr><tr><td>eval/loss</td><td>█▄▁</td></tr><tr><td>eval/runtime</td><td>▁▄█</td></tr><tr><td>eval/samples_per_second</td><td>█▅▁</td></tr><tr><td>eval/steps_per_second</td><td>█▄▁</td></tr><tr><td>train/epoch</td><td>▁▁▅▅███</td></tr><tr><td>train/global_step</td><td>▁▁▅▅███</td></tr><tr><td>train/learning_rate</td><td>█▅▁</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_pct</td><td>1.01406</td></tr><tr><td>eval/Acceptability_preds</td><td>0.69392</td></tr><tr><td>eval/BERT_score_f1</td><td>0.91684</td></tr><tr><td>eval/BLEU</td><td>0.56564</td></tr><tr><td>eval/Overall</td><td>1.00668</td></tr><tr><td>eval/STA_pct</td><td>1.26842</td></tr><tr><td>eval/STA_preds</td><td>0.60604</td></tr><tr><td>eval/loss</td><td>1.18116</td></tr><tr><td>eval/runtime</td><td>99.0524</td></tr><tr><td>eval/samples_per_second</td><td>24.088</td></tr><tr><td>eval/steps_per_second</td><td>0.192</td></tr><tr><td>train/epoch</td><td>3.0</td></tr><tr><td>train/global_step</td><td>2013</td></tr><tr><td>train/learning_rate</td><td>9e-05</td></tr><tr><td>train/loss</td><td>1.2544</td></tr><tr><td>train/total_flos</td><td>584385280278528.0</td></tr><tr><td>train/train_loss</td><td>1.39091</td></tr><tr><td>train/train_runtime</td><td>485.3651</td></tr><tr><td>train/train_samples_per_second</td><td>1326.795</td></tr><tr><td>train/train_steps_per_second</td><td>41.474</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-bd-shuffle-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/hxjvhsla' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/hxjvhsla</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231026_164214-hxjvhsla/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd_shuffle = raw_datasets_bd.copy()\n",
    "raw_datasets_bd_shuffle[\"train\"] = raw_datasets_bd_shuffle[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "\n",
    "tokenized_datasets_bd_shuffle_t5_small = raw_datasets_bd_shuffle.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_bd_shuffle = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-bd-shuffle\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_bd_shuffle_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_bd_shuffle_t5_small[\"validation\"],\n",
    "    compute_metrics=partial(compute_metrics_bd, bd_dataset=raw_datasets_bd[\"validation\"], shuffled_data=True)\n",
    "    )\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-bd-shuffle-detoxify\")\n",
    "trainer_t5_small_bd_shuffle.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to check that there isn't a bug in sta_pct (seems odd that it's so high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Bi-directional, with Custom Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for original loss function:\n",
    "\n",
    "https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer.html#Trainer.compute_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_loss(self, model, inputs):\n",
    "#         \"\"\"\n",
    "#         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "\n",
    "#         Subclass and override for custom behavior.\n",
    "#         \"\"\"\n",
    "#         outputs = model(**inputs)\n",
    "#         # Save past state if it exists\n",
    "#         # TODO: this needs to be fixed and made cleaner later.\n",
    "#         if self.args.past_index >= 0:\n",
    "#             self._past = outputs[self.args.past_index]\n",
    "\n",
    "#         if self.label_smoother is not None and \"labels\" in inputs:\n",
    "#             return self.label_smoother(outputs, inputs[\"labels\"])\n",
    "#         else:\n",
    "#             # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "#             return outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7442, device='cuda:0', requires_grad=True)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "No inf checks were recorded for this optimizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/garykong/w266_final_project/notebooks/2_Models.ipynb Cell 70\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning-1-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=105'>106</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning-1-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=107'>108</a>\u001b[0m trainer_t5_small_cl\u001b[39m=\u001b[39m setup_trainer()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bdeeplearning-1-vm.europe-west4-b.w266-401709/home/garykong/w266_final_project/notebooks/2_Models.ipynb#Y253sdnNjb2RlLXJlbW90ZQ%3D%3D?line=108'>109</a>\u001b[0m trainer_t5_small_cl\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1591\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1589\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1590\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1591\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1592\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1593\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1594\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1595\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1596\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1963\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1960\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m   1961\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1962\u001b[0m         \u001b[39m# tpu-comment: accelerate wrapped optimizers call xm.optimizer_step\u001b[39;00m\n\u001b[0;32m-> 1963\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m   1964\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdo_grad_scaling:\n\u001b[1;32m   1965\u001b[0m     scale_before \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mget_scale()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/optimizer.py:132\u001b[0m, in \u001b[0;36mAcceleratedOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_patched_step_method\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, closure)\n\u001b[1;32m    133\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerate_step_called:\n\u001b[1;32m    136\u001b[0m         \u001b[39m# If the optimizer step was skipped, gradient overflow was detected.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:368\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mis\u001b[39;00m OptState\u001b[39m.\u001b[39mREADY:\n\u001b[1;32m    366\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_(optimizer)\n\u001b[0;32m--> 368\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mfound_inf_per_device\u001b[39m\u001b[39m\"\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mNo inf checks were recorded for this optimizer.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    370\u001b[0m retval \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_opt_step(optimizer, optimizer_state, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    372\u001b[0m optimizer_state[\u001b[39m\"\u001b[39m\u001b[39mstage\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m OptState\u001b[39m.\u001b[39mSTEPPED\n",
      "\u001b[0;31mAssertionError\u001b[0m: No inf checks were recorded for this optimizer."
     ]
    }
   ],
   "source": [
    "# class Seq2SeqTrainerCustomLoss(Seq2SeqTrainer):\n",
    "#     def compute_loss(self, model, inputs, return_outputs=False):\n",
    "#         \"\"\"\n",
    "#         Compute custom loss for the model.\n",
    "\n",
    "#         Args:\n",
    "#             model (torch.nn.Module): The model training or evaluating.\n",
    "#             inputs (dict): The inputs and targets of the model.\n",
    "#             return_outputs (bool): Whether to return model outputs.\n",
    "\n",
    "#         Returns:\n",
    "#             torch.FloatTensor: The loss value.\n",
    "#         \"\"\"\n",
    "#         # Call prediction_step\n",
    "#         loss, encoded_y_pred, encoded_y_test = self.prediction_step(model, inputs, prediction_loss_only=False)\n",
    "    \n",
    "#         # Decode the generated tokens\n",
    "#         if isinstance(encoded_y_pred, tuple):\n",
    "#             encoded_y_pred = encoded_y_pred[0]\n",
    "#         decoded_y_pred = self.tokenizer.batch_decode(encoded_y_pred, skip_special_tokens=True)\n",
    "\n",
    "#         # Decode the labels\n",
    "#         ## Replace -100s in the labels as we can't decode them\n",
    "#         encoded_y_test = torch.where(encoded_y_test != -100, encoded_y_test, torch.tensor(self.tokenizer.pad_token_id))\n",
    "#         decoded_y_test = self.tokenizer.batch_decode(encoded_y_test, skip_special_tokens=True)\n",
    "\n",
    "#         # Some simple post-processing\n",
    "#         decoded_y_pred = [pred.strip() for pred in decoded_y_pred]\n",
    "#         decoded_y_test = [label.strip() for label in decoded_y_test]\n",
    "        \n",
    "#         # Calculate metrics\n",
    "#         # TO DO: MAKE THIS WORK WITH PYTORCH LIGHTNING\n",
    "#         composite_score = evaluate_metrics(\n",
    "#             decoded_y_test,\n",
    "#             decoded_y_pred,\n",
    "#             tokenizer_toxicity=tokenizer_toxicity,\n",
    "#             model_toxicity=model_toxicity,\n",
    "#             tokenizer_acceptability=tokenizer_acceptability,\n",
    "#             model_acceptability=model_acceptability,\n",
    "#             include_bleurt=False\n",
    "#         )['Overall']\n",
    "\n",
    "#         # # Composite score will be on a scale of 0 - 1, so we can invert it to get the loss\n",
    "#         custom_loss = torch.tensor(1 - composite_score, dtype=torch.float, requires_grad=True, device=self.args.device)\n",
    "\n",
    "#         print(custom_loss)\n",
    "#         return custom_loss\n",
    "    \n",
    "# def setup_trainer(model_name_t5=\"t5-small\",\n",
    "#                 per_device_train_batch_size=64,\n",
    "#                 per_device_eval_batch_size=64,\n",
    "#                 learning_rate=3e-4,\n",
    "#                 weight_decay=0.01,\n",
    "#                 num_train_epochs=10,\n",
    "#                 max_length=50,\n",
    "#                 num_beams=4):\n",
    "    \n",
    "#     # Define generation config\n",
    "#     generation_config = GenerationConfig(\n",
    "#         max_length=max_length,\n",
    "#         num_beams=num_beams,\n",
    "#         eos_token_id=tokenizer_t5.eos_token_id,\n",
    "#         bos_token_id=tokenizer_t5.bos_token_id,\n",
    "#         pad_token_id=tokenizer_t5.pad_token_id,\n",
    "#         decoder_start_token_id=tokenizer_t5.pad_token_id)\n",
    "\n",
    "#     # Save the generation config\n",
    "#     GEN_CONFIG_PATH = \"../models/t5-small-detoxify-cl/generation_config\"\n",
    "#     generation_config.save_pretrained(GEN_CONFIG_PATH)\n",
    "\n",
    "#     # Define the training arguments\n",
    "#     args = Seq2SeqTrainingArguments(\n",
    "#         output_dir=f'../models/{model_name_t5}_detoxify_cl',\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         logging_strategy=\"epoch\",\n",
    "#         save_total_limit=1,\n",
    "#         num_train_epochs=num_train_epochs,\n",
    "#         per_device_train_batch_size=per_device_train_batch_size,\n",
    "#         per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "#         learning_rate=learning_rate, \n",
    "#         weight_decay=weight_decay,\n",
    "#         predict_with_generate=True,\n",
    "#         generation_config=GEN_CONFIG_PATH,\n",
    "#         fp16=True,\n",
    "#         report_to=\"wandb\",\n",
    "#         logging_steps=100,\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"Overall\",\n",
    "#         greater_is_better=True,\n",
    "#     )\n",
    "\n",
    "#     # Reinstantiate the model\n",
    "#     model_t5 = T5ForConditionalGeneration.from_pretrained(model_name_t5)\n",
    "\n",
    "#     # Instantiate the trainer\n",
    "#     trainer = Seq2SeqTrainerCustomLoss(\n",
    "#         model = model_t5,\n",
    "#         args = args,\n",
    "#         train_dataset=tokenized_datasets[\"train\"],\n",
    "#         eval_dataset=tokenized_datasets[\"validation\"],\n",
    "#         data_collator=data_collator,\n",
    "#         tokenizer=tokenizer_t5,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#     )\n",
    "\n",
    "#     return trainer\n",
    "\n",
    "# trainer_t5_small_cl= setup_trainer()\n",
    "# trainer_t5_small_cl.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune T5 Model (With Negative Word List)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-Jvh1WXlz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
