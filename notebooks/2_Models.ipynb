{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 10:49:52.012125: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-03 10:49:52.012184: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-03 10:49:52.012220: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    T5Config,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    GenerationConfig,\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    EarlyStoppingCallback,\n",
    "    pipeline,\n",
    ")\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import nltk\n",
    "import csv\n",
    "import time\n",
    "import gc\n",
    "import GPUtil\n",
    "import evaluate\n",
    "import pprint\n",
    "from numba import cuda\n",
    "import optuna\n",
    "import wandb\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import wandb\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, Union, Optional, Tuple, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgarykong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "wandb.login()\n",
    "os.environ[\"WAND_NOTEBOOK_NAME\"] = \"w266_final_project_models\"\n",
    "os.environ[\"WANDB_DIR\"] = \"../models/wandb\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"w266_final_project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "# Parameters for classification\n",
    "BATCH_SIZE_EVAL = 32\n",
    "BATCH_SIZE_TRAIN = 32\n",
    "\n",
    "# Parameters for T5 model fine-tuning\n",
    "PER_DEVICE_TRAIN_BATCH_SIZE = 64\n",
    "PER_DEVICE_EVAL_BATCH_SIZE = 128\n",
    "LEARNING_RATE = 3e-4\n",
    "NUM_TRAIN_EPOCHS = 20\n",
    "\n",
    "# Setting the DEVICE to cuda\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Set path for profane word list\n",
    "PROFANE_WORD_PATH = \"../data/raw/en.txt\"\n",
    "\n",
    "# Set path for raw dataset dictionary\n",
    "RAW_DATASET_PATH = \"../data/processed/raw_dataset.pkl\"\n",
    "\n",
    "# Set maximum length for input and output\n",
    "MAX_INPUT_LENGTH = 64\n",
    "MAX_OUTPUT_LENGTH = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and models\n",
    "tokenizer_t5_base = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "model_t5_base = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(DEVICE)\n",
    "tokenizer_t5_small = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model_t5_small = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(DEVICE)\n",
    "tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "model_toxicity = RobertaForSequenceClassification.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\").to(DEVICE)\n",
    "tokenizer_acceptability = AutoTokenizer.from_pretrained(\"iproskurina/tda-bert-en-cola\")\n",
    "model_acceptability = AutoModelForSequenceClassification.from_pretrained(\"iproskurina/tda-bert-en-cola\").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset and get lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 10733\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 1193\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['source', 'target'],\n",
       "        num_rows: 671\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "raw_datasets = DatasetDict.load_from_disk(RAW_DATASET_PATH)\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def get_lengths(raw_datasets, tokenizer):\n",
    "#     \"\"\"\n",
    "#     Get the lengths of the source and target sequences in the dataset.\n",
    "\n",
    "#     Args:\n",
    "#         raw_datasets (DatasetDict): dictionary containing the raw dataset\n",
    "#         tokenizer (PreTrainedTokenizer): tokenizer to use for encoding the sequences\n",
    "    \n",
    "#     Returns:\n",
    "#         source_lengths (list): list of lengths of source sequences\n",
    "#         target_lengths (list): list of lengths of target sequences\n",
    "#         max_source_length (int): maximum length of source sequence\n",
    "#         max_target_length (int): maximum length of target sequence\n",
    "#     \"\"\"\n",
    "#     train_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"train\"][\"source\"]]\n",
    "#     val_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"validation\"][\"source\"]]\n",
    "#     test_source_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"test\"][\"source\"]]\n",
    "#     source_lengths = train_source_lengths + val_source_lengths + test_source_lengths\n",
    "#     max_source_length = max(source_lengths)\n",
    "\n",
    "#     train_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"train\"][\"target\"]]\n",
    "#     val_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"validation\"][\"target\"]]\n",
    "#     test_target_lengths = [len(tokenizer(x)[\"input_ids\"]) for x in raw_datasets[\"test\"][\"target\"]]\n",
    "#     target_lengths = train_target_lengths + val_target_lengths + test_target_lengths\n",
    "#     max_target_length = max(target_lengths)\n",
    "    \n",
    "#     return source_lengths, target_lengths, max_source_length, max_target_length\n",
    "\n",
    "# def plot_lengths(source_lengths, target_lengths):\n",
    "#     \"\"\"\n",
    "#     Plot the distribution of lengths of source and target sequences.\n",
    "\n",
    "#     Args:\n",
    "#         source_lengths (list): list of lengths of source sequences\n",
    "#         target_lengths (list): list of lengths of target sequences\n",
    "#     \"\"\"\n",
    "#     fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "#     sns.histplot(source_lengths, ax=axes[0])\n",
    "#     sns.histplot(target_lengths, ax=axes[1])\n",
    "#     axes[0].set_title(\"Source Lengths\")\n",
    "#     axes[1].set_title(\"Target Lengths\")\n",
    "#     axes[0].set_xlabel(\"Length\")\n",
    "#     axes[1].set_xlabel(\"Length\")\n",
    "#     axes[0].set_ylabel(\"Count\")\n",
    "#     axes[1].set_ylabel(\"Count\")\n",
    "#     axes[0].set_xlim(0, 100)\n",
    "#     axes[1].set_xlim(0, 100)\n",
    "\n",
    "# # Get distribution of lengths of source across train, val, and test\n",
    "# bart_source_lengths, bart_target_lengths, bart_max_source_length, bart_max_target_length = get_lengths(raw_datasets, AutoTokenizer.from_pretrained(\"s-nlp/bart-base-detox\"))\n",
    "\n",
    "# # Check what lengths would be after tokenization with t5\n",
    "# t5_source_lengths, t5_target_lengths, t5_max_source_length, t5_max_target_length = get_lengths(raw_datasets, AutoTokenizer.from_pretrained(\"t5-base\"))\n",
    "\n",
    "# # Print max lengths\n",
    "# print(\"Maximum source length for BART:\", bart_max_source_length)\n",
    "# print(\"Maximum target length for BART:\", bart_max_target_length)\n",
    "# print(\"Maximum source length for T5:\", t5_max_source_length)\n",
    "# print(\"Maximum target length for T5:\", t5_max_target_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_time(func, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculates the time it takes to run a function.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    result = func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Function {func.__name__} took {elapsed_time:.2f} seconds to run.\")\n",
    "    return result\n",
    "\n",
    "def get_gpu_memory():\n",
    "    \"\"\"\n",
    "    Gets the GPU memory information.\n",
    "    \"\"\"\n",
    "    gpus = GPUtil.getGPUs()\n",
    "    gpu = gpus[0]\n",
    "    print(f\"Total GPU memory: {gpu.memoryTotal}MB\")\n",
    "    print(f\"Free GPU memory: {gpu.memoryFree}MB\")\n",
    "    print(f\"Used GPU memory: {gpu.memoryUsed}MB\")\n",
    "\n",
    "def force_clear_GPU_memory():\n",
    "    \"\"\"\n",
    "    Force clears the GPU memory.\n",
    "    \"\"\"\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    Cleans up the GPU memory.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model variables\n",
    "model_bleurt = None\n",
    "model_bertscore = None\n",
    "model_sacrebleu = None\n",
    "\n",
    "def calc_sacrebleu(refs, preds):\n",
    "    \"\"\"\n",
    "    Calculates the SacreBLEU score.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences\n",
    "        preds (list): List of predicted sentences\n",
    "    \n",
    "    Returns:\n",
    "        results (float): SacreBLEU score\n",
    "    \"\"\"\n",
    "    global model_sacrebleu\n",
    "\n",
    "    if model_sacrebleu is None:\n",
    "        model_sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    results = model_sacrebleu.compute(predictions=preds, references=refs)[\"score\"]\n",
    "    results = results/100\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_bert_score(\n",
    "    refs, preds, model_type=\"microsoft/deberta-large-mnli\", output_mean=True\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Calculates BERT score per line. Note: https://docs.google.com/spreadsheets/d/1RKOVpselB98Nnh_EOC4A2BYn8_201tmPODpNWu4w7xI/edit#gid=0 lists the best performing models\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        y_pred (list): List of predicted sentences.\n",
    "        model_type (str): Type of BERT model to use.\n",
    "        output_mean (bool): Whether to output the mean of the scores.\n",
    "\n",
    "    Returns:\n",
    "        list of precision, recall, f1 scores.\n",
    "\n",
    "    \"\"\"\n",
    "    global model_bertscore\n",
    "\n",
    "    if model_bertscore is None:\n",
    "        model_bertscore = evaluate.load(\"bertscore\")\n",
    "        \n",
    "    results = model_bertscore.compute(predictions=preds, references=refs, model_type=model_type)\n",
    "    precision = np.array(results[\"precision\"])\n",
    "    recall = np.array(results[\"recall\"])\n",
    "    f1 = np.array(results[\"f1\"])\n",
    "    \n",
    "    if output_mean:\n",
    "        precision = precision.mean()\n",
    "        recall = recall.mean()\n",
    "        f1 = f1.mean()\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def calc_bleurt(refs, preds, checkpoint=\"BLEURT-20_D12\", output_mean = True):\n",
    "    \"\"\"\n",
    "    Calculates BLEURT score per line.\n",
    "\n",
    "    Args:\n",
    "        refs (list): List of reference sentences.\n",
    "        preds (list): List of predicted sentences.\n",
    "        output_type (str): Type of output to return. Either 'numpy' or 'list'.\n",
    "\n",
    "    Returns:\n",
    "        list/array of BLEURT scores.\n",
    "    \"\"\"\n",
    "    global model_bleurt\n",
    "\n",
    "    if model_bleurt is None:\n",
    "        model_bleurt = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=checkpoint)\n",
    "\n",
    "    results = np.array(model_bleurt.compute(predictions=preds, references=refs)[\"scores\"])\n",
    "\n",
    "    if output_mean:\n",
    "        results = results.mean()\n",
    "\n",
    "    return results\n",
    "\n",
    "def calc_tox_acceptability(\n",
    "    data,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    output_score=True,\n",
    "    output_mean=True):\n",
    "    \"\"\"\n",
    "    Calculates toxicity and acceptability scores for a given dataset.\n",
    "\n",
    "    Args:\n",
    "        data = list of strings to be evaluated\n",
    "        tokenizer = tokenizer for the model\n",
    "        model = model to be used for evaluation\n",
    "        output_score = whether to output the score or the label\n",
    "        output_mean = whether to output the mean of the scores or the scores for each sentence\n",
    "    \n",
    "    Returns:\n",
    "        array of toxicity and acceptability scores.\n",
    "    \"\"\"\n",
    "    model = model.to(DEVICE)\n",
    "    \n",
    "    inputs = tokenizer(data, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs)[\"logits\"]\n",
    "        if output_score:\n",
    "            result = torch.nn.functional.softmax(logits, dim=1)[:, 1]\n",
    "        else:\n",
    "            result = logits.argmax(1).data\n",
    "        result = result.cpu().numpy()\n",
    "\n",
    "    if output_mean:\n",
    "        result = result.mean()\n",
    "        \n",
    "    return result\n",
    "\n",
    "def evaluate_metrics(\n",
    "    refs,\n",
    "    preds,\n",
    "    tokenizer_toxicity=tokenizer_toxicity,\n",
    "    model_toxicity=model_toxicity,\n",
    "    tokenizer_acceptability=tokenizer_acceptability,\n",
    "    model_acceptability=model_acceptability,\n",
    "    to_neutral=True,\n",
    "    weights={\n",
    "        \"BLEU\": 0.2,\n",
    "        \"STA\": 0.4,\n",
    "        \"Acceptability\": 0.2,\n",
    "        \"BERT_Score\": 0.2\n",
    "    },\n",
    "    include_bleurt=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Calculates and returns a dictionary of evaluation metrics\n",
    "\n",
    "    Args:\n",
    "        refs (list): list of strings (reference)\n",
    "        preds (list): list of strings (predictions)\n",
    "        tokenizer_toxicity (tokenizer): tokenizer for toxicity model\n",
    "        model_toxicity (model): toxicity model\n",
    "        tokenizer_acceptability (tokenizer): tokenizer for acceptability model\n",
    "        model_acceptability (model): acceptability model\n",
    "        to_neutral (bool): whether the goal is to transfer to neutral (True) or to toxic (False)\n",
    "        weights (dict): dictionary of weights for each metric\n",
    "        include_bleurt (bool): whether to include BLEURT score in the output\n",
    "\n",
    "    Returns:\n",
    "        results (dict): dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate BLEU score\n",
    "    bleu = calc_sacrebleu(refs, preds)\n",
    "\n",
    "    # Calculate toxicity classification\n",
    "    # tox_ref = calc_tox_acceptability(refs, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "    tox_pred = calc_tox_acceptability(preds, tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "\n",
    "    # Calculate style transfer accuracy as proportion of sentences that were correctly classified (as non-toxic / toxic)\n",
    "    if to_neutral:\n",
    "        sta_correct_label = 0\n",
    "    else:\n",
    "        sta_correct_label = 1\n",
    "\n",
    "    # sta_ref = (tox_ref == sta_correct_label).sum() / len(tox_ref)\n",
    "    sta_pred = (tox_pred == sta_correct_label).sum() / len(tox_pred)\n",
    "    # sta_pct = sta_pred / sta_ref\n",
    "\n",
    "    # Calculate acceptability scores\n",
    "    # acc_ref = calc_tox_acceptability(refs, tokenizer_acceptability, model_acceptability)\n",
    "    acc_pred = calc_tox_acceptability(preds, tokenizer_acceptability, model_acceptability)\n",
    "    # acc_pct = acc_pred / acc_ref\n",
    "\n",
    "    # Calculate similarity score\n",
    "    bert_score_f1 = calc_bert_score(refs, preds, model_type=\"distilbert-base-uncased\")[2]\n",
    "\n",
    "    # Calculate BLEURT score if include_bleurt is True\n",
    "    bleurt = None\n",
    "    if include_bleurt:\n",
    "        bleurt = calc_bleurt(refs, preds)\n",
    "\n",
    "    # Calculate composite score\n",
    "    composite_score = weights[\"BLEU\"] * bleu + weights[\"STA\"] * sta_pred + weights[\"Acceptability\"] * acc_pred + weights[\"BERT_Score\"] * bert_score_f1\n",
    "\n",
    "    # Return a dictionary of metrics\n",
    "    results = {\n",
    "        \"BLEU\": bleu,\n",
    "        \"STA_preds\": sta_pred,\n",
    "        # \"STA_pct\": sta_pct,\n",
    "        \"Acceptability_preds\": acc_pred,\n",
    "        # \"Acceptability_pct\": acc_pct,\n",
    "        \"BERT_score_f1\": bert_score_f1,\n",
    "        \"Overall\": composite_score,\n",
    "    }\n",
    "    if include_bleurt:\n",
    "        results[\"BLEURT\"] = bleurt\n",
    "        \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (DELETE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.5291006187073797,\n",
       " 'STA_preds': 0.6596814752724225,\n",
       " 'Acceptability_preds': 0.47865131,\n",
       " 'BERT_score_f1': 0.9118211825455524,\n",
       " 'Overall': 0.6477872133543217}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def baseline_detoxifier(text_list, profane_word_path=PROFANE_WORD_PATH):\n",
    "    \"\"\"\n",
    "    Returns a detoxified version of the text by replacing toxic terms with blanks\n",
    "\n",
    "    Args:\n",
    "        text_list (list): list of strings to be detoxified\n",
    "        toxic_list (list): list of toxic terms to be removed from text_list\n",
    "\n",
    "    Returns:\n",
    "        detoxified_text_list (list): list of detoxified strings\n",
    "    \"\"\"\n",
    "    # Load list of profane words\n",
    "    profane_words = []\n",
    "    with open(profane_word_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            profane_words.append(line.strip())\n",
    "\n",
    "    # Detoxify text\n",
    "    detoxified_text_list = []\n",
    "    for text in text_list:\n",
    "        for term in profane_words:\n",
    "            text = text.replace(term, \"\")\n",
    "        detoxified_text_list.append(text)\n",
    "\n",
    "    return detoxified_text_list\n",
    "\n",
    "y_pred_delete = baseline_detoxifier(raw_datasets[\"validation\"]['source'])\n",
    "delete_eval_validation = evaluate_metrics(raw_datasets[\"validation\"]['target'], y_pred_delete)\n",
    "delete_eval_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model (BART Base Detox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_PRED_BART_PICKLE_FILE = \"../data/processed/y_pred_bart.pkl\"\n",
    "\n",
    "if os.path.isfile(Y_PRED_BART_PICKLE_FILE):\n",
    "    # Load predictions from pickle file\n",
    "    with open(Y_PRED_BART_PICKLE_FILE, \"rb\") as f:\n",
    "        y_pred_bart = pickle.load(f)\n",
    "else:\n",
    "    # Create predictions using BART\n",
    "    pipe_bart = pipeline(\"text2text-generation\", model=\"s-nlp/bart-base-detox\", device=DEVICE)\n",
    "\n",
    "    # Create predictions using BART and show progress using tqdm\n",
    "    y_pred_bart = pipe_bart(raw_datasets[\"validation\"]['source'], max_length=MAX_OUTPUT_LENGTH, truncation=True)\n",
    "\n",
    "    # Convert to list of strings\n",
    "    y_pred_bart = [x[\"generated_text\"] for x in y_pred_bart]\n",
    "    y_pred_bart[:5]\n",
    "\n",
    "    # Save predictions as a pickle file\n",
    "    with open(Y_PRED_BART_PICKLE_FILE, \"wb\") as f:\n",
    "        pickle.dump(y_pred_bart, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'BLEU': 0.7015951162845684,\n",
       " 'STA_preds': 0.9178541492036881,\n",
       " 'Acceptability_preds': 0.71802455,\n",
       " 'BERT_score_f1': 0.9451393306205379,\n",
       " 'Overall': 0.8400934594361843}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate BART predictions\n",
    "bart_eval_validation = evaluate_metrics(raw_datasets[\"validation\"]['target'], y_pred_bart)\n",
    "bart_eval_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions to Fine-tune T5 Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(datasetdict, prefix=\"to_neutral: \"):\n",
    "    \"\"\"Adds a prefix to the source sequence in the dataset.\"\"\"\n",
    "    datasetdict_copy = datasetdict.copy()\n",
    "    datasetdict_copy[\"train\"] = datasetdict_copy[\"train\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"validation\"] = datasetdict_copy[\"validation\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy[\"test\"] = datasetdict_copy[\"test\"].map(lambda x: {\"source\": prefix + x[\"source\"]})\n",
    "    datasetdict_copy = DatasetDict(datasetdict_copy)\n",
    "    return datasetdict_copy\n",
    "\n",
    "def preprocess_function(examples, tokenizer):\n",
    "    \"\"\"Preprocess function for T5.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"source\"],\n",
    "        text_target=examples[\"target\"],\n",
    "        max_length=MAX_INPUT_LENGTH,\n",
    "        truncation=True,\n",
    "    )\n",
    "    return model_inputs\n",
    "\n",
    "def post_process(preds, refs, tokenizer):\n",
    "    \"\"\"\n",
    "    Post-process function for T5.\n",
    "\n",
    "    Args:\n",
    "        preds (list): list of predicted sequences\n",
    "        refs (list): list of reference sequences\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding\n",
    "\n",
    "    Returns:\n",
    "        decoded_preds (list): list of decoded predicted sequences\n",
    "        decoded_refs (list): list of decoded reference sequences\n",
    "    \"\"\"\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "    return decoded_preds, decoded_refs\n",
    "\n",
    "def compute_metrics(eval_preds, tokenizer):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "\n",
    "    Args:\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # Post-process the predictions and references\n",
    "    decoded_preds, decoded_refs = post_process(preds, refs, tokenizer)\n",
    "    \n",
    "    # Evaluate metrics\n",
    "    return evaluate_metrics(\n",
    "        decoded_refs,\n",
    "        decoded_preds,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_acceptability=tokenizer_acceptability,\n",
    "        model_acceptability=model_acceptability,\n",
    "        include_bleurt=False\n",
    "    )\n",
    "\n",
    "def setup_trainer(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=4,\n",
    "                compute_metrics=compute_metrics,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "                trainer_class = Seq2SeqTrainer,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        model_checkpoint (str): Name of the pre-trained model to use.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Validation/test dataset.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of generated sequences.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        compute_metrics (function): Function to compute evaluation metrics.\n",
    "        callbacks (list): List of callbacks to use.\n",
    "        trainer_class (Seq2SeqTrainer): Trainer class to use.\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    GEN_CONFIG_PATH = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(GEN_CONFIG_PATH)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArguments(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=GEN_CONFIG_PATH,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "    )\n",
    "\n",
    "    # Create a partial function with the tokenizer argument included\n",
    "    compute_metrics_with_tokenizer = partial(compute_metrics, tokenizer=tokenizer)\n",
    "    \n",
    "    # Instantiate the trainer\n",
    "    trainer = trainer_class(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_with_tokenizer,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "def create_bidirectional_dataset(datasets, shuffle=True):\n",
    "    \"\"\"\n",
    "    Creates a bi-directional dataset from the original dataset.\n",
    "\n",
    "    Args:\n",
    "        datasets (DatasetDict): DatasetDict object containing the original dataset.\n",
    "        shuffle (bool): Whether to shuffle the dataset or not.\n",
    "    \n",
    "    Returns:\n",
    "        extended_datasets (DatasetDict): DatasetDict object containing the bi-directional dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def bidirectional_extension(dataset):\n",
    "        new_data = {\n",
    "            \"source\": [],\n",
    "            \"target\": []\n",
    "        }\n",
    "        for src, tgt in zip(dataset['source'], dataset['target']):\n",
    "            new_data['source'].extend([f'to_neutral: {src}', f'to_toxic: {tgt}'])\n",
    "            new_data['target'].extend([tgt, src])\n",
    "        return new_data\n",
    "\n",
    "    extended_train_data = bidirectional_extension(datasets[\"train\"])\n",
    "    extended_validation_data = bidirectional_extension(datasets[\"validation\"])\n",
    "    extended_test_data = bidirectional_extension(datasets[\"test\"])\n",
    "\n",
    "    extended_datasets = DatasetDict({\n",
    "        \"train\": Dataset.from_dict(extended_train_data),\n",
    "        \"validation\": Dataset.from_dict(extended_validation_data),\n",
    "        \"test\": Dataset.from_dict(extended_test_data)\n",
    "    })\n",
    "\n",
    "    if shuffle:\n",
    "        extended_datasets[\"train\"] = extended_datasets[\"train\"].shuffle(seed=RANDOM_SEED)\n",
    "        \n",
    "    return extended_datasets\n",
    "\n",
    "def compute_metrics_bd(eval_preds, tokenizer, bd_dataset, shuffled_data=False):\n",
    "    \"\"\"\n",
    "    Function to calculate the metrics for trainer.evaluate().\n",
    "    This function is for the bi-directional model.\n",
    "    \n",
    "    Args:\n",
    "        eval_preds (tuple): Tuple containing the predictions and references\n",
    "        tokenizer (PreTrainedTokenizer): tokenizer to use for decoding the predictions\n",
    "        shuffled_data (bool): Whether the data is shuffled or not\n",
    "        bd_dataset (DatasetDict): Bidirectional dataset to use for testing created using create_bidirectional_datasets\n",
    "                                  For example, raw_datasets_bd[\"validation\"] or raw_datasets_bd[\"test\"]\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing the metrics\n",
    "    \"\"\"\n",
    "    preds, refs = eval_preds\n",
    "\n",
    "    # In case the model returns more than the prediction logits\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100s in the labels as we can't decode them\n",
    "    refs = np.where(refs != -100, refs, tokenizer.pad_token_id)\n",
    "    decoded_refs = tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "    \n",
    "    # If shuffled data is false, have to_neutral_preds and to_neutral_refs just be predictions and refs with even indices\n",
    "    if not shuffled_data:\n",
    "        to_neutral_preds = decoded_preds[::2]\n",
    "        to_neutral_refs = decoded_refs[::2]\n",
    "    # Otherwise, get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "    else:\n",
    "        # Get the indices to use when splitting predictions and refs to to_neutral and to_toxic\n",
    "        to_neutral_idx = [i for i, input_sentence in enumerate(bd_dataset['source']) if input_sentence.startswith(\"to_neutral\")]\n",
    "\n",
    "        # Retrieve based on the indices\n",
    "        to_neutral_preds = [decoded_preds[i] for i in to_neutral_idx]\n",
    "        to_neutral_refs = [decoded_refs[i] for i in to_neutral_idx]\n",
    "    \n",
    "    # Evaluate metrics for to_neutral\n",
    "    to_neutral_metrics = evaluate_metrics(\n",
    "        to_neutral_refs,\n",
    "        to_neutral_preds,\n",
    "        to_neutral=True\n",
    "    )\n",
    "\n",
    "    # Return dictionary of to_neutral metrics\n",
    "    return to_neutral_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 (Unidirectional)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5-Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-acee6fdb32e75384.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-022152bc8e674931.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-68aee8936da2e12b.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bhnnox0v) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>▁</td></tr><tr><td>eval/BERT_score_f1</td><td>▁</td></tr><tr><td>eval/BLEU</td><td>▁</td></tr><tr><td>eval/Overall</td><td>▁</td></tr><tr><td>eval/STA_preds</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.69185</td></tr><tr><td>eval/BERT_score_f1</td><td>0.9231</td></tr><tr><td>eval/BLEU</td><td>0.58942</td></tr><tr><td>eval/Overall</td><td>0.77415</td></tr><tr><td>eval/STA_preds</td><td>0.83319</td></tr><tr><td>eval/loss</td><td>0.95093</td></tr><tr><td>eval/runtime</td><td>45.4757</td></tr><tr><td>eval/samples_per_second</td><td>26.234</td></tr><tr><td>eval/steps_per_second</td><td>0.22</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>168</td></tr><tr><td>train/learning_rate</td><td>0.00028</td></tr><tr><td>train/loss</td><td>1.1814</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/bhnnox0v' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/bhnnox0v</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231101_134845-bhnnox0v/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bhnnox0v). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730f799e572448ca9eb33342f56e9471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111249964445354, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231101_135037-64fpgb5d</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/64fpgb5d' target=\"_blank\">t5-small-detoxify</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/64fpgb5d' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/64fpgb5d</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1176' max='3360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1176/3360 07:26 < 13:50, 2.63 it/s, Epoch 7/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.181400</td>\n",
       "      <td>0.950930</td>\n",
       "      <td>0.589415</td>\n",
       "      <td>0.833194</td>\n",
       "      <td>0.691846</td>\n",
       "      <td>0.923100</td>\n",
       "      <td>0.774150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.988400</td>\n",
       "      <td>0.929747</td>\n",
       "      <td>0.596775</td>\n",
       "      <td>0.856664</td>\n",
       "      <td>0.695719</td>\n",
       "      <td>0.924650</td>\n",
       "      <td>0.786094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.912700</td>\n",
       "      <td>0.922288</td>\n",
       "      <td>0.596999</td>\n",
       "      <td>0.866723</td>\n",
       "      <td>0.692071</td>\n",
       "      <td>0.925726</td>\n",
       "      <td>0.789648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.852600</td>\n",
       "      <td>0.918338</td>\n",
       "      <td>0.603083</td>\n",
       "      <td>0.885163</td>\n",
       "      <td>0.703110</td>\n",
       "      <td>0.925951</td>\n",
       "      <td>0.800494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.806200</td>\n",
       "      <td>0.915330</td>\n",
       "      <td>0.606150</td>\n",
       "      <td>0.902766</td>\n",
       "      <td>0.707668</td>\n",
       "      <td>0.925923</td>\n",
       "      <td>0.809055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.765900</td>\n",
       "      <td>0.931547</td>\n",
       "      <td>0.604702</td>\n",
       "      <td>0.891869</td>\n",
       "      <td>0.701773</td>\n",
       "      <td>0.926318</td>\n",
       "      <td>0.803306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.732500</td>\n",
       "      <td>0.938001</td>\n",
       "      <td>0.599846</td>\n",
       "      <td>0.891031</td>\n",
       "      <td>0.706182</td>\n",
       "      <td>0.926642</td>\n",
       "      <td>0.802946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56328a4e2a014600aac4af328fa3519a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.072039…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>▁▃▁▆█▅▇</td></tr><tr><td>eval/BERT_score_f1</td><td>▁▄▆▇▇▇█</td></tr><tr><td>eval/BLEU</td><td>▁▄▄▇█▇▅</td></tr><tr><td>eval/Overall</td><td>▁▃▄▆█▇▇</td></tr><tr><td>eval/STA_preds</td><td>▁▃▄▆█▇▇</td></tr><tr><td>eval/loss</td><td>█▄▂▂▁▄▅</td></tr><tr><td>eval/runtime</td><td>▅▅▁▂▆▆█</td></tr><tr><td>eval/samples_per_second</td><td>▄▄█▇▃▃▁</td></tr><tr><td>eval/steps_per_second</td><td>▃▄█▇▃▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.70618</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92664</td></tr><tr><td>eval/BLEU</td><td>0.59985</td></tr><tr><td>eval/Overall</td><td>0.80295</td></tr><tr><td>eval/STA_preds</td><td>0.89103</td></tr><tr><td>eval/loss</td><td>0.938</td></tr><tr><td>eval/runtime</td><td>46.8849</td></tr><tr><td>eval/samples_per_second</td><td>25.445</td></tr><tr><td>eval/steps_per_second</td><td>0.213</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>1176</td></tr><tr><td>train/learning_rate</td><td>0.00019</td></tr><tr><td>train/loss</td><td>0.7325</td></tr><tr><td>train/total_flos</td><td>758042557218816.0</td></tr><tr><td>train/train_loss</td><td>0.8914</td></tr><tr><td>train/train_runtime</td><td>446.4182</td></tr><tr><td>train/train_samples_per_second</td><td>480.85</td></tr><tr><td>train/train_steps_per_second</td><td>7.527</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-detoxify</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/64fpgb5d' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/64fpgb5d</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231101_135037-64fpgb5d/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "tokenized_datasets_t5_small = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-2\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-detoxify-2\")\n",
    "trainer_t5_small.train() # General rule is 10% number of epochs\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Investigate difference between checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, overall scores are maximized even as validation loss increases (see t5-small-detoxify). We can see that the main aspect that improves is style transfer accuracy (STA), but also acceptability. Next, we check samples of text generated at the checkpoint in which validation loss is minimized vs. the checkpoint where the overall score is maximized and compare them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define checkpoint paths\n",
    "CHECKPOINT_T5_SMALL_MINLOSS = \"../models/t5-small-detoxify/checkpoint-1680\" # Epoch 5, min loss\n",
    "CHECKPOINT_T5_SMALL_BEST = \"../models/t5-small-detoxify/checkpoint-2016\" # Epoch 15, best overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# The trainer object for t5-small-detoxify is already the best model, so we can just load it\n",
    "trainer_t5_small_best = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=CHECKPOINT_T5_SMALL_BEST,\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")\n",
    "\n",
    "# Load the trainer object for t5-small-detoxify with the minimum loss\n",
    "trainer_t5_small_minloss = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify\",\n",
    "    model_checkpoint=CHECKPOINT_T5_SMALL_MINLOSS,\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get predictions from trainer objects\n",
    "def get_preds_df (trainer_1=trainer_t5_small_best,\n",
    "                  trainer_2=trainer_t5_small_minloss,\n",
    "                  tokenizer=tokenizer_t5_small,\n",
    "                  tokenized_data=tokenized_datasets_t5_small[\"validation\"],\n",
    "                  raw_data=raw_datasets[\"validation\"]):\n",
    "    \"\"\"\n",
    "    Gets the predictions from a trainer object and returns a dataframe containing the source, target, and prediction.\n",
    "\n",
    "    Args:\n",
    "        trainer_object (Seq2SeqTrainer): trainer object containing the model\n",
    "        tokenized_data (Dataset): dataset to be evaluated\n",
    "        raw_data (Dataset): dataset containing the raw data\n",
    "\n",
    "    Returns:\n",
    "        preds (Seq2SeqPrediction): Seq2SeqPrediction object containing the predictions\n",
    "        decoded_preds (list): list of decoded predictions\n",
    "        df (DataFrame): dataframe containing the source, target, and prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get predictions\n",
    "    preds_1 = trainer_1.predict(tokenized_data)\n",
    "    decoded_preds_1 = tokenizer.batch_decode(preds_1.predictions, skip_special_tokens=True)\n",
    "    preds_2 = trainer_2.predict(tokenized_data)\n",
    "    decoded_preds_2 = tokenizer.batch_decode(preds_2.predictions, skip_special_tokens=True)\n",
    "\n",
    "    # Create dataframe containing source, target, and prediction\n",
    "    df = pd.DataFrame()\n",
    "    df[\"source\"] = raw_data[\"source\"]\n",
    "    df[\"target\"] = raw_data[\"target\"]\n",
    "    df[\"pred_1\"] = decoded_preds_1\n",
    "    df[\"pred_2\"] = decoded_preds_2\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Get predictions for t5-small-detoxify\n",
    "t5_small_df = get_preds_df(trainer_1=trainer_t5_small_best,\n",
    "                                trainer_2=trainer_t5_small_minloss,\n",
    "                                tokenizer=tokenizer_t5_small,\n",
    "                                tokenized_data=tokenized_datasets_t5_small[\"validation\"],\n",
    "                                raw_data=raw_datasets[\"validation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index: 62\n",
      "Source: Spank them and send them back to california.\n",
      "Target: Scold them and send them back to california\n",
      "Prediction (Best model): Send them back to california\n",
      "Prediction (Min val error): Spank them and send them back to California.\n",
      "Diff in toxicity: -0.994\n",
      "Diff in acceptability: -0.006\n",
      "Diff in BERT: -0.002\n",
      "\n",
      "Index: 84\n",
      "Source: Its the adults who are screaming and carrying on like idiots.\n",
      "Target: Its the adults who are screaming and carrying on\n",
      "Prediction (Best model): It's the adults who are screaming and carrying on.\n",
      "Prediction (Min val error): Its the adults who are screaming and carrying on like idiots.\n",
      "Diff in toxicity: -0.999\n",
      "Diff in acceptability: 0.699\n",
      "Diff in BERT: -0.013\n",
      "\n",
      "Index: 132\n",
      "Source: All canadians are paying for what the left wing voted in, many canadians were not so stupid.\n",
      "Target: All canadians are paying for what the left wing voted in, many canadians had common sense.\n",
      "Prediction (Best model): All canadians are paying for what the left wing voted in, many canadians were not so smart.\n",
      "Prediction (Min val error): All canadians are paying for what the left wing voted in, many canadians were not so stupid.\n",
      "Diff in toxicity: -0.928\n",
      "Diff in acceptability: -0.195\n",
      "Diff in BERT: -0.001\n",
      "\n",
      "Index: 142\n",
      "Source: To be fair obama is just doing what any corrupt republican piece of shit would do.\n",
      "Target: To be fair obama is just doing what any corrupt republican would do\n",
      "Prediction (Best model): To be fair Obama is just doing what any corrupt republican would do.\n",
      "Prediction (Min val error): To be fair obama is just doing what any corrupt republican would do.\n",
      "Diff in toxicity: -0.544\n",
      "Diff in acceptability: 0.1\n",
      "Diff in BERT: 0.0\n",
      "\n",
      "Index: 144\n",
      "Source: Never take any responsibility and then post silly comments blaming liberals.\n",
      "Target: Never take any responsibility and then post bad comments blaming liberals\n",
      "Prediction (Best model): Never take any responsibility and then post comments blaming liberals.\n",
      "Prediction (Min val error): Never take any responsibility and then post silly comments blaming liberals.\n",
      "Diff in toxicity: -0.951\n",
      "Diff in acceptability: 0.001\n",
      "Diff in BERT: -0.002\n",
      "\n",
      "Index: 237\n",
      "Source: Yes yung n i feel stupid ova here tryin keep up wiff yuh know i juss made dis shit\n",
      "Target: Yes yung and i feel naive ova here tryin keep up yuh know i just made this\n",
      "Prediction (Best model): Yes yung n i feel bad ova here tryin keep up wiff yuh know i juss made dis\n",
      "Prediction (Min val error): Yes yung n i feel bad ova here tryin keep up wiff yuh know i juss made dis shit\n",
      "Diff in toxicity: -0.994\n",
      "Diff in acceptability: 0.035\n",
      "Diff in BERT: -0.004\n",
      "\n",
      "Index: 251\n",
      "Source: Are you suggesting that denver is as corrupt as brazil and greece?\n",
      "Target: Are you implying that Denver, like Brazil and Greece, is corrupt?\n",
      "Prediction (Best model): Are you suggesting that denver is corrupt as brazil and greece?\n",
      "Prediction (Min val error): Are you suggesting that denver is as corrupt as Brazil and greece?\n",
      "Diff in toxicity: -0.574\n",
      "Diff in acceptability: -0.395\n",
      "Diff in BERT: 0.004\n",
      "\n",
      "Index: 253\n",
      "Source: Would u want a nigga to act like a bitch lol?\n",
      "Target: Would you want a person to act like that lol?\n",
      "Prediction (Best model): Would you want a man to act like a bad person lol?\n",
      "Prediction (Min val error): Would you want a nigga to act like a bad person lol?\n",
      "Diff in toxicity: -0.991\n",
      "Diff in acceptability: 0.014\n",
      "Diff in BERT: 0.02\n",
      "\n",
      "Index: 285\n",
      "Source: President obama was a fool, or a liar.\n",
      "Target: President Obama was not suitable\n",
      "Prediction (Best model): President Obama was not good.\n",
      "Prediction (Min val error): President Obama was a liar\n",
      "Diff in toxicity: -0.991\n",
      "Diff in acceptability: -0.006\n",
      "Diff in BERT: 0.032\n",
      "\n",
      "Index: 291\n",
      "Source: Since you are taught from a book that was written by a moron you become one.\n",
      "Target: You are what you learn from.\n",
      "Prediction (Best model): Since you are taught from a book that was written by a writer you become one.\n",
      "Prediction (Min val error): Since you are taught from a book that was written by a moron you become one.\n",
      "Diff in toxicity: -0.999\n",
      "Diff in acceptability: -0.413\n",
      "Diff in BERT: 0.004\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Calculate relevant metric columns for t5_small_df\n",
    "# t5_small_df[\"pred_1_BLEURT\"] = calc_bleurt(t5_small_df[\"target\"], t5_small_df[\"pred_1\"], output_mean=False)\n",
    "# t5_small_df[\"pred_2_BLEURT\"] = calc_bleurt(t5_small_df[\"target\"], t5_small_df[\"pred_2\"], output_mean=False)\n",
    "t5_small_df[\"pred_1_toxic_class\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "t5_small_df[\"pred_2_toxic_class\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=False, output_mean=False)\n",
    "t5_small_df[\"pred_1_toxic_score\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_2_toxic_score\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_toxicity, model_toxicity, output_score=True, output_mean=False)\n",
    "t5_small_df[\"source_acceptability\"] = calc_tox_acceptability(t5_small_df[\"source\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"target_acceptability\"] = calc_tox_acceptability(t5_small_df[\"target\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_1_acceptability\"] = calc_tox_acceptability(t5_small_df[\"pred_1\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_2_acceptability\"] = calc_tox_acceptability(t5_small_df[\"pred_2\"].tolist(), tokenizer_acceptability, model_acceptability, output_score=True, output_mean=False)\n",
    "t5_small_df[\"pred_1_BERT_score\"] = calc_bert_score(t5_small_df[\"target\"], t5_small_df[\"pred_1\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "t5_small_df[\"pred_2_BERT_score\"] = calc_bert_score(t5_small_df[\"target\"], t5_small_df[\"pred_2\"], model_type=\"distilbert-base-uncased\", output_mean=False)[2]\n",
    "\n",
    "# Calculate differences in BLEURT, toxicity, acceptability, and BERT score\n",
    "t5_small_df[\"diff_toxic_score\"] = t5_small_df[\"pred_1_toxic_score\"] - t5_small_df[\"pred_2_toxic_score\"]\n",
    "# t5_small_df[\"diff_BLEURT\"] = t5_small_df[\"pred_1_BLEURT\"] - t5_small_df[\"pred_2_BLEURT\"]\n",
    "t5_small_df[\"diff_acceptability\"] = t5_small_df[\"pred_1_acceptability\"] - t5_small_df[\"pred_2_acceptability\"]\n",
    "t5_small_df[\"diff_BERT_score\"] = t5_small_df[\"pred_1_BERT_score\"] - t5_small_df[\"pred_2_BERT_score\"]\n",
    "\n",
    "# Filter to rows where pred_1_toxicity is less than pred_2_toxicity\n",
    "t5_small_df_filtered = t5_small_df[t5_small_df[\"pred_1_toxic_class\"] < t5_small_df[\"pred_2_toxic_class\"]]\n",
    "\n",
    "# Print as individual lines\n",
    "for index, row in t5_small_df_filtered.head(10).iterrows():\n",
    "    print(\"Index:\", index)\n",
    "    print(\"Source:\", row[\"source\"])\n",
    "    print(\"Target:\", row[\"target\"])\n",
    "    print(\"Prediction (Best model):\", row[\"pred_1\"])\n",
    "    print(\"Prediction (Min val error):\", row[\"pred_2\"])\n",
    "    print(\"Diff in toxicity:\", round(row[\"diff_toxic_score\"], 3))\n",
    "    # print(\"Diff in BLEURT:\", round(row[\"diff_BLEURT\"], 3))\n",
    "    print(\"Diff in acceptability:\", round(row[\"diff_acceptability\"], 3))\n",
    "    print(\"Diff in BERT:\", round(row[\"diff_BERT_score\"], 3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this sample, it looks like the predictions from the best model are indeed better at detoxifying text (exmple 108, 211) and is sufficiently different from the target text that there is no indication of overfitting per se. So going forward we prioritize selecting best models based on the overall score as opposed to minimizing validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T5-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07b870512b7744d289f70941923e2a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10733 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331e36ca2e674efeb292c75eb6dea922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1193 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f163397727e479d9eff619c3c84b06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/671 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/models/t5/tokenization_t5.py:238: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231101_135832-3yr3rzpk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/3yr3rzpk' target=\"_blank\">t5-base-detoxify-2</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/3yr3rzpk' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/3yr3rzpk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1344' max='3360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1344/3360 13:33 < 20:22, 1.65 it/s, Epoch 8/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.964100</td>\n",
       "      <td>0.804225</td>\n",
       "      <td>0.605989</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>0.718452</td>\n",
       "      <td>0.925950</td>\n",
       "      <td>0.807496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.753700</td>\n",
       "      <td>0.805599</td>\n",
       "      <td>0.603332</td>\n",
       "      <td>0.880972</td>\n",
       "      <td>0.708433</td>\n",
       "      <td>0.926131</td>\n",
       "      <td>0.799968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.821858</td>\n",
       "      <td>0.598515</td>\n",
       "      <td>0.911987</td>\n",
       "      <td>0.718174</td>\n",
       "      <td>0.926636</td>\n",
       "      <td>0.813460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>0.603491</td>\n",
       "      <td>0.917016</td>\n",
       "      <td>0.727561</td>\n",
       "      <td>0.926918</td>\n",
       "      <td>0.818400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.893294</td>\n",
       "      <td>0.596087</td>\n",
       "      <td>0.918692</td>\n",
       "      <td>0.727935</td>\n",
       "      <td>0.926084</td>\n",
       "      <td>0.817498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.955339</td>\n",
       "      <td>0.601445</td>\n",
       "      <td>0.923722</td>\n",
       "      <td>0.731821</td>\n",
       "      <td>0.925172</td>\n",
       "      <td>0.821176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.022282</td>\n",
       "      <td>0.588833</td>\n",
       "      <td>0.910310</td>\n",
       "      <td>0.727603</td>\n",
       "      <td>0.924775</td>\n",
       "      <td>0.812366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>nan</td>\n",
       "      <td>1.065334</td>\n",
       "      <td>0.591486</td>\n",
       "      <td>0.919531</td>\n",
       "      <td>0.729870</td>\n",
       "      <td>0.924277</td>\n",
       "      <td>0.816939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f42f967f11fd479392e3dfa9da78fe14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.015 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.124705…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>▄▁▄▇▇█▇▇</td></tr><tr><td>eval/BERT_score_f1</td><td>▅▆▇█▆▃▂▁</td></tr><tr><td>eval/BLEU</td><td>█▇▅▇▄▆▁▂</td></tr><tr><td>eval/Overall</td><td>▃▁▅▇▇█▅▇</td></tr><tr><td>eval/STA_preds</td><td>▃▁▆▇▇█▆▇</td></tr><tr><td>eval/loss</td><td>▁▁▁▂▃▅▇█</td></tr><tr><td>eval/runtime</td><td>▁▆█▇▇▆▆▄</td></tr><tr><td>eval/samples_per_second</td><td>█▃▁▂▂▃▃▅</td></tr><tr><td>eval/steps_per_second</td><td>█▃▁▃▃▃▃▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▁      </td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.72987</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92428</td></tr><tr><td>eval/BLEU</td><td>0.59149</td></tr><tr><td>eval/Overall</td><td>0.81694</td></tr><tr><td>eval/STA_preds</td><td>0.91953</td></tr><tr><td>eval/loss</td><td>1.06533</td></tr><tr><td>eval/runtime</td><td>54.1456</td></tr><tr><td>eval/samples_per_second</td><td>22.033</td></tr><tr><td>eval/steps_per_second</td><td>0.185</td></tr><tr><td>train/epoch</td><td>8.0</td></tr><tr><td>train/global_step</td><td>1344</td></tr><tr><td>train/learning_rate</td><td>0.00018</td></tr><tr><td>train/loss</td><td>nan</td></tr><tr><td>train/total_flos</td><td>3899257280409600.0</td></tr><tr><td>train/train_loss</td><td>nan</td></tr><tr><td>train/train_runtime</td><td>813.895</td></tr><tr><td>train/train_samples_per_second</td><td>263.744</td></tr><tr><td>train/train_steps_per_second</td><td>4.128</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-base-detoxify-2</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/3yr3rzpk' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/3yr3rzpk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231101_135832-3yr3rzpk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "tokenized_datasets_t5_base = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_base},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_base = setup_trainer(\n",
    "    output_dir_name=\"t5-base-detoxify-2\",\n",
    "    model_checkpoint=\"t5-base\",\n",
    "    train_dataset=tokenized_datasets_t5_base[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_base[\"validation\"],\n",
    ")\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-base-detoxify-2\")\n",
    "trainer_t5_base.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like t5-small and t5-base performs similarly, but t5-base reaches convergence much earlier. There's also indication of overfitting occurring much earlier (e.g., by epoch 5 validation loss jumps up significantly) without overall performance improving. We will use t5-small going forward given computational constraints. Similar performance may indicate that the task at hand is not too complex and the extra complexity of t5-base is not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Bi-directional, No custom loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial without shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c2cd6689004dcd9499d24d795377e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48e858f6699b489c81789186d386cfe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1c24d1731c44bcb9c7110bd29b4519",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231102_172332-h2jr0wwm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/h2jr0wwm' target=\"_blank\">t5-small-bd-noshuffle-detoxify-2</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/h2jr0wwm' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/h2jr0wwm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3024' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3024/6720 18:54 < 23:07, 2.66 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.487700</td>\n",
       "      <td>1.235429</td>\n",
       "      <td>0.589117</td>\n",
       "      <td>0.784577</td>\n",
       "      <td>0.678174</td>\n",
       "      <td>0.923197</td>\n",
       "      <td>0.751928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.257100</td>\n",
       "      <td>1.167136</td>\n",
       "      <td>0.599725</td>\n",
       "      <td>0.836547</td>\n",
       "      <td>0.691290</td>\n",
       "      <td>0.925403</td>\n",
       "      <td>0.777902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.171800</td>\n",
       "      <td>1.151006</td>\n",
       "      <td>0.600607</td>\n",
       "      <td>0.860855</td>\n",
       "      <td>0.689214</td>\n",
       "      <td>0.925960</td>\n",
       "      <td>0.787498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>1.143344</td>\n",
       "      <td>0.605002</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.692520</td>\n",
       "      <td>0.926318</td>\n",
       "      <td>0.801851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.063400</td>\n",
       "      <td>1.128787</td>\n",
       "      <td>0.604396</td>\n",
       "      <td>0.903604</td>\n",
       "      <td>0.704711</td>\n",
       "      <td>0.926202</td>\n",
       "      <td>0.808504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.024400</td>\n",
       "      <td>1.140509</td>\n",
       "      <td>0.602471</td>\n",
       "      <td>0.898575</td>\n",
       "      <td>0.700976</td>\n",
       "      <td>0.925787</td>\n",
       "      <td>0.805277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.989600</td>\n",
       "      <td>1.132926</td>\n",
       "      <td>0.601382</td>\n",
       "      <td>0.909472</td>\n",
       "      <td>0.705822</td>\n",
       "      <td>0.925794</td>\n",
       "      <td>0.810388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.956400</td>\n",
       "      <td>1.134517</td>\n",
       "      <td>0.600770</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.702148</td>\n",
       "      <td>0.925799</td>\n",
       "      <td>0.805844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.930500</td>\n",
       "      <td>1.137347</td>\n",
       "      <td>0.604801</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.699633</td>\n",
       "      <td>0.926039</td>\n",
       "      <td>0.806531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>▁▄▄▅█▇█▇▆</td></tr><tr><td>eval/BERT_score_f1</td><td>▁▆▇██▇▇▇▇</td></tr><tr><td>eval/BLEU</td><td>▁▆▆██▇▆▆█</td></tr><tr><td>eval/Overall</td><td>▁▄▅▇█▇█▇█</td></tr><tr><td>eval/STA_preds</td><td>▁▄▅▇█▇█▇█</td></tr><tr><td>eval/loss</td><td>█▄▂▂▁▂▁▁▂</td></tr><tr><td>eval/runtime</td><td>▇█▄▇▆▁▆▃▃</td></tr><tr><td>eval/samples_per_second</td><td>▂▁▄▂▃█▃▆▆</td></tr><tr><td>eval/steps_per_second</td><td>▂▁▄▂▃█▃▆▆</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▃▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.69963</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92604</td></tr><tr><td>eval/BLEU</td><td>0.6048</td></tr><tr><td>eval/Overall</td><td>0.80653</td></tr><tr><td>eval/STA_preds</td><td>0.90109</td></tr><tr><td>eval/loss</td><td>1.13735</td></tr><tr><td>eval/runtime</td><td>86.6586</td></tr><tr><td>eval/samples_per_second</td><td>27.533</td></tr><tr><td>eval/steps_per_second</td><td>0.219</td></tr><tr><td>train/epoch</td><td>9.0</td></tr><tr><td>train/global_step</td><td>3024</td></tr><tr><td>train/learning_rate</td><td>0.00016</td></tr><tr><td>train/loss</td><td>0.9305</td></tr><tr><td>train/total_flos</td><td>1859842187919360.0</td></tr><tr><td>train/train_loss</td><td>1.11014</td></tr><tr><td>train/train_runtime</td><td>1134.9693</td></tr><tr><td>train/train_samples_per_second</td><td>378.266</td></tr><tr><td>train/train_steps_per_second</td><td>5.921</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-bd-noshuffle-detoxify-2</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/h2jr0wwm' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/h2jr0wwm</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231102_172332-h2jr0wwm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd = create_bidirectional_dataset(raw_datasets, shuffle=False)\n",
    "\n",
    "tokenized_datasets_bd_t5_small = raw_datasets_bd.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_bd = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-bd-noshuffle-2\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_bd_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_bd_t5_small[\"validation\"],\n",
    "    compute_metrics=partial(compute_metrics_bd, bd_dataset=raw_datasets_bd[\"validation\"], shuffled_data=False)\n",
    "    )\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-bd-noshuffle-detoxify-2\")\n",
    "trainer_t5_small_bd.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial with shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acb9545a0a04bad8a364c2d8016f321",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06ad8c2f7924ef2b1e8f50cdf9109ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff07e44ae0a14a7987f9667c24ff28d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ndifr734) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01a79c8c55945c5be8e8a18bcdddb05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-detoxify-cl-bd-shuffle</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/ndifr734' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/ndifr734</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231102_165432-ndifr734/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ndifr734). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae1a9c6c9d4c4feea6837c38b82df2c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112585377779599, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231102_171104-7omyzpvb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/7omyzpvb' target=\"_blank\">t5-small-bd-shuffle-detoxify-2</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/7omyzpvb' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/7omyzpvb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1680' max='6720' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1680/6720 10:36 < 31:53, 2.63 it/s, Epoch 5/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.495300</td>\n",
       "      <td>1.230033</td>\n",
       "      <td>0.596087</td>\n",
       "      <td>0.780386</td>\n",
       "      <td>0.690938</td>\n",
       "      <td>0.922194</td>\n",
       "      <td>0.753998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.257900</td>\n",
       "      <td>1.177033</td>\n",
       "      <td>0.598875</td>\n",
       "      <td>0.854987</td>\n",
       "      <td>0.690168</td>\n",
       "      <td>0.923962</td>\n",
       "      <td>0.784596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.171200</td>\n",
       "      <td>1.142365</td>\n",
       "      <td>0.599957</td>\n",
       "      <td>0.891869</td>\n",
       "      <td>0.720199</td>\n",
       "      <td>0.923604</td>\n",
       "      <td>0.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.110500</td>\n",
       "      <td>1.137581</td>\n",
       "      <td>0.606112</td>\n",
       "      <td>0.881811</td>\n",
       "      <td>0.699919</td>\n",
       "      <td>0.925512</td>\n",
       "      <td>0.799033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.061300</td>\n",
       "      <td>1.134472</td>\n",
       "      <td>0.603866</td>\n",
       "      <td>0.896060</td>\n",
       "      <td>0.701756</td>\n",
       "      <td>0.925853</td>\n",
       "      <td>0.804719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>▁▁█▃▄</td></tr><tr><td>eval/BERT_score_f1</td><td>▁▄▄▇█</td></tr><tr><td>eval/BLEU</td><td>▁▃▄█▆</td></tr><tr><td>eval/Overall</td><td>▁▅█▇█</td></tr><tr><td>eval/STA_preds</td><td>▁▆█▇█</td></tr><tr><td>eval/loss</td><td>█▄▂▁▁</td></tr><tr><td>eval/runtime</td><td>█▂▆▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▃▄█</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▃▄█</td></tr><tr><td>train/epoch</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/global_step</td><td>▁▁▃▃▅▅▆▆███</td></tr><tr><td>train/learning_rate</td><td>█▆▅▃▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.70176</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92585</td></tr><tr><td>eval/BLEU</td><td>0.60387</td></tr><tr><td>eval/Overall</td><td>0.80472</td></tr><tr><td>eval/STA_preds</td><td>0.89606</td></tr><tr><td>eval/loss</td><td>1.13447</td></tr><tr><td>eval/runtime</td><td>86.1316</td></tr><tr><td>eval/samples_per_second</td><td>27.702</td></tr><tr><td>eval/steps_per_second</td><td>0.221</td></tr><tr><td>train/epoch</td><td>5.0</td></tr><tr><td>train/global_step</td><td>1680</td></tr><tr><td>train/learning_rate</td><td>0.00022</td></tr><tr><td>train/loss</td><td>1.0613</td></tr><tr><td>train/total_flos</td><td>1033270684090368.0</td></tr><tr><td>train/train_loss</td><td>1.21922</td></tr><tr><td>train/train_runtime</td><td>637.6595</td></tr><tr><td>train/train_samples_per_second</td><td>673.275</td></tr><tr><td>train/train_steps_per_second</td><td>10.539</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-bd-shuffle-detoxify-2</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/7omyzpvb' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/7omyzpvb</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231102_171104-7omyzpvb/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd_shuffle = create_bidirectional_dataset(raw_datasets, shuffle=True)\n",
    "\n",
    "tokenized_datasets_bd_shuffle_t5_small = raw_datasets_bd_shuffle.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_bd_shuffle = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-bd-shuffle-2\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_bd_shuffle_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_bd_shuffle_t5_small[\"validation\"],\n",
    "    compute_metrics=partial(compute_metrics_bd, bd_dataset=raw_datasets_bd_shuffle[\"validation\"], shuffled_data=True)\n",
    "    )\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-bd-shuffle-detoxify-2\")\n",
    "trainer_t5_small_bd_shuffle.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tune T5 Model (Uni-directional, with Custom Loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for original loss function:\n",
    "\n",
    "https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer.html#Trainer.compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier-guided loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test code for classifier-guided loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at SkolkovoInstitute/roberta_toxicity_classifier were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Case 1: All Toxic Texts\n",
      "Calculating classifier-guided loss...\n",
      "Logits: tensor([[-3.9838,  3.9090],\n",
      "        [-3.5358,  3.3512],\n",
      "        [-3.9934,  3.9302]], device='cuda:0')\n",
      "Classifier-guided loss: 7.5684027671813965\n",
      "Loss for all toxic texts: 7.5684027671813965\n",
      "\n",
      "Test Case 2: All Neutral Texts\n",
      "Calculating classifier-guided loss...\n",
      "Logits: tensor([[ 4.1200, -4.0838],\n",
      "        [ 4.8206, -5.1404],\n",
      "        [ 4.3735, -4.3328]], device='cuda:0')\n",
      "Classifier-guided loss: 0.00016210721514653414\n",
      "Loss for all neutral texts: 0.00016210721514653414\n",
      "\n",
      "Test Case 3: Mixed Texts\n",
      "Calculating classifier-guided loss...\n",
      "Logits: tensor([[-3.9838,  3.9090],\n",
      "        [ 4.8206, -5.1404],\n",
      "        [ 4.9345, -5.1655],\n",
      "        [-3.9934,  3.9302]], device='cuda:0')\n",
      "Classifier-guided loss: 3.954313278198242\n",
      "Loss for mixed texts: 3.954313278198242\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Load tokenizer and model for toxicity classification\n",
    "# tokenizer_toxicity = RobertaTokenizer.from_pretrained(\"SkolkovoInstitute/roberta_toxicity_classifier\")\n",
    "# model_toxicity = RobertaForSequenceClassification.from_pretrained(\n",
    "#     \"SkolkovoInstitute/roberta_toxicity_classifier\"\n",
    "# ).to(DEVICE)\n",
    "\n",
    "# # Mock class\n",
    "# class MockClass:\n",
    "#     def __init__(self):\n",
    "#         self.tokenizer_toxicity = tokenizer_toxicity\n",
    "#         self.model_toxicity = model_toxicity\n",
    "\n",
    "#     def classifier_guided_loss(self, generated_text, target_label):\n",
    "#         print(\"Calculating classifier-guided loss...\")\n",
    "#         inputs = self.tokenizer_toxicity(generated_text, return_tensors=\"pt\", padding=True).to(DEVICE)\n",
    "#         batch_size = inputs[\"input_ids\"].shape[0]\n",
    "#         target_label = torch.tensor([target_label]*batch_size, dtype=torch.long).to(DEVICE)   \n",
    "#         with torch.no_grad():\n",
    "#             logits = self.model_toxicity(**inputs)[\"logits\"]\n",
    "#         print(f\"Logits: {logits}\")\n",
    "#         loss_fct = torch.nn.CrossEntropyLoss()\n",
    "#         loss = loss_fct(logits, target_label)\n",
    "#         print(f\"Classifier-guided loss: {loss}\")\n",
    "#         return loss\n",
    "\n",
    "# # Test the classifier_guided_loss method with a list of generated text\n",
    "# target_label_neutral = 0\n",
    "\n",
    "# mock_instance = MockClass(tokenizer_toxicity, model_toxicity)\n",
    "\n",
    "# # Test Case 1: All Toxic Texts\n",
    "# generated_texts_toxic = [\"You are stupid.\", \"Go to hell.\", \"You're an idiot.\"]\n",
    "# print(\"Test Case 1: All Toxic Texts\")\n",
    "# loss_toxic = mock_instance.classifier_guided_loss(generated_texts_toxic, target_label_neutral)\n",
    "# print(f\"Loss for all toxic texts: {loss_toxic}\\n\")\n",
    "\n",
    "# # Test Case 2: All Neutral Texts\n",
    "# generated_texts_neutral = [\"You are smart.\", \"Have a nice day.\", \"You're intelligent.\"]\n",
    "# print(\"Test Case 2: All Neutral Texts\")\n",
    "# loss_neutral = mock_instance.classifier_guided_loss(generated_texts_neutral, target_label_neutral)\n",
    "# print(f\"Loss for all neutral texts: {loss_neutral}\\n\")\n",
    "\n",
    "# # Test Case 3: Mixed Texts\n",
    "# generated_texts_mixed = [\"You are stupid.\", \"Have a nice day.\", \"You are so nice\", \"You're an idiot.\"]\n",
    "# print(\"Test Case 3: Mixed Texts\")\n",
    "# loss_mixed = mock_instance.classifier_guided_loss(generated_texts_mixed, target_label_neutral)\n",
    "# print(f\"Loss for mixed texts: {loss_mixed}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTrainerCustomLossUnidirectional(Seq2SeqTrainer):\n",
    "    def __init__(self, tokenizer_toxicity, model_toxicity, *args, **kwargs):\n",
    "        super(Seq2SeqTrainerCustomLossUnidirectional, self).__init__(*args, **kwargs)\n",
    "        self.tokenizer_toxicity = tokenizer_toxicity\n",
    "        self.model_toxicity = model_toxicity.to(DEVICE)\n",
    "\n",
    "    def prediction_step_custom(\n",
    "            self,\n",
    "            inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "            **gen_kwargs,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Generates tokens using the model.\n",
    "\n",
    "        Args:\n",
    "            inputs (dict): Dictionary of inputs to feed to the model.\n",
    "            gen_kwargs (dict): Additional keyword arguments to pass along to the\n",
    "                generate method of the model.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Tuple consisting of the generated tokens and the labels.\n",
    "        \"\"\"\n",
    "        # Check if 'labels' key exists in the inputs dictionary\n",
    "        has_labels = \"labels\" in inputs\n",
    "        \n",
    "        # Prepare the inputs (this is a custom method you should have in your class)\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        # Merge default generation kwargs with any user-provided kwargs\n",
    "        if len(gen_kwargs) == 0 and hasattr(self, \"_gen_kwargs\"):\n",
    "            gen_kwargs = self._gen_kwargs.copy()\n",
    "            \n",
    "        # Remove 'num_beams' and 'max_length' from gen_kwargs if they are None\n",
    "        if \"num_beams\" in gen_kwargs and gen_kwargs[\"num_beams\"] is None:\n",
    "            gen_kwargs.pop(\"num_beams\")\n",
    "        if \"max_length\" in gen_kwargs and gen_kwargs[\"max_length\"] is None:\n",
    "            gen_kwargs.pop(\"max_length\")\n",
    "        \n",
    "        # Set 'synced_gpus' to False in gen_kwargs\n",
    "        gen_kwargs[\"synced_gpus\"] = False\n",
    "        \n",
    "        # Copy inputs to a new dictionary for generation\n",
    "        generation_inputs = inputs.copy()\n",
    "        \n",
    "        # Remove 'decoder_input_ids' if it was created from 'labels'\n",
    "        if (\"labels\" in generation_inputs and \n",
    "            \"decoder_input_ids\" in generation_inputs and \n",
    "            generation_inputs[\"labels\"].shape == generation_inputs[\"decoder_input_ids\"].shape):\n",
    "            generation_inputs = {k: v for k, v in inputs.items() if k != \"decoder_input_ids\"}\n",
    "        \n",
    "        # Generate tokens using the model\n",
    "        generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
    "        \n",
    "        # Hack to ensure generation config is not initialized for each iteration (temporary)\n",
    "        if self.model.generation_config._from_model_config:\n",
    "            self.model.generation_config._from_model_config = False\n",
    "        \n",
    "        # Retrieve the GenerationConfig from the model\n",
    "        gen_config = self.model.generation_config\n",
    "        \n",
    "        # Pad generated tokens if they are shorter than max length\n",
    "        if generated_tokens.shape[-1] < gen_config.max_length:\n",
    "            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_length)\n",
    "        \n",
    "        # Pad labels if they are shorter than max length\n",
    "        if has_labels:\n",
    "            labels = inputs[\"labels\"]\n",
    "            if labels.shape[-1] < gen_config.max_length:\n",
    "                labels = self._pad_tensors_to_max_len(labels, gen_config.max_length)\n",
    "        else:\n",
    "            labels = None\n",
    "        \n",
    "        return generated_tokens, labels\n",
    "\n",
    "    def classifier_guided_loss(self, generated_text, target_label):\n",
    "        \"\"\"\n",
    "        Calculate the classifier-guided loss for the generated text.\n",
    "\n",
    "        Args:\n",
    "            generated_text (list): list of generated text\n",
    "            target_label (int): target label for the classifier (0 for neutral, 1 for toxic)\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The loss value.\n",
    "        \"\"\"\n",
    "        # Tokenize the generated text\n",
    "        inputs = self.tokenizer_toxicity(generated_text, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {key: tensor.to(DEVICE) for key, tensor in inputs.items()}\n",
    "        \n",
    "        # Get the batch size\n",
    "        batch_size = inputs[\"input_ids\"].shape[0]\n",
    "\n",
    "        # Ensure target label is a tensor and on the correct device and shape\n",
    "        target_label = torch.tensor([target_label]*batch_size, dtype=torch.long).to(DEVICE)\n",
    "\n",
    "        # Get the classifier's output logits\n",
    "        logits = self.model_toxicity(**inputs)[\"logits\"]\n",
    "        \n",
    "        # Calculate the cross-entropy loss\n",
    "        loss_fct = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "        loss = loss_fct(logits, target_label)\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def compute_loss(self, model, inputs):\n",
    "        \"\"\"\n",
    "        Compute custom loss for the model.\n",
    "\n",
    "        Args:\n",
    "            model (torch.nn.Module): The model training or evaluating.\n",
    "            inputs (dict): The inputs and targets of the model.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The loss value.\n",
    "        \"\"\"\n",
    "        # Get outputs, predictions and prediction loss\n",
    "        outputs = model(**inputs)\n",
    "        pred_loss = outputs.loss if isinstance(outputs, dict) else outputs[0]\n",
    "\n",
    "        # Call prediction_step\n",
    "        preds, refs = self.prediction_step_custom(model, inputs)\n",
    "        \n",
    "        # Post-process the predictions and references\n",
    "        ## In case the model returns more than the prediction logits\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "\n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "        ## Replace -100s in the labels as we can't decode them\n",
    "        refs = torch.where(refs != -100, refs, torch.tensor(self.tokenizer.pad_token_id).to(DEVICE))\n",
    "        decoded_refs = self.tokenizer.batch_decode(refs, skip_special_tokens=True)\n",
    "\n",
    "        ## Some simple post-processing\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        decoded_refs = [ref.strip() for ref in decoded_refs]\n",
    "\n",
    "        # Calculate the classifier-guided loss\n",
    "        classifier_loss = self.classifier_guided_loss(decoded_preds, 0)\n",
    "\n",
    "        # Calculate total loss as a weighted sum of the classifier loss and the prediction loss\n",
    "        total_loss = 0.5 * classifier_loss + 0.5 * pred_loss\n",
    "\n",
    "        # Return total loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-ac03a1eae4857ca9.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-1a3c402aca53efae.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-52371f98a42bda9e.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/train/cache-88d9bd7e6952bb73.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/validation/cache-eca5d73b870defd3.arrow\n",
      "WARNING:datasets.arrow_dataset:Loading cached processed dataset at /home/garykong/w266_final_project/data/processed/raw_dataset.pkl/test/cache-dcb64ffb202cf6f7.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:kqfby0jr) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">laced-wave-54</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/kqfby0jr' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/kqfby0jr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231101_120503-kqfby0jr/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:kqfby0jr). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e249965f10bf45c4beeb9e1f4e0c35fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112500044429098, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/garykong/w266_final_project/notebooks/wandb/run-20231101_122310-9dkw5o1m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/garykong/w266_final_project/runs/9dkw5o1m' target=\"_blank\">t5-small-detoxify-cl</a></strong> to <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/garykong/w266_final_project' target=\"_blank\">https://wandb.ai/garykong/w266_final_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/garykong/w266_final_project/runs/9dkw5o1m' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/9dkw5o1m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1512' max='3360' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1512/3360 1:09:31 < 1:25:05, 0.36 it/s, Epoch 9/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Sta Preds</th>\n",
       "      <th>Acceptability Preds</th>\n",
       "      <th>Bert Score F1</th>\n",
       "      <th>Overall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.485000</td>\n",
       "      <td>0.956872</td>\n",
       "      <td>0.589097</td>\n",
       "      <td>0.811400</td>\n",
       "      <td>0.682539</td>\n",
       "      <td>0.922900</td>\n",
       "      <td>0.763467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.914700</td>\n",
       "      <td>0.922291</td>\n",
       "      <td>0.601533</td>\n",
       "      <td>0.854987</td>\n",
       "      <td>0.705333</td>\n",
       "      <td>0.925098</td>\n",
       "      <td>0.788388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.923340</td>\n",
       "      <td>0.599175</td>\n",
       "      <td>0.859179</td>\n",
       "      <td>0.692997</td>\n",
       "      <td>0.926027</td>\n",
       "      <td>0.787311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.729200</td>\n",
       "      <td>0.916919</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.891869</td>\n",
       "      <td>0.708302</td>\n",
       "      <td>0.925563</td>\n",
       "      <td>0.804275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.686000</td>\n",
       "      <td>0.917784</td>\n",
       "      <td>0.604230</td>\n",
       "      <td>0.882649</td>\n",
       "      <td>0.700565</td>\n",
       "      <td>0.925639</td>\n",
       "      <td>0.799146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.648600</td>\n",
       "      <td>0.915834</td>\n",
       "      <td>0.602242</td>\n",
       "      <td>0.893546</td>\n",
       "      <td>0.707961</td>\n",
       "      <td>0.926771</td>\n",
       "      <td>0.804813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.604700</td>\n",
       "      <td>0.930312</td>\n",
       "      <td>0.603585</td>\n",
       "      <td>0.900251</td>\n",
       "      <td>0.704489</td>\n",
       "      <td>0.925885</td>\n",
       "      <td>0.806892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.580700</td>\n",
       "      <td>0.939820</td>\n",
       "      <td>0.602446</td>\n",
       "      <td>0.901090</td>\n",
       "      <td>0.702236</td>\n",
       "      <td>0.925975</td>\n",
       "      <td>0.806567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.552500</td>\n",
       "      <td>0.952478</td>\n",
       "      <td>0.601942</td>\n",
       "      <td>0.892707</td>\n",
       "      <td>0.704415</td>\n",
       "      <td>0.925871</td>\n",
       "      <td>0.803529</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b4e2654f0d410baffe516fd45fd976",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.002 MB of 0.027 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.071996…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>▁▇▄█▆█▇▆▇</td></tr><tr><td>eval/BERT_score_f1</td><td>▁▅▇▆▆█▆▇▆</td></tr><tr><td>eval/BLEU</td><td>▁▇▆██▇█▇▇</td></tr><tr><td>eval/Overall</td><td>▁▅▅█▇███▇</td></tr><tr><td>eval/STA_preds</td><td>▁▄▅▇▇▇██▇</td></tr><tr><td>eval/loss</td><td>█▂▂▁▁▁▃▅▇</td></tr><tr><td>eval/runtime</td><td>█▂▃▇▂▁▂▁▂</td></tr><tr><td>eval/samples_per_second</td><td>▁▇▆▂▆█▇█▇</td></tr><tr><td>eval/steps_per_second</td><td>▁▇▆▂▆█▇█▇</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▅▄▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/Acceptability_preds</td><td>0.70441</td></tr><tr><td>eval/BERT_score_f1</td><td>0.92587</td></tr><tr><td>eval/BLEU</td><td>0.60194</td></tr><tr><td>eval/Overall</td><td>0.80353</td></tr><tr><td>eval/STA_preds</td><td>0.89271</td></tr><tr><td>eval/loss</td><td>0.95248</td></tr><tr><td>eval/runtime</td><td>44.776</td></tr><tr><td>eval/samples_per_second</td><td>26.644</td></tr><tr><td>eval/steps_per_second</td><td>0.223</td></tr><tr><td>train/epoch</td><td>9.0</td></tr><tr><td>train/global_step</td><td>1512</td></tr><tr><td>train/learning_rate</td><td>0.00016</td></tr><tr><td>train/loss</td><td>0.5525</td></tr><tr><td>train/total_flos</td><td>972997587369984.0</td></tr><tr><td>train/train_loss</td><td>0.77883</td></tr><tr><td>train/train_runtime</td><td>4175.4272</td></tr><tr><td>train/train_samples_per_second</td><td>51.41</td></tr><tr><td>train/train_steps_per_second</td><td>0.805</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">t5-small-detoxify-cl</strong> at: <a href='https://wandb.ai/garykong/w266_final_project/runs/9dkw5o1m' target=\"_blank\">https://wandb.ai/garykong/w266_final_project/runs/9dkw5o1m</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20231101_122310-9dkw5o1m/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prefixed_datasets = add_prefix(raw_datasets)\n",
    "\n",
    "tokenized_datasets_t5_small = prefixed_datasets.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "trainer_t5_small_cl = setup_trainer(\n",
    "    output_dir_name=\"t5-small-detoxify-cl\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_t5_small[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small[\"validation\"],\n",
    "    trainer_class=Seq2SeqTrainerCustomLoss,\n",
    ")\n",
    "\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-detoxify-cl\")\n",
    "trainer_t5_small_cl.train()\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune T5 Model (Bi-directional, Custom Loss Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqTrainingArgumentsCustomLoss(Seq2SeqTrainingArguments):\n",
    "    def __init__(self, classifier_loss_weight, prediction_loss_weight, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.classifier_loss_weight = classifier_loss_weight\n",
    "        self.prediction_loss_weight = prediction_loss_weight\n",
    "\n",
    "class Seq2SeqTrainerCustomLoss(Seq2SeqTrainer):\n",
    "    def __init__(self, model_toxicity, tokenizer_toxicity, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.classifier_loss_weight = self.args.classifier_loss_weight\n",
    "        self.prediction_loss_weight = self.args.prediction_loss_weight\n",
    "        self.model_toxicity = model_toxicity.to(self.model.device)\n",
    "        self.tokenizer_toxicity = tokenizer_toxicity\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss().to(self.model.device)\n",
    "\n",
    "    def generate_pred_tokens(\n",
    "            self,\n",
    "            inputs: Dict[str, Union[torch.Tensor, Any]],\n",
    "            **gen_kwargs,\n",
    "        ):        \n",
    "        # Prepare the inputs\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        # Merge default generation kwargs with any user-provided kwargs\n",
    "        if len(gen_kwargs) == 0 and hasattr(self, \"_gen_kwargs\"):\n",
    "            gen_kwargs = self._gen_kwargs.copy()\n",
    "            \n",
    "        # # Remove 'num_beams' and 'max_length' from gen_kwargs if they are None\n",
    "        # if \"num_beams\" in gen_kwargs and gen_kwargs[\"num_beams\"] is None:\n",
    "        #     gen_kwargs.pop(\"num_beams\")\n",
    "        # if \"max_length\" in gen_kwargs and gen_kwargs[\"max_length\"] is None:\n",
    "        #     gen_kwargs.pop(\"max_length\")\n",
    "        \n",
    "        # Set 'synced_gpus' to False in gen_kwargs\n",
    "        gen_kwargs[\"synced_gpus\"] = False\n",
    "        \n",
    "        # Copy inputs to a new dictionary for generation\n",
    "        generation_inputs = inputs.copy()\n",
    "        \n",
    "        # Remove 'decoder_input_ids' if it was created from 'labels'\n",
    "        if (\"labels\" in generation_inputs and \n",
    "            \"decoder_input_ids\" in generation_inputs and \n",
    "            generation_inputs[\"labels\"].shape == generation_inputs[\"decoder_input_ids\"].shape):\n",
    "            generation_inputs = {k: v for k, v in inputs.items() if k != \"decoder_input_ids\"}\n",
    "        \n",
    "        # Generate tokens using the model\n",
    "        generated_tokens = self.model.generate(**generation_inputs, **gen_kwargs)\n",
    "        \n",
    "        # # Hack to ensure generation config is not initialized for each iteration (temporary)\n",
    "        # if self.model.generation_config._from_model_config:\n",
    "        #     self.model.generation_config._from_model_config = False\n",
    "        \n",
    "        # # Retrieve the GenerationConfig from the model\n",
    "        # gen_config = self.model.generation_config\n",
    "        \n",
    "        # # Pad generated tokens if they are shorter than max length\n",
    "        # if generated_tokens.shape[-1] < gen_config.max_length:\n",
    "        #     generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_length)\n",
    "        \n",
    "        return generated_tokens\n",
    "\n",
    "    def classifier_guided_loss(self, generated_text, target_labels):\n",
    "        # Tokenize the generated text\n",
    "        preds_tokenized = self.tokenizer_toxicity(generated_text, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the toxicity model\n",
    "        preds_tokenized = {key: tensor.to(self.model_toxicity.device) for key, tensor in preds_tokenized.items()}\n",
    "        \n",
    "        # Get the classifier's output logits\n",
    "        logits = self.model_toxicity(**preds_tokenized)[\"logits\"]\n",
    "        \n",
    "        # Calculate the cross-entropy loss\n",
    "        loss = self.loss_fct(logits, target_labels)\n",
    "        return loss\n",
    "\n",
    "    def compute_loss(self, model, inputs):\n",
    "        # start_time = timer()\n",
    "\n",
    "        # Get outputs, predictions and prediction loss\n",
    "        outputs = model(**inputs)\n",
    "        pred_loss = outputs.loss if isinstance(outputs, dict) else outputs[0]\n",
    "        # pred_loss_time = timer()\n",
    "        # print(f\"Time to compute prediction loss: {pred_loss_time - start_time:.4f} seconds\")\n",
    "\n",
    "        # Call prediction_step\n",
    "        preds = self.generate_pred_tokens(inputs)\n",
    "        # preds_time = timer()\n",
    "        # print(f\"Time to generate predictions: {preds_time - pred_loss_time:.4f} seconds\")\n",
    "\n",
    "        # Decode predictions\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = self.tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "        # decoding_time = timer()\n",
    "        # print(f\"Time to decode predictions: {decoding_time - preds_time:.4f} seconds\")\n",
    "\n",
    "        # Identify indices corresponding to <to_neutral> and <to_toxic>\n",
    "        decoded_inputs = self.tokenizer.batch_decode(inputs[\"input_ids\"], skip_special_tokens=True)\n",
    "        # to_neutral_idx = [i for i, decoded_input in enumerate(decoded_inputs) if decoded_input.startswith(\"to_neutral\")]\n",
    "        # to_toxic_idx = [i for i, decoded_input in enumerate(decoded_inputs) if decoded_input.startswith(\"to_toxic\")]\n",
    "\n",
    "        # Convert decoded_inputs to a NumPy array\n",
    "        decoded_inputs_np = np.array(decoded_inputs)\n",
    "\n",
    "        # Vectorized operation to identify indices\n",
    "        to_neutral_mask = np.char.startswith(decoded_inputs_np, \"to_neutral\")\n",
    "        to_toxic_mask = np.char.startswith(decoded_inputs_np, \"to_toxic\")\n",
    "\n",
    "        # Get the indices where the conditions are True\n",
    "        to_neutral_idx = np.where(to_neutral_mask)[0].tolist()\n",
    "        to_toxic_idx = np.where(to_toxic_mask)[0].tolist()\n",
    "        # indices_time = timer()\n",
    "        # print(f\"Time to identify indices: {indices_time - decoding_time:.4f} seconds\")\n",
    "\n",
    "        # Calculate target labels based on indices. The length should be the same as the number of predictions\n",
    "        target_labels = torch.zeros(len(decoded_preds), dtype=torch.long, device=self.model_toxicity.device)\n",
    "        target_labels[to_neutral_idx] = 0\n",
    "        target_labels[to_toxic_idx] = 1\n",
    "        # target_labels_time = timer()\n",
    "        # print(f\"Time to calculate target labels: {target_labels_time - indices_time:.4f} seconds\")\n",
    "\n",
    "        # Debug print statement to check first 10 decoded inputs and target labels\n",
    "        # for i in range(10):\n",
    "        #     print(f\"Decoded input: {decoded_inputs[i]}, Target label: {target_labels[i]}\")\n",
    "\n",
    "        # Calculate the classifier-guided loss\n",
    "        classifier_loss = self.classifier_guided_loss(decoded_preds, target_labels)\n",
    "        # classifier_loss_time = timer()\n",
    "        # print(f\"Time to compute classifier-guided loss: {classifier_loss_time - target_labels_time:.4f} seconds\")\n",
    "\n",
    "        # Calculate total loss as a weighted sum of the classifier loss and the prediction loss\n",
    "        total_loss = self.classifier_loss_weight * classifier_loss + self.prediction_loss_weight * pred_loss\n",
    "        # total_loss_time = timer()\n",
    "        # print(f\"Time to compute total loss: {total_loss_time - classifier_loss_time:.4f} seconds\")\n",
    "\n",
    "        # Return total loss\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_trainer_custom(output_dir_name,\n",
    "                train_dataset,\n",
    "                eval_dataset,\n",
    "                model_checkpoint=\"t5-small\",\n",
    "                per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,\n",
    "                per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH_SIZE,\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                num_train_epochs=NUM_TRAIN_EPOCHS,\n",
    "                max_length=MAX_OUTPUT_LENGTH,\n",
    "                num_beams=2,\n",
    "                callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    "                trainer_class = Seq2SeqTrainerCustomLoss,\n",
    "                classifier_loss_weight=0.5,\n",
    "                prediction_loss_weight=0.5\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Set up a Seq2SeqTrainer object for training a T5 model.\n",
    "\n",
    "    Default parameters based on this: https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py#L55\n",
    "\n",
    "    Args:\n",
    "        output_dir_name (str): What to name the model in the output directory.\n",
    "        model_checkpoint (str): Name of the pre-trained model to use.\n",
    "        train_dataset (Dataset): Training dataset.\n",
    "        eval_dataset (Dataset): Validation/test dataset.\n",
    "        per_device_train_batch_size (int): Batch size for training.\n",
    "        per_device_eval_batch_size (int): Batch size for evaluation.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        weight_decay (float): Weight decay for optimizer.\n",
    "        num_train_epochs (int): Number of training epochs.\n",
    "        max_length (int): Maximum length of generated sequences.\n",
    "        num_beams (int): Number of beams for beam search.\n",
    "        compute_metrics (function): Function to compute evaluation metrics.\n",
    "        callbacks (list): List of callbacks to use.\n",
    "        trainer_class (Seq2SeqTrainer): Trainer class to use.\n",
    "        classifier_loss_weight (float): Weight for classifier loss.\n",
    "        prediction_loss_weight (float): Weight for prediction loss.\n",
    "\n",
    "    Returns:\n",
    "        Seq2SeqTrainer: Trainer object for training the T5 model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate model and tokenizer\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_checkpoint).to(DEVICE)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "    # Define the data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    # Define generation config\n",
    "    generation_config = GenerationConfig(\n",
    "        max_length=max_length,\n",
    "        num_beams=num_beams,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        bos_token_id=model.config.bos_token_id,\n",
    "        pad_token_id=model.config.pad_token_id,\n",
    "        decoder_start_token_id=model.config.pad_token_id\n",
    "        )\n",
    "\n",
    "    # Save the generation config\n",
    "    GEN_CONFIG_PATH = f\"../models/{output_dir_name}/generation_config\"\n",
    "    generation_config.save_pretrained(GEN_CONFIG_PATH)\n",
    "\n",
    "    # Define the training arguments\n",
    "    args = Seq2SeqTrainingArgumentsCustomLoss(\n",
    "        output_dir=f'../models/{output_dir_name}',\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        logging_strategy=\"epoch\",\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "        learning_rate=learning_rate, \n",
    "        predict_with_generate=True,\n",
    "        generation_config=GEN_CONFIG_PATH,\n",
    "        fp16=True,\n",
    "        report_to=\"wandb\",\n",
    "        logging_steps=100,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"Overall\",\n",
    "        greater_is_better=True,\n",
    "        generation_max_length=max_length,\n",
    "        classifier_loss_weight=classifier_loss_weight,\n",
    "        prediction_loss_weight=prediction_loss_weight\n",
    "    )\n",
    "\n",
    "    # Create a partial function with the tokenizer argument included\n",
    "    compute_metrics_with_tokenizer = partial(compute_metrics, tokenizer=tokenizer)\n",
    "    \n",
    "    # Instantiate the trainer\n",
    "    trainer = trainer_class(\n",
    "        model=model,\n",
    "        model_toxicity=model_toxicity,\n",
    "        tokenizer_toxicity=tokenizer_toxicity,\n",
    "        args=args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_with_tokenizer,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d394fcc48f4f1e8129226dd022d014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/21466 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15517e77165c4a98bae34f32bf5c4197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2386 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0022879b27584f7288fdb16e7a185bb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1342 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "raw_datasets_bd = create_bidirectional_dataset(raw_datasets, shuffle=False)\n",
    "\n",
    "tokenized_datasets_t5_small_bd = raw_datasets_bd.map(\n",
    "    preprocess_function,\n",
    "    fn_kwargs={'tokenizer': tokenizer_t5_small},\n",
    "    batched=True,\n",
    "    remove_columns=[\"source\", \"target\"],\n",
    ")\n",
    "\n",
    "# Instantiate trainer object\n",
    "trainer_t5_small_cl_bd = setup_trainer_custom(\n",
    "    output_dir_name=\"t5-small-detoxify-cl-bd\",\n",
    "    model_checkpoint=\"t5-small\",\n",
    "    train_dataset=tokenized_datasets_t5_small_bd[\"train\"],\n",
    "    eval_dataset=tokenized_datasets_t5_small_bd[\"validation\"],\n",
    "    trainer_class=Seq2SeqTrainerCustomLoss,\n",
    "    classifier_loss_weight=0.5,\n",
    "    prediction_loss_weight=0.5\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "wandb.init(project=\"w266_final_project\", name=\"t5-small-detoxify-cl-bd\")\n",
    "trainer_t5_small_cl_bd.train()\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "w266-Jvh1WXlz-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
